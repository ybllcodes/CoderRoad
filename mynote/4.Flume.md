使用手册：http://flume.apache.org/FlumeUserGuide.html

# Flume

# 一、Flume概述

### 1.1 定义

Flume 是Cloudera提供的一个高可用、高可靠、分布式的海量日志采集、聚合、传输的系统；Flume基于流式架构，灵活简单

### 1.2 基础架构

![image-20220322210149313](http://ybll.vip/md-imgs/202203291305026.png)

> 1. `Agent` : 一个JVM进程，以事件的形式将数据从源头送至目的；三个组成部分，Source、Channel、Sink
>
> 2. `Source` : 负责接收数据到Flume Agent的组件，可以处理各种类型、各种格式的日志数据，包括：
>
>    + **avro**：本质是RPC框架，支持跨语言、跨平台的数据传输，avro Source在flume中多**用于Agent的连接**。
>    + **netcat**：本质是Linux下的端口类工具，netcat Source在Flume中用于**采集端口传输的数据**。
>    + **exec**：支持执行命令的，并将命令执行后的标准输出作为数据采集，多用于**采集一个可追加文件**。
>    + **spooling directory**：支持对一个目录进行监听，**采集目录中一个或多个新生成的文件数据**。
>    + **taildir**：支持对多个目录进行监听，**采集一个或多个目录下的一个或多个可追加文件，支持断点续传**。
>    + 其他：thrift、jms、sequence generator、syslog、http、自定义Source。
>
> 3. `Sink` : 不断地 **轮询** Channel中的事件且批量地移除他们，将这些数据 **批量的、事务的** 写入存储或索引系统、或者被发送到另一个Flume Agent
>
>    + **logger**：logger Sink组件则是将数据写到成Flume框架的运行日志中，配合运行参数 
>
>      `-Dflume.root.logger=INFO,console` 可以将Flume运行日志（其中就包含了采集的数据）输出到控制台，多用于测试环境。
>
>    + **hdfs**：hdfs Sink组件是负责将数据传输到HDFS分布式文件系统中。
>
>    + **avro**：avro Sink组件配合avro Source组件可以实现Agent的连接。
>
>    + **file**：file Sink组件是将采集到的数据直接输出到本地文件系统中，即linux的磁盘上。
>
>    + 其他：thrift、ipc、file、HBase、solr、自定义。
>
> 4. `Channel` : 是**负责暂存数据**的，是位于Source和Sink组件之间的**缓冲区**。
>
>    + 由于Channel组件的存在，使得Source和Sink组件可以运作在不同的**速率**上。
>
>    + Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。
>
>    + **Flume自带两种Channel：**
>
>      **①** **Memory Channel**：基于内存的队列存储事件，适用于对数据安全性要求不高的场景。
>
>      **②** **File Channel**：基于磁盘存储事件，宕机数据不丢失，适用于对数据安全敏感度高的场景。
>
> 5. `Event` : agent中的事件，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。
>
>    + **Header**：K-V结构，用来存放该event的一些属性
>    + **Body**：形式为字节数组，用来存放该条数据
>    + ![image-20220713093122509](http://ybll.vip/md-imgs/202207130933997.png)

+++

### 1.3 Flume安装及入门案例

```bash
#解压安装包
tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module
#将lib文件夹下的guava-11.0.2.jar删除以兼容Hadoop3.1.3  (或者直接重命名)
rm /opt/module/flume/lib/guava-11.0.2.jar

##入门案例:使用Flume监听一个端口,收集该端口数据，并打印到控制台
#1.安装netcat工具
sudo yum install -y nc
sudo netstat -nlp | grep 44444 #检查端口是否被调用
nc -l 44444 #启动服务端
nc localhost 44444 #启动客户端

#2.创建配置文件 $FLUME_HOME/job/flume-netcat-logger.conf
mkdir job
vim flume-netcat-logger.conf

##3.添加如下内容
# Name the components on this agent
a1.sources = r1 #表示a1的Source的名称
a1.sinks = k1 #表示a1的Sink的名称
a1.channels = c1 #表示a1的Channel的名称
# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
# Describe the sink
a1.sinks.k1.type = logger
# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# Bind the source and sink to the channel
a1.sources.r1.channels = c1 #表示将r1和c1连接起来
a1.sinks.k1.channel = c1 #表示将k1和c1连接起来

#4.开启flume监听端口
bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console
#第二种写法
bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console

#参数说明：
#--conf -c :表示配置文件存储在conf/目录
#--name -n :表示给agent起名为a1
#--conf-f -f:本次启动读取的配置文件是哪个文件
#-D:表示flume运行时动态修改参数属性值
#flumn.root.logger=INFO,console :将控制台日志级别设置为INFO级别
#5.使用netcat工具像本机的44444端口发送内容,并观察flume页面的监听数据情况
nc localhost 44444 #启动客户端
```

+++

### 1.4 tailDir案例

 `使用Flume监听整个目录的实时追加文件，并上传至HDFS`

① Exec Source：适用于监控一个实时追加的文件，不能实现断点续传；
② Spooldir Source：适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；
③ Taildir Source：适合用于监听多个实时追加的文件，并且能够实现断点续传。

> 说明(修改源码) 监听文件更名时，不要重新记录所有内容至hdfs
>
> + 修改 `flume-taildir-source` 源码包 
>
> + TailFile.java --> updatePos() 方法
>
>   ![image-20220714095303046](http://ybll.vip/md-imgs/202207140953302.png)
>
> +  ReliableTaildirEventReader.java (只看inode值，不看绝对路径)
>
>   ![image-20220714095237472](http://ybll.vip/md-imgs/202207140952738.png)

> ![img](http://ybll.vip/md-imgs/202207131134693.png)
>
> + 实操内容
>
> ```bash
> #flume根目录创建两个文件夹 datas/tailCase/files  datas/tailCase/logs
> mkdir -p datas/tailCase/files datas/tailCase/logs
> #创建测试文件
> touch files/file1.txt files/log1.txt logs/log2.txt logs/file2.txt
> #编写配置文件
> vim flume-2-taildir-hdfs.conf
> ```
>
> ```yaml
> # agent
> a2.sources=r1
> a2.sinks=k1
> a2.channels=c1
> 
> # source
> a2.sources.r1.type=TAILDIR
> a2.sources.r1.positionFile=/opt/module/flume-1.9.0/taildir_position.json
> a2.sources.r1.filegroups=f1 f2
> a2.sources.r1.filegroups.f1=/opt/module/flume-1.9.0/datas/tailCase/files/.*file.*
> a2.sources.r1.filegroups.f2=/opt/module/flume-1.9.0/datas/tailCase/logs/.*log.*
> 
> # sink
> a2.sinks.k1.type = hdfs
> a2.sinks.k1.hdfs.path = hdfs://hadoop102:8020/flume/tailDir/%Y%m%d/%H
> # 上传文件的前缀
> a2.sinks.k1.hdfs.filePrefix = tail-
> # 是否按照时间滚动文件夹
> a2.sinks.k1.hdfs.round = true
> # 多少时间单位创建一个新的文件夹
> a2.sinks.k1.hdfs.roundValue = 1
> # 重新定义时间单位
> a2.sinks.k1.hdfs.roundUnit = hour
> # 是否使用本地时间戳
> a2.sinks.k1.hdfs.useLocalTimeStamp = true
> # 积攒多少个Event才flush到HDFS一次
> a2.sinks.k1.hdfs.batchSize = 100
> # 设置文件类型，（可选择设置支持压缩的CompressedStream或者不支持压缩的DataStream） 
> a2.sinks.k1.hdfs.fileType = DataStream
> # 多久生成一个新的文件
> a2.sinks.k1.hdfs.rollInterval = 60
> # 设置每个文件的滚动大小大概是128M
> a2.sinks.k1.hdfs.rollSize = 134217700
> # 文件的滚动与Event数量无关
> a2.sinks.k1.hdfs.rollCount = 0
> 
> # channel
> a2.channels.c1.type=memory
> a2.channels.c1.capacity=10000
> a2.channels.c1.transactionCapacity=100
> 
> # bind
> #一个sink只能从一个channel获取数据
> a2.sources.r1.channels=c1
> a2.sinks.k1.channel=c1
> ```
>
> ```bash
> #启动flume监控文件夹(在flume根目录下执行)
> flume-ng agent -c conf/ -n a2 -f job/simplecase/flume-2-tailDir-hdfs.conf -Dflume.root.logger=INFO,console
> 
> #往4个测试文件中追加内容或者重写覆盖进行测试
> ```
>
> 结果说明：
>
> + hdfs上会生成对应的目录`/flume/tailDir/20220713/11` ,每个目录下会有多个文件 `tail-.时间戳`
>
> + 检测到文件内容更新时，会产生.tmp文件，1分钟后会正式更名为`tail-.时间戳`文件，后续文件的更新会产生新的.tmp文件，并记录在此中
> + 对测试文件的追加操作都会记录，并存入hdfs中，覆盖重写则只会将第一次重写记录，存入hdfs中
> + 断点续传：`a2.sources.r1.positionFile=/opt/module/flume-1.9.0/taildir_position.json`
>   + Taildir Source维护了一个json格式的position File，其会定期的往position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File的格式如下：
>   + ![image-20220713115230782](http://ybll.vip/md-imgs/202207131152853.png)
>   + 注：Linux中**储存文件元数据**的区域就叫做**inode**，每个inode都有一个号码，操作系统用inode号码来识别不同的文件，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。
>
> +++

# 二、Flume进阶(案例)

### 2.1 事务和内部原理

#### 2.1.1 Flume事务

> ![image-20220713212314411](http://ybll.vip/md-imgs/202207132123508.png)
>
> + ![12334](http://ybll.vip/md-imgs/202207141005590.png)



#### 2.1.2 Agent内部原理(重点)

> 
>
> ![image-20220713212409173](http://ybll.vip/md-imgs/202207132124267.png)
>
> + 123
> + ![1234](http://ybll.vip/md-imgs/202207141005590.png)

#### 2.1.3  拓扑结构

##### 简单串联

![image-20220714102043102](http://ybll.vip/md-imgs/202207141020169.png)

+ 这种模式是将多个flume顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。

##### 复制和多路复用

![image-20220714102142722](http://ybll.vip/md-imgs/202207141021792.png)

+ **Channel1,Channel2,Channel3是复制关系，数据一致** ，Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。

##### 负载均衡和故障转移（Sink组）

![image-20220714102335671](http://ybll.vip/md-imgs/202207141023750.png)

+ Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。

##### 聚合（最常用）

![image-20220714102545707](http://ybll.vip/md-imgs/202207141025786.png)

+ 这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等，进行日志分析。

### 2.2 复制案例

+ 需求：

  > Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS；同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。
  >
  > +++
  >
  > ![img](http://ybll.vip/md-imgs/202207131819470.png)

  +++

+ 准备工作

  > ```bash
  > mkdir -p /opt/module/flume-1.9.0/job/defaultcase/copy #创建复制案例的配置文件
  > touch /opt/module/flume-1.9.0/datas/realtime.log  #创建测试文件，对其进行监听
  > ```
  >
  > ```yaml
  > ######### flume-1-exec-avro.conf
  > 
  > # agent
  > a1.sources=r1
  > a1.sinks=k1 k2
  > a1.channels=c1 c2
  > 
  > # source
  > a1.sources.r1.type=exec
  > a1.sources.r1.command=tail -F /opt/module/flume-1.9.0/datas/realtime.log
  > #a1.sources.r1.selector.type=replicating
  > 
  > # sink
  > a1.sinks.k1.type=avro
  > a1.sinks.k1.hostname=hadoop102
  > a1.sinks.k1.port=4141
  > 
  > a1.sinks.k2.type=avro
  > a1.sinks.k2.hostname=hadoop102
  > a1.sinks.k2.port=4142
  > 
  > # channel
  > a1.channels.c1.type=memory
  > a1.channels.c1.capacity=10000
  > a1.channels.c1.transactionCapacity=100
  > 
  > # bind
  > #一个sink只能从一个channel获取数据
  > a1.sources.r1.channels=c1 c2
  > a1.sinks.c1.channel=c1
  > a1.sinks.c2.channel=c2
  > 
  > 
  > 
  > ######### flume-2-avro-hdfs.conf
  > # agent
  > a2.sources=r1
  > a2.sinks=k1
  > a2.channels=c1
  > 
  > # source
  > a2.sources.r1.type=avro
  > a2.sources.r1.bind=hadoop102
  > a2.sources.r1.port=4141
  > 
  > # sink
  > a2.sinks.k1.type = hdfs
  > a2.sinks.k1.hdfs.path = hdfs://hadoop102:8020/flume/copycase/%Y%m%d/%H
  > # 上传文件的前缀
  > a2.sinks.k1.hdfs.filePrefix = copy-
  > # 是否按照时间滚动文件夹
  > a2.sinks.k1.hdfs.round = true
  > # 多少时间单位创建一个新的文件夹
  > a2.sinks.k1.hdfs.roundValue = 1
  > # 重新定义时间单位
  > a2.sinks.k1.hdfs.roundUnit = hour
  > # 是否使用本地时间戳
  > a2.sinks.k1.hdfs.useLocalTimeStamp = true
  > # 积攒多少个Event才flush到HDFS一次
  > a2.sinks.k1.hdfs.batchSize = 100
  > # 设置文件类型，（可选择设置支持压缩的CompressedStream或者不支持压缩的DataStream） 
  > a2.sinks.k1.hdfs.fileType = DataStream
  > # 多久生成一个新的文件
  > a2.sinks.k1.hdfs.rollInterval = 60
  > # 设置每个文件的滚动大小大概是128M
  > a2.sinks.k1.hdfs.rollSize = 134217700
  > # 文件的滚动与Event数量无关
  > a2.sinks.k1.hdfs.rollCount = 0
  > 
  > # channel
  > a2.channels.c1.type=memory
  > a2.channels.c1.capacity=10000
  > a2.channels.c1.transactionCapacity=100
  > 
  > # bind
  > #一个sink只能从一个channel获取数据
  > a2.sources.r1.channels=c1
  > a2.sinks.k1.channel=c1
  > 
  > 
  > ######### flume-3-avro-file.conf
  > # agent
  > a3.sources=r1
  > a3.sinks=k1
  > a3.channels=c1
  > 
  > # source
  > a3.sources.r1.type=avro
  > a3.sources.r1.bind=hadoop102
  > a3.sources.r1.port=4142
  > 
  > # sink
  > a3.sinks.k1.type = file_roll
  > a3.sinks.k1.sink.directory = /opt/module/flume-1.9.0/datas/file_roll
  > 
  > 
  > # channel
  > a3.channels.c1.type=memory
  > a3.channels.c1.capacity=10000
  > a3.channels.c1.transactionCapacity=100
  > 
  > # bind
  > #一个sink只能从一个channel获取数据
  > a3.sources.r1.channels=c1
  > a3.sinks.k1.channel=c1
  > ```

  +++

+ 进行测试

  > ```yaml
  > # sink存入本地文件时，要求目录必须存在 /opt/module/flume-1.9.0/datas/file_roll
  > mkdir -p /opt/module/flume-1.9.0/datas/file_roll
  > 
  > #必须先启动下游的flume，下游的flume相当于服务器
  > flume-ng agent -n a2 -c conf/ -f job/defaultcase/copy/flume-2-avro-hdfs.conf -Dflume.root.logger=INFO,console
  > 
  > flume-ng agent -n a3 -c conf/ -f job/defaultcase/copy/flume-3-avro-file.conf -Dflume.root.logger=INFO,console
  > 
  > flume-ng agent -n a1 -c conf/ -f job/defaultcase/copy/flume-1-exec-avro.conf -Dflume.root.logger=INFO,console
  > ```
  >
  > **注意**：
  >
  > 1. file Sink采集数据到本地磁盘时，本地文件是按照时间滚动产生的，即使没有时间采集过来，本地也会生成空文件。
  > 2. 关闭上述3个flume进程后，继续追加内容，重新启动后，所有数据都会存入最新的hdfs文件和本地文件中（包括之前追加时记载的内容）

+++

### 2.3 多路复用和拦截器的应用

+ 需求：使用flume采集服务器端口日志数据，需要按照日志类型的不同，将不同种类的日志发往不同分析系统

  > ![image-20220713190936782](http://ybll.vip/md-imgs/202207131909865.png)
  >
  > +++
  >
  > ![image-20220713191006402](http://ybll.vip/md-imgs/202207131910470.png)

+ 自定义拦截器(java程序)

  > 1. 引入依赖
  >
  >    ```xml
  >    <dependency>
  >        <groupId>org.apache.flume</groupId>
  >        <artifactId>flume-ng-core</artifactId>
  >        <version>1.9.0</version>
  >    </dependency>
  >    ```
  >
  > 2. 自定义拦截器类，实现 `Interceptor` 接口
  >
  >    ```java
  >    public class CustomInterceptor implements Interceptor {
  >    
  >        @Override
  >        public void initialize() {
  >    
  >        }
  >    
  >        /**
  >         * 拦截单个事件
  >         * @param event
  >         * @return
  >         */
  >        @Override
  >        public Event intercept(Event event) {
  >            //1.获取数据
  >            byte[] msg = event.getBody();
  >            //2.判断
  >            if (msg[0]>='a' && msg[0] <='z' || msg[0]>='A' && msg[0] <='Z'){
  >                //2.1 向头部插入属性 k:type v:letter
  >                Map<String, String> headers = event.getHeaders();
  >                headers.put("type","letter");
  >            }
  >            if (msg[0]>='0' && msg[0] <='9'){
  >                //2.1 向头部插入属性 k:type v:number
  >                Map<String, String> headers = event.getHeaders();
  >                headers.put("type","number");
  >            }
  >            return event;
  >        }
  >    
  >        /**
  >         * 拦截批量事件
  >         * @param list
  >         * @return
  >         */
  >        @Override
  >        public List<Event> intercept(List<Event> list) {
  >            //1.遍历列表
  >            for (Event event : list){
  >                Event intercept = intercept(event);
  >            }
  >            //2.返回列表（地址传递，无需创建新列表）
  >            return list;
  >        }
  >    
  >        @Override
  >        public void close() {
  >    
  >        }
  >    
  >        /**
  >         * 静态内部类，运行时由此创建对象
  >         */
  >        public static class Builder implements Interceptor.Builder{
  >    
  >            @Override
  >            public Interceptor build() {
  >                return new CustomInterceptor();
  >            }
  >            @Override
  >            public void configure(Context context) {
  >            }
  >        }
  >    }
  >    ```
  >
  >    + 将项目打包jar,导入到flume的 `lib` 目录下

  +++

+ 编辑配置文件

```yaml
#########flume-1-netcat-avro.conf
# agent
a1.sources=r1
a1.sinks=k1 k2
a1.channels=c1 c2

# source
a1.sources.r1.type=netcat
a1.sources.r1.bind=hadoop102
a1.sources.r1.port=44444
#Interceptor 拦截器
a1.sources.r1.interceptors=i1
a1.sources.r1.interceptors.i1.type=com.atguigu.flume.CustomInterceptor$Builder
#Channel Selector:Channel选择器
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = type
a1.sources.r1.selector.mapping.letter = c1
a1.sources.r1.selector.mapping.number = c2

# sink
a1.sinks.k1.type=avro
a1.sinks.k1.hostname=hadoop102
a1.sinks.k1.port=4141

a1.sinks.k2.type=avro
a1.sinks.k2.hostname=hadoop102
a1.sinks.k2.port=4142

# channel
a1.channels.c1.type=memory
a1.channels.c1.capacity=10000
a1.channels.c1.transactionCapacity=100

a1.channels.c2.type=memory
a1.channels.c2.capacity=10000
a1.channels.c2.transactionCapacity=100

# bind
#一个sink只能从一个channel获取数据
a1.sources.r1.channels=c1 c2
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c2


#########flume-2-avro-logger.conf
# agent
a2.sources=r1
a2.sinks=k1
a2.channels=c1

# source
a2.sources.r1.type=avro
a2.sources.r1.bind=hadoop102
a2.sources.r1.port=4141

# sink
a2.sinks.k1.type=logger

# channel
a2.channels.c1.type=memory
a2.channels.c1.capacity=10000
a2.channels.c1.transactionCapacity=100

# bind
#一个sink只能从一个channel获取数据
a2.sources.r1.channels=c1
a2.sinks.k1.channel=c1


#########flume-3-avro-logger.conf
# agent
a3.sources=r1
a3.sinks=k1
a3.channels=c1

# source
a3.sources.r1.type=avro
a3.sources.r1.bind=hadoop102
a3.sources.r1.port=4142

# sink
a3.sinks.k1.type=logger


# channel
a3.channels.c1.type=memory
a3.channels.c1.capacity=10000
a3.channels.c1.transactionCapacity=100

# bind
#一个sink只能从一个channel获取数据
a3.sources.r1.channels=c1
a3.sinks.k1.channel=c1
```

+ 测试

```bash
#分别启动 flume2 flume3 flume1 进程
flume-ng agent -n a2 -c conf/ -f job/defaultcase/multi/flume-2-avro-logger.conf -Dflume.root.logger=INFO,console

flume-ng agent -n a3 -c conf/ -f job/defaultcase/multi/flume-3-avro-logger.conf -Dflume.root.logger=INFO,console

flume-ng agent -n a1 -c conf/ -f job/defaultcase/multi/flume-1-netcat-avro.conf -Dflume.root.logger=INFO,console

#利用netcat向hadoop102:44444 发送数据（不能写localhost）
nc hadoop102 44444

#结果：
字母开头的字符，字符串都在 flume2进程
数字开头的字符，字符串都在 flume3进程
```

+++

### 2.4 聚合案例

+ 需求

  > hadoop102上的flume-1监控文件/opt/module/flume/datas/.*file*.，
  >
  > hadoop103上的flume-2监控某一个端口的数据流，
  >
  > hadoop104上的flume-3，接收flume-1和flume-2的数据，flume-3将最终数据打印到控制台。
  >
  > +++
  >
  > ![](http://ybll.vip/md-imgs/202207132010193.png)

+ 准备工作

  > ```bash
  > #创建文件夹存放案例中的配置文件
  > mkdir -p /opt/module/flume-1.9.0/job/defaultcase/juhe
  > 
  > #将Flume的全部目录分发到集群其他节点
  > xsync /opt/module/flume-1.9.0
  > ```
  >
  > ```yaml
  > ########### flume-1-tailDir-avro.conf
  > # agent
  > a1.sources=r1
  > a1.sinks=k1
  > a1.channels=c1
  > 
  > # source
  > a1.sources.r1.type=TAILDIR
  > a1.sources.r1.positionFile=/opt/module/flume-1.9.0/taildir_position2.json
  > a1.sources.r1.filegroups=f1
  > a1.sources.r1.filegroups.f1=/opt/module/flume-1.9.0/datas/realtime.log
  > 
  > # sink
  > a1.sinks.k1.type = avro
  > a1.sinks.k1.hostname=hadoop104
  > a1.sinks.k1.port=4141
  > 
  > # channel
  > a1.channels.c1.type=memory
  > a1.channels.c1.capacity=10000
  > a1.channels.c1.transactionCapacity=100
  > 
  > # bind
  > #一个sink只能从一个channel获取数据
  > a1.sources.r1.channels=c1
  > a1.sinks.k1.channel=c1
  > 
  > 
  > ##############flume-2-netcat-avro.conf
  > # agent
  > a1.sources=r1
  > a1.sinks=k1
  > a1.channels=c1
  > 
  > # source
  > a1.sources.r1.type=netcat
  > a1.sources.r1.bind=localhost
  > a1.sources.r1.port=44444
  > 
  > # sink
  > a1.sinks.k1.type = avro
  > a1.sinks.k1.hostname=hadoop104
  > a1.sinks.k1.port=4141
  > 
  > # channel
  > a1.channels.c1.type=memory
  > a1.channels.c1.capacity=10000
  > a1.channels.c1.transactionCapacity=100
  > 
  > # bind
  > #一个sink只能从一个channel获取数据
  > a1.sources.r1.channels=c1
  > a1.sinks.k1.channel=c1
  > 
  > 
  > ##############flume-3-avro-logger.conf
  > # agent
  > a1.sources=r1
  > a1.sinks=k1
  > a1.channels=c1
  > 
  > # source
  > a1.sources.r1.type=avro
  > a1.sources.r1.bind=hadoop104
  > a1.sources.r1.port=4141
  > 
  > # sink
  > a1.sinks.k1.type=logger
  > 
  > 
  > # channel
  > a1.channels.c1.type=memory
  > a1.channels.c1.capacity=10000
  > a1.channels.c1.transactionCapacity=100
  > 
  > # bind
  > #一个sink只能从一个channel获取数据
  > a1.sources.r1.channels=c1
  > a1.sinks.k1.channel=c1
  > ```

+ 测试

  > ```bash
  > #hadoop104上执行，必须最先执行
  > bin/flume-ng agent -n a1 -c conf/ -f job/defaultcase/juhe/flume-3-avro-logger.conf -Dflume.root.logger=INFO,console
  > 
  > #hadoop102上执行
  > bin/flume-ng agent -n a1 -c conf/ -f job/defaultcase/juhe/flume-2-netcat-avro.conf -Dflume.root.logger=INFO,console
  > #hadoop103上执行
  > bin/flume-ng agent -n a1 -c conf/ -f job/defaultcase/juhe/flume-1-tailDir-avro.conf -Dflume.root.logger=INFO,console
  > 
  > #hadoop102上netcat发送数据，hadoop104上收集
  > nc localhost 44444
  > #hadoop103上追加文件内容，hadoop104也会收集
  > echo 'hello' >> datas/realtime.log
  > ```
  >
  > +++

# 三、Flume数据流监控

### 3.1 Ganglia的安装与部署

> Ganglia由gmond、gmetad和gweb三部分组成。
>
> + gmond（Ganglia Monitoring Daemon）：
>   是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。
>   使用gmond，你可以很容易收集很多系统指标数据，如CPU、内存、磁盘、网络和活跃进程的数据等
> + gmetad（Ganglia Meta Daemon）：
>   整合所有信息，并将其以RRD格式存储至磁盘的服务。
> + gweb（Ganglia Web）Ganglia可视化工具：
>   gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。
>   在Web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。

**规划与安装**

|           | gweb | gmetad | gmond |
| --------- | ---- | ------ | ----- |
| hadoop102 | ture | true   | true  |
| hadoop103 |      |        | true  |
| hadoop104 |      |        | true  |

**安装步骤**

```bash
# 三台节点安装 epel-release,以及 ganglia-gmond
sudo yum -y install epel-release
sudo yum -y install ganglia-gmond

# hadoop102上安装 gmetad gweb
sudo yum -y install ganglia-gmetad
sudo yum -y install ganglia-web
```

> 修改配置信息
>
> + hadoop102 修改 /etc/httpd/conf.d/ganglia.conf
>
> ```mysql
> ###### /etc/httpd/conf.d/ganglia.conf
> # 修改为红颜色的配置：
> # Ganglia monitoring system php web frontend
> Alias /ganglia /usr/share/ganglia
> <Location /ganglia>
>   # Require local
>   # 通过windows访问ganglia,需要配置Linux对应的主机(windows)ip地址
>   Require ip 192.168.1.1    // 修改该项,Require只能有一个         
>   #Require all granted              // 如果上面的不好使，就用这个，此配置仅保留一个
>   # Require ip 10.1.2.3
>   # Require host example.org
> </Location>
> ```
>
> + 当前window访问集群时通过Vmnet8网卡访问，因此ip应写Vmnet8的地址
>
> +++
>
> + hadoop102 修改 /etc/ganglia/gmetad.conf
>
> ```text
> data_source "my cluster" hadoop102
> ```
>
> +++
>
> + 三台节点修改 /etc/ganglia/gmode.conf
>
> ```bash
> cluster {
>   name = "my cluster"   #修改该项，与上面对应
>   owner = "unspecified"
>   latlong = "unspecified"
>   url = "unspecified"
> }
> udp_send_channel {
>   #bind_hostname = yes # Highly recommended, soon to be default.
>                        # This option tells gmond to use a source address
>                        # that resolves to the machine's hostname.  Without
>                        # this, the metrics may appear to come from any
>                        # interface and the DNS names associated with
>                        # those IPs will be used to create the RRDs.
>   # mcast_join = 239.2.11.71
>   # 数据发送给hadoop102
>   host = hadoop102  #修改该项
>   port = 8649
>   ttl = 1
> }
> udp_recv_channel {
>   # mcast_join = 239.2.11.71
>   port = 8649
>   # 接收来自任意连接的数据
>   bind = 0.0.0.0   #修改该项
>   retry_bind = true
>   # Size of the UDP buffer. If you are handling lots of metrics you really
>   # should bump it up to e.g. 10MB or even higher.
>   # buffer = 10485760
> }
> ```
>
> +++
>
> + hadoop102 修改 /etc/selinux/config
>
> ```mysql
> # This file controls the state of SELinux on the system.
> # SELINUX= can take one of these three values:
> #     enforcing - SELinux security policy is enforced.
> #     permissive - SELinux prints warnings instead of enforcing.
> #     disabled - No SELinux policy is loaded.
> SELINUX=disabled
> # SELINUXTYPE= can take one of these two values:
> #     targeted - Targeted processes are protected,
> #     mls - Multi Level S
> ```
>
> + 说明
>
>   + `selinux本次生效关闭`  必须重启，或者执行命令使其临时生效
>
>     ```bash
>     sudo setenforce 0
>     ```
>
> +++

**启动　ｇａｎｇｌｉａ　**

```bash
#三台节点都执行
sudo systemctl start gmond

# hadoop102 执行
sudo systemctl start httpd
sudo systemctl start gmetad
```

**浏览 ganglia 页面**

http://hadoop102/ganglia

+ 若提示权限不足，则修改 `/var/lib/ganglia` 目录的权限

  ```bash
  sudo chmod -R 777 /var/lib/ganglia
  ```

**操作 Flume 测试监控**

















