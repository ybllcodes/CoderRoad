# Spark

## Spark 入门

### 1-spark-yarn搭建

> + 安装操作合集
>
>   ```bash
>   mv spark-3.0.0-bin-hadoop3.2/ spark-yarn
>   
>   #修改配置文件 spark-env.sh
>   mv spark-env.sh.template spark-env.sh
>   vim spark-env.sh
>   	#添加该行(hadoop配置文件所在目录)
>   	YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
>   
>   #启动hdsf、yarn
>   start-dfs.sh #hadoop102
>   start-yarn.sh #hadoop103
>   
>   # 执行程序进行测试， http://hadoop103:8088（查看运行页面）
>    bin/spark-submit \
>   --class org.apache.spark.examples.SparkPi \
>   --master yarn \
>   ./examples/jars/spark-examples_2.12-3.0.0.jar \
>   10
>   ```
>
>   ```bash
>   ########配置历史服务器#######
>   #修改配置文件 spark-defaults.conf
>   mv spark-defaults.conf.template spark-defaults.conf
>   vim spark-defaults.conf  #添加如下内容
>   	spark.eventLog.enabled          true
>   	spark.eventLog.dir               hdfs://hadoop102:8020/directory
>   	spark.yarn.historyServer.address=hadoop102:18080
>   	spark.history.ui.port=18080
>   #后两个参数含义：配置spark历史服务器关联路径。
>   #目的：点击yarn（8088）上spark任务运行时的history按钮，进入的是spark历史服务器（18080），而不再是yarn历史服务器（19888）
>   
>   
>   #修改 spark-env.sh,添加如下内容
>   vim spark-env.sh
>   
>   export SPARK_HISTORY_OPTS="
>   -Dspark.history.ui.port=18080 
>   -Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory 
>   -Dspark.history.retainedApplications=30"
>   #最后一个参数含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。
>   
>   
>   #重启spark历史服务
>   sbin/stop-history-server.sh
>   sbin/start-history-server.sh
>   
>   #提交程序测试
>   bin/spark-submit \
>   --class org.apache.spark.examples.SparkPi \
>   --master yarn \
>   ./examples/jars/spark-examples_2.12-3.0.0.jar \
>   10
>   ```
>   
>+ Spark三种模式对比
> 
> | 模式       | Spark安装机器数 | 需启动的进程   | 所属者 |
> | ---------- | --------------- | -------------- | ------ |
> | Local      | 1               | 无             | Spark  |
> | Standalone | 3               | Master及Worker | Spark  |
> | Yarn       | 1               | Yarn及HDFS     | Hadoop |
> 

### 2-spark-submit常用参数

> + Spark Shell会预加载 SparkContext 对象
>
>   ![image-20220922205239520](http://ybll.vip/md-imgs/202209222052676.png)
>
> +++
>
> + `--master` 指定任务提交到哪个资源调度器中
>
>      > + local模式：
>      >   + local: 使用一个线程模拟执行,代表同一时间只能执行一个task.【task串行执行】
>      >   + local[*]: 使用cpu个数个线程模拟执行, 代表同一时间只能执行cpu个数个task，【task并行执行】
>      >   + local[N]: 使用N个线程模拟执行,代表同一时间能执行N个task.【大于cpu核数，一个核可执行多个task】
>      > + standalone模式: spark://master主机名:7077,... 
>      >   + 【spark://hadoop102:7077,hadoop103:7077】
>      > + yarn模式:  yarn  （不需要搭建集群）
> + `--deploy-mode` 指定部署模式 [client/cluster]
>
>   > + standalone client与cluster模式的区别: Driver所在的位置不一样
>   >   + client模式: Driver在SparkSubmit进程中,此时SparkSubmit进程不能关闭的,关闭之后Driver消失,不能进行任务调度,spark程序会终止。【测试使用】
>   >   + cluster模式: Driver在任意一个Worker中,此时SparkSubmit进程可以关闭的,关闭之后不影响Driver【生产环境使用】
>   > + yarn client与cluster模式的区别: Driver所在的位置不一样
>   >   + client模式: Driver在SparkSubmit进程中,此时SparkSubmit进程不能关闭的,关闭之后Driver消失,不能进行任务调度,spark程序会终止。[测试使用]
>   >   + cluster模式: Driver在ApplicationMaster进程中,此时SparkSubmit进程可以关闭的,关闭之后不影响Driver
>
> + --class 指定任务执行的带有main方法的类的全类名
> + --driver-memory 指定Driver内存大小(Yarn模式下是线程)。
> + --executor-memory  指定executor的内存大小(进程)。
> + --executor-cores 指定每个executor的核数
> + --num-executors 指定任务需要多少个executor [仅限于yarn模式使用]
> + --queue 指定任务提交到哪个资源队列中 [仅限于yarn模式使用]
> + --total-executor-cores 指定任务需要的所有的executor的总核数[仅限于standalone模式使用]
>
> ```bash
> #执行命令，效果如下：
> bin/spark-submit --class org.apache.spark.examples.SparkPi --num-executors 3 --executor-cores 1 --executor-memory 2G --master yarn ./examples/jars/spark-examples_2.12-3.0.0.jar 100
> 
> #启动三个executor,每个都占用1个核(core)，2G内存
> #AppMaster 作为中间协调者，单独占用一核1G
> #Driver进程也会占用 1G内存
> ```
>
> ![image-20220802024648097](http://ybll.vip/md-imgs/202208020247817.png)


https://gitee.com/ruby54/yuntai-template.git
https://gitee.com/ruby54/yuntai2022.git





### 3-Spark入门 回顾

#### 3.1  spark概述

+ 什么是spark
  + Spark是基于内存的大数据分析计算引擎。
+ spark的使用场景
  + spark可以用于离线、实时、机器学习、图计算
+ spark与MR的区别
  + 在进行数据传递的时候,spark是基于内存的,MR是基于磁盘的.
  + MR的task是进程 , Spark的task是线程。

#### 3.2 Spark运行模式

##### local模式

单机安装即可

+ 任务提交: bin/spark-submit --master local/local[N]/local[*]  --class 全类名 jar包路径 参数值 ....*
+ local: 使用一个线程模拟执行,同一时间只能执行一个task
+ local[N]: 使用N个线程模拟执行,同一时间能执行N个task
+ local[*]: 使用cpu个数个线程模拟执行,同一时间能执行cpu个数个task

+++

##### 集群模式

###### standalone模式

> 1. Master 【hadoop中的 RM，节点上启动的进程名】
>
>    职责: 负责资源的管理和分配
>
> 2. Worker ：资源节点【hadoop中的  NM】
>
>    职责: 资源节点与任务执行节点
>
> + Master与Worker是随着集群的启动而启动,是随着集群的停止而消失(进程结束)
> + **Master与Worker就是spark自带的资源调度框架（进程，类似RM,NM）,只有standalone模式才有**
>
> +++
>
> ![image-20220923115330678](http://ybll.vip/md-imgs/202209231153747.png)
>
> 3. Driver【入口：用户提交的maIn方法】
>
>    职责:
>
>    + 负责将用户的程序转成job
>    + 负责将task提交到executor执行
>    + 负责监听task的执行状况
>    + 负责程序运行过程中webUI界面的展示
>
> 4. Executor 【进程，Executor进程内，启动task线程，执行任务】
>    ​职责: 任务的执行进行
>
> + **Executor**在 Worker节点启动
> + **task是线程,是运行在Executor中的线程**
> + **Driver与Executor是随着任务的提交而出现,随着任务的完成而消失。【线程/进程结束】**
>
> +++
>
> > standalone模式: 使用spark自带的资源调度【Master Worker(多个)】
> >
> > + 任务提交: bin/spark-submit --master spark://hadoop102:7077 --class 带有main方法的全类名 jar包路径 参数值 ...【sparksubmit进程】
> > + standalone client与cluster模式的区别 : Driver所在位置不一样
> >   + client模式: Driver在sparksubmit进程中,此时SparkSubmit进程不能关闭的,关闭之后Driver消失,无法进行任务调度,spark程序会终止
> >   
> >     ![image-20220922212930189](http://ybll.vip/md-imgs/202209222129372.png)
> >   
> >   + cluster模式: **Driver在任意一个Worker中,此时SparkSubmit关闭不影响程序的执行**
> >
> > +++
> >
> > + 可配置历史服务器
> > + 可配置高可用

>+++

  

###### yarn模式

+ 使用 YARN 作为资源调度器	

> 
>
> ![image-20220923103055614](http://ybll.vip/md-imgs/202209231030709.png)	
>
> +++
>
> ![image-20220923103651043](http://ybll.vip/md-imgs/202209231036120.png)
>
> + 任务提交: bin/spark-submit --master yarn --class 带有main方法的全类名 jar包路径 参数值 ....
> + yarn client与cluster模式的区别:Driver所在位置不一样。
>   + `--deploy-mode` 指定部署模式 [client/cluster]
>   + client模式(默认): Driver在sparksubmit进程中,此时SparkSubmit进程不能关闭的,关闭之后Driver消失,无法进行任务调度,当前任务会终止【适用于交互，调试，可以在shell端看到app的输出】
>   + cluster模式: Driver进程由ApplicationMaster进程启动,此时SparkSubmit关闭不影响程序的执行【Driver在集群】
> + yarn模式下， Executor进程 运行在 container 容器中

+++

+ 自我总结

  > 1. Spark-Shell命令启动执行，开启 SparkSubmit进程，内置调用客户端对象Client
  > 2. Client对象向Yarn的RM发送指令，请求执行一个Application应用【获取路径，提交资源，封装job提交给RM,等待分配资源启动RM】
  > 3. RM开启AppMaster进程，AppMaster启动通过反射用户代码里的main方法，启动Driver线程【执行用户提交的代码(初始化SparkContext），task的切分，分配任务到Execuotr执行】
  > 4. Driver会向RM申请资源,得到资源后，在NodeManager启动ExecutorBackend进程【Executor进程】，在EB进程中创建Executor对象【计算对象，有线程池，负责执行task】，然后执行Driver分配的任务
  > 5. NodeManager启动Executor进程也是在container容器中启动的（Yarn）

#### 3.3 端口号总结

> 1. Spark查看当前Spark-shell运行任务情况端口号：4040
>
> 2. Spark Master内部通信服务端口号：7077【类比于yarn的8032(RM和NM的内部通信)端口】
> 3. Spark Standalone模式Master Web端口号：8080（类比于Hadoop YARN任务运行情况查看端口号：8088）
> 4. Spark历史服务器端口号：18080 （类比于Hadoop历史服务器端口号：19888）

+++



## SparkCore

> + 减少分区数，可以没有shuffle;
>
>   > 
>
> + 增大分区数，一定需要shuffle，需要落盘
>
>   > `groupBy()`   `sortBy ()`   `distint()`
>   >
>   > `groupByKey()`   `reduceByKey()`

### 1. RDD概述

1. 什么是RDD？

     > + RDD是弹性分布式数据集
     >
     > + RDD在代码中是抽象类,代表的是弹性、不可变、可分区的、元素可并行计算的集合。
     >
     >   + 弹性
     >
     >     1. 计算过程中中间结果是保存在内存的,如果内存不足会自动保存到磁盘;
     >     2. 如果数据丢失,会根据RDD的依赖关系重新计算得到数据;
     >     3. 如果计算出错,会自动重试;
     >     4. 会根据文件大小自动分区,一个文件切片对应一个分区。 			
     >
     >   + 不可变: 
     >
     >     RDD中只是封装了数据的处理逻辑,如果想要重新改变数据需要调用其他方法生成新的RDD。
     >
     >   + 可分区: 
     >
     >     spark是分布式计算的,所以在处理文件数据的时候,会将文件切片,针对每个切片单独处理,每个切片对应一个分区。
     >
     >   + 元素可并行计算: 多个分区之间是并行计算的。
     >
     >     1. RDD的计算是惰性的,必须调用collect这类算子的时候才会触发数据的读取计算
     >     2. RDD不存储数据[底层是迭代器,所以一旦计算过一次,迭代器中就没有数据了

2. RDD五大特性

     > + 一组分区列表: spark是分布式计算的,文件的一个切片对应RDD一个分区 		
     > + 作用在每个分区上的计算函数: 每个分区计算逻辑是一样的,只是处理的数据不一样。 		
     > + 对其他RDD的依赖关系: 后续如果数据丢失,会根据依赖关系重新读取数据重新计算得到丢失的数据 
     > + 分区器[可选]: 元素是KV键值对的RDD，在执行shuffle操作的时候需要使用分区器规划数据放入子RDD哪个分区中。 		
     > + 计算的优先位置[可选]: spark在计算的时候会规划在资源充足的情况下会将计算逻辑放入数据所在的数据,避免从网络拉取数据进行计算。
     
     +++

+++



### 2. RDD的编程

> ```java
>     public void createRddCollection() throws InterruptedException {
>         // 1.创建配置对象
>         SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("sparkCore");
> 
>         // 2. 创建sparkContext
>         JavaSparkContext sc = new JavaSparkContext(conf);
> 
>         // 3. 编写代码
>         JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 0));
>         List<Integer> collect = rdd.collect();
>         System.out.println(rdd.getNumPartitions());
>         //collect.forEach(System.out::println);
> 
>         // 4. 关闭sc
>         sc.stop();
>     }
> ```
>
> ```xml
> <!-- pom.xml -->
>     <dependencies>
> <!--        打包集群运行时，不需要该依赖-->
>         <dependency>
>             <groupId>org.apache.spark</groupId>
>             <artifactId>spark-core_2.12</artifactId>
>             <version>3.0.0</version>
>         </dependency>
>         <dependency>
>             <groupId>org.apache.spark</groupId>
>             <artifactId>spark-sql_2.12</artifactId>
>             <version>3.0.0</version>
> <!--            <scope>provided</scope>-->
>         </dependency>
> 
>         <dependency>
>             <groupId>org.apache.spark</groupId>
>             <artifactId>spark-hive_2.12</artifactId>
>             <version>3.0.0</version>
>         </dependency>
> 
>         <dependency>
>             <groupId>mysql</groupId>
>             <artifactId>mysql-connector-java</artifactId>
>             <version>5.1.49</version>
>         </dependency>
>     </dependencies>
> 
>     <build>
>         <plugins>
>             <plugin>
>                 <groupId>org.apache.maven.plugins</groupId>
>                 <artifactId>maven-shade-plugin</artifactId>
>                 <version>3.1.1</version>
>                 <executions>
>                     <execution>
>                         <phase>package</phase>
>                         <goals>
>                             <goal>shade</goal>
>                         </goals>
>                         <configuration>
>                             <artifactSet>
>                                 <excludes>
>                                     <exclude>com.google.code.findbugs:jsr305</exclude>
>                                     <exclude>org.slf4j:*</exclude>
>                                     <exclude>log4j:*</exclude>
>                                     <exclude>org.apache.hadoop:*</exclude>
>                                 </excludes>
>                             </artifactSet>
>                             <filters>
>                                 <filter>
>                                     <!-- Do not copy the signatures in the META-INF folder.Otherwise, this might cause SecurityExceptions when using the JAR. -->
>                                     <!-- 打包时不复制META-INF下的签名文件，避免报非法签名文件的SecurityExceptions异常-->
>                                     <artifact>*:*</artifact>
>                                     <excludes>
>                                         <exclude>META-INF/*.SF</exclude>
>                                         <exclude>META-INF/*.DSA</exclude>
>                                         <exclude>META-INF/*.RSA</exclude>
>                                     </excludes>
>                                 </filter>
>                             </filters>
> 
>                             <transformers combine.children="append">
>                                 <!-- The service transformer is needed to merge META-INF/services files -->
>                                 <!-- connector和format依赖的工厂类打包时会相互覆盖，需要使用ServicesResourceTransformer解决-->
>                                 <transformer
>                                         implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
>                             </transformers>
>                         </configuration>
>                     </execution>
>                 </executions>
>             </plugin>
>         </plugins>
>     </build>
> ```
>
> +++

#### RDD的创建

+ 通过集合创建RDD : sc.parallelize(集合)
+ 读取外部存储系统创建RDD : sc.textFile(path)【文件读取】、hdfs读取，hbase等
+ 其他的RDD衍生出新RDD : rdd.map/flatMap...

#### RDD的分区数

> + 通过集合创建RDD: sc.parallelize(集合**[,numSlices]**)
>
>   1. 如果有指定numSlices参数,此时创建出的RDD的分区数 = 指定的numSlices参数值
>
>   2. 如果没有指定numSlices参数,此时创建出的RDD的分区数 = defaultParallelism
>
>      + 如果在sparkconf中有设置spark.default.parallelism,此时defaultParallelism = 设置的spark.default.parallelism参数值
>
>      + 如果没有设置spark.default.parallelism
>
>        master=local, defaultParallelism=1
>        master=local[N], defaultParallelism=N
>        master=local[*], defaultParallelism=本地cpu最大线程数据
>        **master=spark://..., defaultParallelism = max(任务所有executor总核数,2)**
>
>   > parallelize() 参数  ==> defaultParallelism ==> 部署方式决定【local,local[N],local[*],】
>
> + 读取文件创建: sc.textFile(path[,minPartitions])
>
>   + 读取文件创建出的RDD分区数 >= minPartitions
>     minPartitions的默认值 = min( defaultParallelism,2 )
>   + 读取文件创建出的RDD分区数最终由文件的切片数决定,文件有多少切片就有多少分区。
>
> + 其他的RDD衍生: rdd.map/flatMap...
>
>   + 其他的RDD衍生出的新RDD分区数 = 依赖的第一个父RDD分区数

+++

#### 切片

> + 读取集合创建RDD的时候
>   RDD每个分区读取的集合切片范围 【左闭右开的范围】:
>   + 开始角标 = (分区号) * 集合长度 / 分区数
>   + 结束角标 = (分区号 + 1 ) * 集合长度 / 分区数
>+ 文件的切片
>   **`计算切片大小 = Math.max(切片最小的大小, Math.min(文件总大小/分区数, 块大小))`**

+++

#### 转换算子

> spark的算子分为两种:
>
> + Transformation[转换]算子: 会生成新的RDD,不会触发任务的计算,只是封装了数据的处理逻辑。
> + Action[行动]算子: 不会生成新的RDD,会触发任务的计算。
>
> +++

##### map()

> map()  : 一对一映射 [原RDD一个元素计算得到新RDD一个元素]
>
> + map里面需要传入一个接口对象,该接口中有一个方法,该方法的参数只有一个,类型就是原RDD元素类型,后续会每个元素传入该方法中,方法计算之后返回的结果就是新RDD的一个元素。
>
> + map生成新RDD元素个数 = 原RDD元素个数
>
> + map的使用场景: 一般用于对数据类型做转换/值做转换[**一对一转换**]

##### flatMap()

> flatMap()  : 数据转换[map]  +  炸开
>
> +  flatMap要求传入一个接口,接口中只有一个需要重写的方法call() ，方法要求必须返回Iterator，方法的参数是原RDD元素类型,该方法是针对原RDD每个元素操作
> + flatMap生成的新RDD元素个数一般是>=原RDD元素个数
> + flatMap的使用场景:**一对多转换**

##### groupBy()

> groupBy : 根据指定的字段对元素进行分组
>
> + groupBy是根据方法的返回值对原RDD所有元素分组；
>
> + 生成的新RDD元素类型是**KV键值对**,K是方法的返回值,V是集合,集合中装载的是K对应原RDD的所有元素；
>
> + **groupBy会产生shuffle操作。**
>
>   ```java
>   /**
>    * groupBy: 按照指定的字段对元素进行分组
>    *      groupBy是根据接口中方法的返回值对元素进行分组
>    *      groupBy生成的新RDD元素类型是KV键值对,K就是接口方法的返回值,V是key对应原RDD的所有的元素
>    *
>    * groupBy会产生shuffle操作
>    *
>    */
>   @Test
>   public void groupBy(){
>   
>       //创建sparkcontext
>       SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>       JavaSparkContext sc = new JavaSparkContext(conf);
>   
>       JavaRDD<Person> rdd1 = sc.parallelize(Arrays.asList(
>               new Person("1001", "lisi", 20, "man", "深圳"),
>               new Person("1002", "zhangsan", 26, "woman", "上海"),
>               new Person("1003", "wangwu", 18, "man", "深圳"),
>               new Person("1004", "zhaoliu", 66, "woman", "北京"),
>               new Person("1005", "lilei", 32, "woman", "北京")
>       ));
>   
>       //按照性别分组
>       //JavaPairRDD<String, Iterable<Person>> rdd2 = rdd1.groupBy(new Function<Person, String>() {
>       //    @Override
>       //    public String call(Person p) throws Exception {
>       //        return p.getSex();
>       //    }
>       //});
>   
>       //按照 sex+region 分组
>       JavaPairRDD<Tuple2, Iterable<Person>> rdd2 = rdd1.groupBy(new Function<Person, Tuple2>() {
>           @Override
>           public Tuple2 call(Person v1) throws Exception {
>               return new Tuple2(v1.getSex(), v1.getRegion());
>           }
>       });
>   
>       System.out.println(rdd2.collect());
>   }
>   ```
>
> +++

##### filter()

> filter : 按照指定条件对RDD元素进行过滤
>
> >  + filter保留的是方法的返回值为true的数据
> >
> >    ```java
> >    /**
> >     * filter: 按照指定条件对数据过滤
> >     *      filter后续是根据接口中方法的返回值进行过滤,如果返回值为true,数据会保留
> >     *
> >     */
> >    @Test
> >    public void filter(){
> >    
> >        //创建sparkcontext
> >        SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >        JavaSparkContext sc = new JavaSparkContext(conf);
> >    
> >        JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 5, 9, 20, 3, 7, 10, 8));
> >    
> >        //需要过滤出偶数
> >        JavaRDD<Integer> rdd2 = rdd1.filter(new Function<Integer, Boolean>() {
> >            @Override
> >            public Boolean call(Integer v1) throws Exception {
> >                return v1 % 2 == 0 ;
> >            }
> >        });
> >    
> >        System.out.println(rdd2.collect());
> >    }
> >    ```
> >
> >  +++

##### distint()

>  distint : **去重 ，会产生 shuffle操作**
>
> >  ```java
> >  /**
> >   * 去重
> >   * distinct会产生shuffle
> >   *
> >   * MR的整个计算过程:  数据 -> InputFormat[切片] -> kv键值对 -> map ->环形缓冲区[分区排序]【80%】 ->  [combiner]  ->磁盘 -> 合并小文件 -> reducer拉取数据 -> 归并排序 -> reduce ->outputFormat -> hdfs
> >   * MR的shuffle: ->环形缓冲区[分区排序]【80%】 ->  [combiner]  ->磁盘 -> 合并小文件 -> reducer拉取数据 -> 归并排序 ->
> >   * spark的shuffle: ->缓冲区[分区【排序】] ->  [combiner]  ->磁盘 -> 合并小文件 -> 分区拉取数据 -> 归并【排序】 ->
> >   */
> >  @Test
> >  public void distinct(){
> >  
> >      //创建sparkcontext
> >      SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >      JavaSparkContext sc = new JavaSparkContext(conf);
> >  
> >      JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 2, 5, 1, 2, 6, 1, 5, 2, 8, 6));
> >  
> >      //JavaRDD<Integer> rdd2 = rdd1.distinct();
> >      //System.out.println(rdd2.collect());
> >  
> >      //自己实现去重
> >      JavaPairRDD<Integer, Iterable<Integer>> rdd2 = rdd1.groupBy(new Function<Integer, Integer>() {
> >          @Override
> >          public Integer call(Integer v1) throws Exception {
> >  
> >              return v1;
> >          }
> >      });
> >      //[ 1 -> Iterable( 1 , 1,  1),
> >      //2 -> Iterable(2,2,2)
> >      //5 -> ...]
> >  
> >      JavaRDD<Integer> rdd3 = rdd2.map(new Function<Tuple2<Integer, Iterable<Integer>>, Integer>() {
> >          @Override
> >          public Integer call(Tuple2<Integer, Iterable<Integer>> v1) throws Exception {
> >              return v1._1;
> >          }
> >      });
> >  
> >      System.out.println(rdd3.collect());
> >  }
> >  ```
> >
> >  +++

##### sortBy()

> sortBy : 根据指定字段对RDD元素排序
>
> >  + sortBy是根据方法的返回值对RDD元素排序
> >
> >  + 默认是升序,如果想要降序设置ascding=false即可
> >
> >  + 会产生shuffle
> >
> >    ```java
> >    /**
> >     *  sortBy: 根据指定的字段对RDD元素排序
> >     *          sortBy是根据方法的返回值对原RDD元素排序
> >     *  sortBy有shuffle
> >     */
> >    @Test
> >    public void sortBy(){
> >        //创建sparkcontext
> >        SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >        JavaSparkContext sc = new JavaSparkContext(conf);
> >    
> >        JavaRDD<Person> rdd1 = sc.parallelize(Arrays.asList(
> >                new Person("1001", "lisi", 20, "man", "深圳"),
> >                new Person("1002", "zhangsan", 26, "woman", "上海"),
> >                new Person("1003", "wangwu", 18, "man", "深圳"),
> >                new Person("1004", "zhaoliu", 66, "woman", "北京"),
> >                new Person("1005", "lilei", 32, "woman", "北京")
> >        ));
> >    	// 泛型参数一：输入类型 ； 泛型参数二：输出的类型（进行比较的类型）
> >        JavaRDD<Person> rdd2 = rdd1.sortBy(new Function<Person, Integer>() {
> >            @Override
> >            public Integer call(Person v1) throws Exception {
> >                return v1.getAge();
> >            }
> >        }, false,3);
> >    
> >        rdd2.collect().forEach(System.out::println);
> >    }
> >    ```
> >
> >  +++

##### mapToPair()

> 生成 K-V 形式的RDD对象 `JavaPairRDD`
>
> 
>
> ```java
> JavaPairRDD<String, Integer> rdd2 = rdd1.mapToPair(new PairFunction<String, String, Integer>() {
>     @Override
>     public Tuple2<String, Integer> call(String s) throws Exception {
>         int value = new Random().nextInt(10);
>         System.out.println(s + "---" + value);
> 
>         return new Tuple2(s, value);
>     }
> });
> ```

##### mapValues() 

> mapValues() : 一对一映射【原RDD一个元素的value值计算得到新RDD一个元素的value值,key保存不变】
>
> >  + 针对是元素的value值进行计算
> >
> >  + 生成的新RDD元素个数 = 原RDD元素个数
> >
> >    ```java
> >    /**
> >     * mapValues: 一对一转换[ 原RDD一个元素的value值计算得到新RDD一个元素的value值,计算过程中,key不变 ]
> >     *      mapValues中接口的方法是针对每个元素的value值操作。
> >     */
> >    @Test
> >    public void mapValues(){
> >    
> >        //创建sparkcontext
> >        SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >        JavaSparkContext sc = new JavaSparkContext(conf);
> >    
> >        JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("hello", "spark", "hadoop", "flume", "kafka"));
> >    
> >        JavaPairRDD<String, Integer> rdd2 = rdd1.mapToPair(new PairFunction<String, String, Integer>() {
> >            @Override
> >            public Tuple2<String, Integer> call(String s) throws Exception {
> >                int value = new Random().nextInt(100);
> >                System.out.println(s+"---"+value);
> >                return new Tuple2(s,value);
> >            }
> >        });
> >    
> >        //JavaPairRDD<String, Integer> rdd3 = rdd2.mapValues(new Function<Integer, Integer>() {
> >        //    @Override
> >        //    public Integer call(Integer v1) throws Exception {
> >        //        return v1 * 10;
> >        //    }
> >        //});
> >    
> >        //自己实现mapValues
> >        JavaPairRDD<String, Integer> rdd3 = rdd2.mapToPair(new PairFunction<Tuple2<String, Integer>, String, Integer>() {
> >            @Override
> >            public Tuple2<String, Integer> call(Tuple2<String, Integer> kv) throws Exception {
> >                return new Tuple2(kv._1, kv._2 * 10);
> >            }
> >        });
> >    
> >        System.out.println(rdd3.collect());
> >    }
> >    ```
> >
> >  +++

##### groupByKey()

> groupByKey : 根据元素的key对元素进行分组
>
> >  + 生成的新RDD元素类型 ：K-V 键值对
> >
> >  + K就是分组的key[元素的key]
> >
> >  + V是集合,集合中装的是key对应原RDD所有的value值
> >
> >    ```java
> >    /**
> >     * groupByKey： 按照key分组
> >     *      groupByKey生成的新RDD中元素类型是KV键值对,K是分组的key,V是key对应原RDD的所有的value值集合
> >     */
> >    @Test
> >    public void groupByKey(){
> >        //创建sparkcontext
> >        SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >        JavaSparkContext sc = new JavaSparkContext(conf);
> >    
> >        JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("hello", "spark", "hadoop", "flume", "kafka","spark","hadoop","hadoop","spark"));
> >    
> >        JavaPairRDD<String, Integer> rdd2 = rdd1.mapToPair(new PairFunction<String, String, Integer>() {
> >            @Override
> >            public Tuple2<String, Integer> call(String s) throws Exception {
> >                int value = new Random().nextInt(100);
> >                System.out.println(s+"---"+value);
> >                return new Tuple2(s,value);
> >            }
> >        });
> >    
> >        //JavaPairRDD<String, Iterable<Integer>> rdd3 = rdd2.groupByKey();
> >    
> >        //使用groupBy实现groupByKey功能
> >        JavaPairRDD<String, Iterable<Tuple2<String, Integer>>> rdd3 = rdd2.groupBy(new Function<Tuple2<String, Integer>, String>() {
> >            @Override
> >            public String call(Tuple2<String, Integer> v1) throws Exception {
> >                return v1._1;
> >            }
> >        });
> >    
> >        JavaPairRDD<String, List<Integer>> rdd4 = rdd3.mapToPair(new PairFunction<Tuple2<String, Iterable<Tuple2<String, Integer>>>, String, List<Integer>>() {
> >            @Override
> >            public Tuple2<String, List<Integer>> call(Tuple2<String, Iterable<Tuple2<String, Integer>>> kv) throws Exception {
> >                List<Integer> result = new ArrayList<>();
> >                for (Tuple2<String, Integer> t2 : kv._2) {
> >                    result.add(t2._2);
> >                }
> >                return new Tuple2(kv._1, result);
> >            }
> >        });
> >        System.out.println(rdd4.collect());
> >    }
> >    ```
> >
> >  +++
> >
>

##### reduceByKey()【重要】

> reduceByKey : 按照key分组,然后对每个组的所有的value值聚合
>
> >  + 针对每个组进行聚合，call()方法两个参数
> >  + 第一个参数：计算的中间变量【每组的第一次计算时，取第一个和第二个元素，只有一个元素的不会执行call()方法】
> >  + 第二个参数：当前待聚合的元素
> >
> >  +++
> >
> >  ```txt
> >  groupByKey与redueByKey的区别:
> >  	groupByKey: 只是单纯的分组没有聚合,没有combiner预聚合。
> >  	reduceByKey: 分组+聚合,在shuffle过程中有combiner预聚合过程的,性能更高。
> >  ```
> >
>
> 案例
>
> > ```java
> > /**
> >  * reduceByKey: 先根据key分组,然后key所有的value值聚合
> >  *      reduceByKey的计算逻辑: (当前待聚合的元素,该组上一次的聚合结果) => 聚合逻辑
> >  * groupByKey与reduceByKey的区别:
> >  *      groupByKey: 只是单纯的分组,没有combiner预聚合操作
> >  *      reduceByKey: 即分组又聚合,有combiner预聚合操作,reduceByKey的性能高更高。工作中推荐使用reduceByKey
> >  */
> > @Test
> > public void reduceByKey(){
> >     //创建sparkcontext
> >     SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >     JavaSparkContext sc = new JavaSparkContext(conf);
> > 
> >     JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("hello", "spark", "hadoop", "flume", "kafka","spark","hadoop","hadoop","spark"));
> > 
> >     JavaPairRDD<String, Integer> rdd2 = rdd1.mapToPair(new PairFunction<String, String, Integer>() {
> >         @Override
> >         public Tuple2<String, Integer> call(String s) throws Exception {
> >             int value = new Random().nextInt(10);
> >             System.out.println(s + "---" + value);
> > 
> >             return new Tuple2(s, value);
> >         }
> >     });
> > 	/**
> > 	* T1:输入1；T2:输入2；T3:输出result类型
> > 	*/
> >     JavaPairRDD<String, Integer> rdd3 = rdd2.reduceByKey(new Function2<Integer, Integer, Integer>() {
> >         /**
> >          *
> >          * @param agg 临时变量,上一次的聚合结果
> >          * @param curr 是当前待聚合的元素
> >          * @return
> >          * @throws Exception
> >          */
> >         @Override
> >         public Integer call(Integer curr, Integer agg) throws Exception {
> >             System.out.println("agg=" + agg + " ,curr=" + curr);
> >             return agg + curr;
> >         }
> >     });
> > 
> >     System.out.println(rdd3.collect());
> > }
> > ```
>
> +++

+++

##### sortByKey()

> sortByKey : 根据key对元素进行排序
>
> ```java
> /**
>  * 根据key对元素排序
>  */
> @Test
> public void sortByKey(){
>     //创建sparkcontext
>     SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>     JavaSparkContext sc = new JavaSparkContext(conf);
>     JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 3, 6, 8, 2, 9, 4, 7, 3, 10));
> 
>     JavaPairRDD<Integer, String> rdd2 = rdd1.mapToPair(new PairFunction<Integer, Integer, String>() {
>         @Override
>         public Tuple2<Integer, String> call(Integer x) throws Exception {
>             return new Tuple2<>(x, null);
>         }
>     });
>     JavaPairRDD<Integer, String> rdd3 = rdd2.sortByKey(false);
>     JavaRDD<Integer> rdd4 = rdd3.map(new Function<Tuple2<Integer, String>, Integer>() {
>         @Override
>         public Integer call(Tuple2<Integer, String> v1) throws Exception {
>             return v1._1;
>         }
>     });
>     //TODO 使用sortBy实现排序
>     JavaRDD<Integer> rdd5 = rdd1.sortBy(new Function<Integer, Integer>() {
>         @Override
>         public Integer call(Integer v1) throws Exception {
>             return v1;
>         }
>     }, false, 4);
>     System.out.println(rdd4.collect());
> }
> ```
>
> 

##### 修改分区

> coalesce(num[,shuffle=false]) : 合并分区数
>
> >  + 默认只能减少分区，没有shuffle操作
> >  + 设置shuffle=true,此时一定会产生shuffle,可以增大和减少分区
> >  + 一般用于减少分区，和filter搭配使用
> >
> >  +++
>
> repartition : 既可以增大分区，又可以减少分区，都会有shuffle
>
> >  + shuffle[因为底层其实就是coalesce(分区数,shuffle=true)]
> >  + 一般用于增大分区数,当数据量膨胀的时候为了提高计算的效率,需要增大分区数,减少每个分区处理的数据量
> >
> >  +++
>
> 案例
>
> >  ```java
> >  /**
> >   *  coalesce: 合并分区
> >   *         coalesce默认只能合并分区数,此时没有shuffle操作的
> >   *         如果想要增大分区数,此时可以可以设置shuffle=true，此时有shuffle操作。
> >   *  repartition其实就是coalesce(分区数,shuffle=true)
> >   *  工作中coalesce一般是搭配filter使用,用于减少分区数
> >   *  工作中如果数据量膨胀了可以使用repartition增大分区数,减少每个分区处理的数据量提高运行效率
> >   */
> >  @Test
> >  public void coalesce() throws InterruptedException {
> >      //创建sparkcontext
> >      SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >      JavaSparkContext sc = new JavaSparkContext(conf);
> >  
> >      JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 4, 3, 7, 9, 10, 2, 10));
> >  
> >      rdd1.map(new Function<Integer, Object>() {
> >          @Override
> >          public Object call(Integer v1) throws Exception {
> >              System.out.println(Thread.currentThread().getName()+"----"+v1);
> >              return null;
> >          }
> >      }).collect();
> >      System.out.println("--------------------------------");
> >      //TODO 没有设置shuffle参数,合并分区数,没有shuffle操作
> >      //JavaRDD<Integer> rdd2 = rdd1.coalesce(3);
> >      //TODO 增大分区数,会产生shuffle操作,必须设置shuffle=true
> >      //JavaRDD<Integer> rdd2 = rdd1.coalesce(6,true);
> >      //TODO repartition既可以增大分区数也可以减少分区数
> >      JavaRDD<Integer> rdd2 = rdd1.repartition(6);
> >      rdd2.map(new Function<Integer, Object>() {
> >          @Override
> >          public Object call(Integer v1) throws Exception {
> >              System.out.println(Thread.currentThread().getName()+"----"+v1);
> >              return null;
> >          }
> >      }).collect();
> >      System.out.println(rdd2.getNumPartitions());
> >  
> >      Thread.sleep(10000000);
> >  }
> >  
> >  ```
> >
> >  ![image-20220923162114227](http://ybll.vip/md-imgs/202209231621359.png)
> >
> >  + 第一个stage：collect()，划分为1个stage，没有shuffle操作
> >  + 第二个stage：repartition(6),分区数增多，有shuffle写操作
> >  + 第三个stage：collect(),划分1个stage,没有shuffle写操作，直接从repartition的shuffle写文件读取数据
>
> +++

+++

#### 行动算子

##### collect()

> 收集RDD所有分区的数据，以集合的形式封装后发送给Driver
> 
> > + Driver内存默认只有1G,若RDD的数据量较大,发送给Driver时可能出现内存溢出。
> >+ 工作中一般将Driver内存设置为5-10G左右
> > + 可以通过  `bin/spark-submit --driver-memory 5G`设置。
> > 
> > +++

##### first()

> ```java
> /**
>  * first: 获取RDD第一个元素
>  *      first首先会启动一个job获取0号分区第一个元素,如果0号分区没有数据,此时会再启动一个job从其他分区获取数据
>  */
> @Test
> public void first() throws InterruptedException {
>     //创建sparkcontext
>     SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>     JavaSparkContext sc = new JavaSparkContext(conf);
> 
>     JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(2,3,4,6,8,9,12,16));
> 
>     JavaPairRDD<Integer, String> rdd2 = rdd1.mapToPair(new PairFunction<Integer, Integer, String>() {
> 
>         @Override
>         public Tuple2<Integer, String> call(Integer x) throws Exception {
> 
>             return new Tuple2(x, null);
>         }
>     });
> 
>     JavaPairRDD<Integer, Iterable<String>> rdd3 = rdd2.groupByKey(5);
> 
>     System.out.println(rdd3.first());
>     //System.out.println(rdd1.first());
> 
>     Thread.sleep(1000000);
> }
> ```

+++

##### take()

> 获取RDD 前N个元素
> 
> > + 会首先启动 `一个Job` 从0号分区获取前N个元素，如果0号分区没有N个数据，会再启动一个job从其他分区获取数据
> >
> > +++
> > 
> > ```java
> >/**
> >  * take: 获取RDD前N个元素。
> > *      take会首先启动一个job从0号分区获取前N个元素,0号分区如果不满N条数据,会再启动一个job从其他分区获取数据。
> >  */
> >@Test
> > public void take(){
> >    //创建sparkcontext
> >     SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> >    JavaSparkContext sc = new JavaSparkContext(conf);
> > 
> >    JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(2,3,4,6,8,9,12,16));
> > 
> >    List<Integer> result = rdd1.take(3);
> > 
> >    System.out.println(result);
> > }
> >```
> 
> +++
> 
> +++

##### countByKey()

> + 统计个每个key的个数
>
> ```java
> /**
>  * countByKey: 统计每个key的个数
>  *      countByKey一般用于数据倾斜场景,用于统计每个key个数,判断是哪个key导致的数据倾斜。
>  */
> @Test
> public void countByKey() throws InterruptedException {
>     //创建sparkcontext
>     SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>     JavaSparkContext sc = new JavaSparkContext(conf);
> 
>     JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("aa", "cc", "dd", "cc", "aa", "aa", "aa", "dd", "cc", "cc", "cc"));
> 
>     JavaPairRDD<String, String> rdd2 = rdd1.mapToPair(new PairFunction<String, String, String>() {
>         @Override
>         public Tuple2<String, String> call(String s) throws Exception {
>             return new Tuple2<>(s, null);
>         }
>     });
> 
>     Map<String, Long> result = rdd2.countByKey();
> }
> ```

+++

##### saveAsTextFile()

> + 将结果保存到文本文件
>
> ```java
> /**
>  * save: 将数据保存到文件
>  */
> @Test
> public void save(){
>     //创建sparkcontext
>     SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>     JavaSparkContext sc = new JavaSparkContext(conf);
> 
>     JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("aa", "cc", "dd", "cc", "aa", "aa", "aa", "dd", "cc", "cc", "cc"));
> 
>     rdd1.saveAsTextFile("output/text");
> }
> ```

+++

##### foreach()

>  对每个元素进行遍历
>
> + foreach里面的call方法是针对每个元素进行调用
>
> ```java
> rdd1.foreach(new VoidFunction<String>() {
>     @Override
>     public void call(String s) throws Exception {
>         //TODO 此处的打印是在task中针对每个分区的数据遍历打印
>         System.out.println(Thread.currentThread().getName()+"----"+s);
>     }
> });
> ```

> > +++

##### foreachPartition()

>  : 对每个分区进行遍历
>
> + 每个分区会调用一次 `call()` ,同一分区内的数据，在call()中自定义处理
> + 一般用于将数据保存到mysql\hbase\redis等位置,可以减少链接的创建与销毁的次数。
>
> ```java
> /**
>  *  foreachPartition: 是针对每个分区遍历
>  *          foreachPartition的call方法是一个分区调用一次
>  *          foreachPartition一般用于将数据保存到mysql/hbase/redis等存储介质中,可以减少链接创建与销毁的次数
>  */
> @Test
> public void foreachPartition() throws SQLException {
> 
>     //创建sparkcontext
>     final SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>     final JavaSparkContext sc = new JavaSparkContext(conf);
> 
>     //需求: 统计每门学科的平均分,并且将结果保存在mysql
> 
>     //1、读文件
>     JavaRDD<String> rdd1 = sc.textFile("datas/score.txt");
>     //2、对每一行切分,将数据转成  学科名称 -> Tuple2(分数,1)
>     JavaPairRDD<String, Tuple2<Integer, Integer>> rdd2 = rdd1.mapToPair(new PairFunction<String, String, Tuple2<Integer, Integer>>() {
> 
>         @Override
>         public Tuple2<String, Tuple2<Integer, Integer>> call(String line) throws Exception {
>             String[] arr = line.split(",");
>             int score = Integer.parseInt(arr[1]);
>             return new Tuple2(arr[0], new Tuple2(score, 1));
>         }
>     });
>     //RDD(
>     //   语文 -> Tuple2(100,1),
>     //   数学 -> Tuple2(80,1),
>     //   语文 -> Tuple2(70,1),
>     //   .....
>     // )
>     //3、按照学科分组,统计总分和总个数
>     JavaPairRDD<String, Tuple2<Integer, Integer>> rdd3 = rdd2.reduceByKey(new Function2<Tuple2<Integer, Integer>, Tuple2<Integer, Integer>, Tuple2<Integer, Integer>>() {
> 
>         @Override
>         public Tuple2<Integer, Integer> call(Tuple2<Integer, Integer> v1, Tuple2<Integer, Integer> v2) throws Exception {
>             return new Tuple2(v1._1 + v2._1, v1._2 + v2._2);
>         }
>     });
>     //RDD(
>     //      语文 -> Tuple2(总分,总个数)
>     //      数学 -> Tuple2(总分,总个数)
>     //      英语 -> Tuple2(总分,总个数)
>     // )
>     //4、计算每个学科的平均分
>     JavaPairRDD<String, Double> rdd4 = rdd3.mapValues(new Function<Tuple2<Integer, Integer>, Double>() {
> 
>         @Override
>         public Double call(Tuple2<Integer, Integer> v1) throws Exception {
>             return v1._1.doubleValue() / v1._2;
>         }
>     });
>   
>     rdd4.foreachPartition(new VoidFunction<Iterator<Tuple2<String, Double>>>() {
>         /**
>          *
>          * @param pit 是一个分区所有数据封装的迭代器
>          * @throws Exception
>          */
>         @Override
>         public void call(Iterator<Tuple2<String, Double>> pit) throws Exception {
> 
>             Connection connection = null;
>             PreparedStatement statement = null;
>             try{
>                 connection = DriverManager.getConnection("jdbc:mysql://hadoop102:3306/test?useUnicode=true&characterEncoding=UTF-8","root","1234");
>                 statement = connection.prepareStatement("insert into score values(?,?)");
>                 int i = 0;
>                 //3、给sql语句赋值
>                 while(pit.hasNext()){
>                     Tuple2<String, Double> avgscore = pit.next();
>                     statement.setString(1,avgscore._1);
>                     statement.setDouble(2,avgscore._2);
>                     //4、将当前sql添加到一个批次中
>                     statement.addBatch();
>                     i+=1;
>                     if(i%1000==0){
>                         //提交一个批次所有sql
>                         statement.executeBatch();
>                         statement.clearBatch();
>                     }
>                 }
>                 //提交最后一个不满1000条数据的批次
>                 statement.executeBatch();
>             }catch (Exception e){
>                 e.printStackTrace();
>             }finally {
>                 //5、关闭资源链接
>                 if(statement!=null) {
>                     statement.close();
>                 }
>                 if(connection!=null) {
>                     connection.close();
>                 }
>             }
>         }
>     });
> }
> ```

> +++

+++



#### RDD序列化

> 1. 为什么要序列化?
> 	+ **RDD算子里面的call方法体中的逻辑是在Task中执行的,call方法体外面的逻辑是在Driver执行的**
> 	+ **所以如果 call() 中有用到方法体外面的对象,此时spark会将Driver对象序列化之后传递给task使用,此时必须要求该Driver对象要能够序列化[继承了序列化接口]**
> 	
> 2. spark的序列化方式有两种: ` Java序列化[默认]`、`Kryo序列化`
>
>     + Kryo序列化的性能比java序列化性能要高10倍左右,工作中推荐使用Kryo序列化
>
> 3. 如何设置spark的序列化方式
>
>     `.set("spark.serializer","org.apache.spark.serializer.KryoSerializer");`
>
>     `.registerKryoClasses(new Class[] {【需要序列化类的class对象】})`
>
> ```java
> /**
>      * 为什么要序列化: call方法体中的代码是执行在task中的,方法体外面的代码是执行在Driver中的,当方法体中使用了方法体外面的对象,此时spark会将Driver对象序列化之后传递给task使用,所以要求该Driver对象必须要能够序列化才能。
>      * spark的序列化方式有两种:
>      *      Java序列化: 将类的信息、类的继承信息、属性类型、属性值、全类名等都会序列化进去[spark默认的序列化方式]
>      *      Kryo序列化: 只会序列化类的属性信息、全类名,性能比Java序列化要高10倍左右<工作中推荐使用Kryo序列化>
>      *设置spark默认的序列化方式
>      *      1、在sparkconf中设置序列化方式: new SparkConf().set("spark.serializer","org.apache.spark.serializer.KryoSerializer")
>      */
>     public static void main(String[] args) {
> 
>         //创建sparkcontext
>         SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
> //                .set("spark.serializer","org.apache.spark.serializer.KryoSerializer");
>                 //.registerKryoClasses(new Class[]{ Person.class ,Student.class });
>         JavaSparkContext sc = new JavaSparkContext(conf);
>         JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 4, 7, 3, 9, 20, 30, 10));
> 
>         //Integer a = 10;
> 
>         //TODO 报错: person是在Driver中定义的,是在task使用的,所以会将Person序列化传递给task使用,但是person对象没有继承序列化接口不能序列化
>         //Person person = new Person();
> //
>         //JavaRDD<Integer> rdd2 = rdd1.map(new Function<Integer, Integer>() {
>         //    @Override
>         //    public Integer call(Integer x) throws Exception {
>         //        return x * person.b;
>         //    }
>         //});
> 
>         //TODO 报错: m1方法中用到了this.b, this指代person对象, 需要将person序列化传给task
>         Person person = new Person();
>         Student student = new Student();
> //        JavaRDD<Integer> rdd2 = person.m1(rdd1);
> 
>         //TODO 不报错: 只是在Driver中用到了person对象
> //        JavaRDD<Integer> rdd2 = student.m2(rdd1,person);
>         
>         //TODO 不报错: person对象在task中创建,在task中使用,不需要通过网络传递。
>         JavaRDD<Integer> rdd2 = rdd1.map(new Function<Integer, Integer>() {
>             @Override
>             public Integer call(Integer v1) throws Exception {
>                 return v1 * new Person().b;
>             }
>         });
>         System.out.println(rdd2.collect());
>     }
> ```

#### RDD依赖关系

> 1. 查看血统:  `toDebugString()`
>
> 2. RDD的依赖
>
>    + 宽依赖: 有shuffle的称之为宽依赖
>
>    + 窄依赖: 没有shuffle的称之为窄依赖
>
>      +++
>
>    + Application: 应用，[一个SparkContext称之为一个Application]
>    + Job: 任务，[一个Action算子一般会产生一个job, `first()` `take()`可能产生两个]
>    + Stage: 阶段，[**一个job中stage的个数 = 宽依赖个数(shuffle个数)+ 1**]
>    + Task: 子任务，[**一个stage中task的个数 = 该stage种最后一个RDD的分区数**]
>
> 3. stage切分: 
>
>    + 由最后一个调用Action算子的RDD的依赖关系从后向前依次查询,一直查询到job第一个RDD为止,
>    + 在查询的过程中会判断依赖关系是否为宽依赖,如果是则切分stage
>
> +++
>
> ```java
> public class $03_Dependencys {
> 
>     /**
>      * 血统: 从第一个RDD到当前RDD的依赖链条[toDebugString查看RDD的血统]
>      *
>      * 依赖: 父子RDD关系称之为依赖
>      * 依赖分为两种:
>      *          宽依赖: 有shuffle的称之为宽依赖
>      *          窄依赖: 没有shuffle的称之为窄依赖
>      *Application: 一个sparkcontext称之为一个Application
>      *      Job: 任务[一个action算子一般会产生一个job]
>      *          Stage: 阶段[一个job中stage的个数 = 宽依赖个数 + 1]
>      *              Task: 子任务[一个stage中task的个数 = 该stage种最后一个RDD的分区数]
>      *stage的切分过程:  根据最后一个调用action算子的RDD的依赖关系从后往前依次查询,一直查询到第一个RDD位置,在查询的过程中遇到宽依赖则切分stage.
>      *
>      */
>     public static void main(String[] args) throws InterruptedException {
> 
>         //创建sparkcontext
>         SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>         JavaSparkContext sc = new JavaSparkContext(conf);
> 
>         //1、读取数据
>         JavaRDD<String> rdd1 = sc.textFile("spark/datas/wc.txt");
>         //RDD( "hello java spark hadoop","flume kafka flume hadoop","spark hadoop hadoop","java flume hadoop")
>         System.out.println("---------------------------------------------------");
>         System.out.println(rdd1.toDebugString());
>         //2、切割
>         JavaRDD<String> rdd2 = rdd1.flatMap(new FlatMapFunction<String, String>() {
>             @Override
>             public Iterator<String> call(String s) throws Exception {
>                 return Arrays.asList(s.split(" ")).iterator();
>             }
>         });
>         System.out.println("---------------------------------------------------");
>         System.out.println(rdd2.toDebugString());
>         //RDD( hello,java,spark,hadoop,flume,kafka,flume,hadoop,spark,hadoop,hadoop,java,flume,hadoop)
>         //3、按照单词分组
>         JavaPairRDD<String, Iterable<String>> rdd3 = rdd2.groupBy(new Function<String, String>() {
>             @Override
>             public String call(String v1) throws Exception {
>                 return v1;
>             }
>         });
>         System.out.println("---------------------------------------------------");
>         System.out.println(rdd3.toDebugString());
>         //RDD(
>         //    hello ->  Iterable( hello ) ,
>         //    hadoop -> Iterable( hadoop,hadoop,hadoop,hadooop,hadoop),
>         //   ....
>         // )
>         //4、统计每个单词的个数
>         JavaRDD<Tuple2<String, Integer>> rdd4 = rdd3.map(new Function<Tuple2<String, Iterable<String>>, 
>                                                          Tuple2<String, Integer>>() {
>             @Override
>             public Tuple2<String, Integer> call(Tuple2<String, Iterable<String>> v1) throws Exception {
>                 //v1 = hello ->  Iterable( hello )
>                 return new Tuple2(v1._1, IteratorUtils.toList(v1._2.iterator()).size());
>             }
>         });
>         System.out.println("---------------------------------------------------");
>         System.out.println(rdd4.toDebugString());
>         System.out.println("---------------------------------------------------");
>         JavaRDD<Tuple2<String, Integer>> rdd5 = rdd4.coalesce(1);
>         System.out.println(rdd5.collect());
> 
>         Thread.sleep(10000000);
>     }
> }
> ```

#### RDD持久化

> 1. 使用场景与使用原因
>    + 一个RDD在多个job中重复使用
>      + 默认情况下,该RDD在每个job执行的时候都会对数据进行重复处理
>      + 此时可以将该RDD的数据保存下来,第一个Job处理后，其他job就不会再数据重复处理了。
>    + 一个JOB的数据处理链条特别长
>      + 依赖连接特别长,如果其中某个RDD出现数据丢失,需要从头开始计算
>      + 此时可以将其中某个RDD数据保存下来,后续丢失数据就从该RDD开始拿到数据重新计算,不需要从头计算了。
>
> 2. 持久化方式
>
>    + RDD的持久化方式有两种:
>
>      > +  缓存
>      >
>      >   1. 数据保存位置： 保存在Task所在主机的内存/磁盘中
>      >
>      >   2. 数据保存时机: 在缓存RDD所在第一个job执行过程中保存的
>      >
>      >   3. 使用方式: `rdd.cache()` / `rdd.persist(存储级别)`
>      >
>      >      + cache与persist的区别
>      >        	cache只能将数据保存在内存中
>      >          	persist可以通过存储级别指定将数据保存在内存/磁盘中
>      >
>      >      + 常用的存储级别:
>      >
>      >        StorageLevel.MEMORY_ONLY：将数据仅保存在内存中[一般用于小数据量场景]
>      >        StorageLevel.MEMORY_AND_DISK：将数据一部分保存在内存,一部分保存在磁盘**[一般用于大数据量场景]**
>      >
>      > + checkpoint
>      >   1. 原因: 缓存是将数据保存在服务器内存/本地磁盘中,如果服务器宕机数据丢失,又需要重新计算得到数据。
>      >   2. 数据保存位置: HDFS
>      >   3. 数据保存时机: 在checkpoint rdd所在第一个job完成之后,会单独触发一个job计算得到数据之后保存。
>      >   4. 使用方式: 
>      >      + 指定数据的保存位置: sc.setCheckpointDir(path)
>      >      + 缓存数据: rdd.checkpoint
>      > + 缓存与checkpoint的区别:
>      >   1. 数据保存位置不一样
>      >      + 缓存是将数据保存在服务器内存/本地磁盘
>      >      + checkpoint是将数据保存在HDFS中
>      >   2. 数据保存时机不一样
>      >      + 缓存是在缓存RDD所在第一个job执行过程中保存的
>      >      + checkpoint是在checkpoint rdd所在第一个job完成之后,会单独触发一个job计算得到数据之后保存。
>      >   3. 依赖关系是否切除不一样
>      >      + 缓存是将数据保存在服务器内存/本地磁盘中,服务器宕机数据丢失,丢失之后需要根据依赖关系重新计算得到数据,此时RDD的依赖关系不会切除
>      >      + checkpoint是将数据保存在HDFS中,数据不会丢失,所以为了节省内存空间,将RDD的依赖关系切除了。
>
>      +++
>
>    + 一般使用两者的结合
>
>      > checkpoint会单独触发一个Job执行，为了避免该job重复计算数据,
>      >
>      > 一般会结合缓存使用: rdd.cache + rdd.checkpoint
>      >
>      > ```java
>      > //创建sparkcontext
>      > SparkConf conf = 
>      >     new SparkConf().setMaster("local[4]").setAppName("test");
>      > JavaSparkContext sc = new JavaSparkContext(conf);
>      > 
>      > //TODO 指定checkpoint数据保存位置
>      > sc.setCheckpointDir("checkpoint");
>      > ...... //rdd1, rdd2
>      > // 缓存数据
>      > rdd2.cache();
>      > rdd2.persist(StorageLevel.MEMORY_ONLY());
>      > 
>      > //TODO checkpoint保存数据
>      > rdd2.checkpoint();
>      > 
>      > //checkpoint启动Job缓存数据时，会直接从cache中读取，而不用重新计算数据
>      > ```
>      >
>      > +++
>
> +++

#### 分区器

> spark分区器分为两种:
>
> + HashPartitioner 分区规则：
>   **`key.hashCode % 分区数 < 0 ? key.hashCode % 分区数 + 分区数 : key.hashCode % 分区数`**
> + RangePartitioner
>   分区规则:
>   1. 对RDD数据进行抽样,确定 N-1 个值
>   2. 通过 N-1 个值确定N个分区的边界
>   3. 后续将数据的key与分区的边界对比,如果key处于分区边界内则数据放入该分区中。

### 3. 累加器 Accumulator

> + 原理: 先在每个分区中对数据进行累加,然后将每个分区的累加结果发送给Driver进行汇总
> + 场景: 一般用于聚合场景并且聚合结果不能太大
>   + 原因：会将分区的结果发送给Driver,如果结果很大,Driver可能会内存溢出
> + 好处: 可以一定程度上避免shuffle操作。
> + 工作中常用的是spark自带的集合累加器。
> + 使用:
>   1. 获取spark自带的累加器
>      + CollectionAccumulator Cacc =  sc.sc().CollectionAccumulator<集合类型>（）
>      + LongAccumulator Lacc = sc.sc().longAccumulator
>      + 集合累加器使用多
>   2. 在分区中累加数据**【一般在action算子中使用 [call()中使用 ]】**: acc.add(集合)
>   3. Driver获取结果: `acc.value()`
>
> +++
>
> ```java
> public class $02_Accumulator{
>     /**
>      * 累加器原理: 首先会先在每个分区中累加,然后将每个分区的累加结果发给Driver进行汇总
>      * 场景:  一般用于聚合场景而且聚合结果不能太大。
>      * 累加器的好处: 能够一定程度上减少shuffle操作
>      */
>     public static void main(String[] args) {
> 
>         //创建sparkcontext
>         SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>         JavaSparkContext sc = new JavaSparkContext(conf);
> 
>         //创建一个Long类型的累加器对象
>         LongAccumulator acc = sc.sc().longAccumulator();
>         // SparkContext sc1 = sc.sc();
> 
>         JavaRDD<Integer> rdd1 = sc.parallelize(
>             Arrays.asList(10, 30, 20, 40, 50, 60), 2);
> 
>         rdd1.foreach(new VoidFunction<Integer>() {
>             @Override
>             public void call(Integer x) throws Exception {
>                 acc.add(x);
>             }
>         });
>         System.out.println(acc.value()); //210
>     }
> }
> ```
>
> +++

### 4. 广播变量 Broadcast

> + 场景
>   1. task使用Driver数据,并且该数据有一定大小[有个上兆大小]
>      + spark算子call方法体中的代码是在task中执行的,call方法体外面的代码是在Driver执行的,此时如果方法体中用到了方法体外面的数据[task用到了driver数据]
>      + 此时,spark会将该Driver数据给所有的task都发送一份,所以数据占用的总内存空间 = task个数 * 数据大小,会占用过多的内存空间。
>      + 好处: 此时可以将该Driver数据广播给Executor,后续Executor中的多个task共用一份数据,所以此时数据占用的总内存空间 = executor个数 * 数据大小,节省了内存空间的占用
>   2. 大表 join 小表
>      + 默认情况下会产生shuffle操作
>      + 好处: 可以将小表广播给Executor,后续大表的task就可以直接拿到小表数据进行map join,不需要shuffle.
> + 使用：
>   1. 广播数据: BroadCast<数据类型> bc = sc.broadcast(数据)
>   2. task使用数据: bc.value
>
> +++
>
> ```java
> public class $04_Broadcast {
>     /**
>      * 广播变量使用场景
>      *  1、在spark算子call方法体中使用到call方法体外面的对象,该对象有一定的大小[有几十上百兆]
>      *     好处: 默认情况下,该对象占用的内存大小 = task个数 * 数据大小
>                 广播给executor之后,占用内存大小 = executor个数 * 数据大小
>                 节省了内存空间的的占用。
>                 
>      *  2、大表 join 小表[sparksql验证]
>      *     好处: 大表join小表的时候会产生shuffle操作,此时可以将小表的数据广播出去,
>                  先map端join,避免shuffle操作
>                  
>                  
>      *广播变量的使用:
>      *      1、广播数据: BroadCast<广播数据类型> bc = sc.broadcast(数据)
>      *      2、task取出广播数据使用: bc.value
>      *
>      */
>     public static void main(String[] args) throws InterruptedException {
> 
>         //创建sparkcontext
>         SparkConf conf = new SparkConf().setMaster("local[4]").setAppName("test");
>         JavaSparkContext sc = new JavaSparkContext(conf);
> 
>         JavaRDD<String> rdd1 = sc.parallelize(
>             Arrays.asList("jd", "atguigu", "tm", "pdd"));
> 
>         Map<String,String> map = new HashMap<String,String>();
>         map.put("jd","www.jd.com");
>         map.put("atguigu","www.atguigu.com");
>         map.put("pdd","www.pdd.com");
>         map.put("tm","www.tm.com");
> 
>         //TODO 广播数据
>         Broadcast<Map<String, String>> bc = sc.broadcast(map);
> 
>         JavaRDD<String> rdd2 = rdd1.map(new Function<String, String>() {
>             @Override
>             public String call(String v1) throws Exception {
>                 //TODO task取出executor广播数据使用
>                 Map<String, String> bMap = bc.value();
>                 return bMap.get(v1);
>             }
>         });
>         System.out.println(rdd2.collect());
>         Thread.sleep(100000);
>     }
> }
> ```

### 5. SparkCore案例

> 

## SparkSQL

### 1. 基础部分

```java
public class $02_SparkSQL {

    public static void main(String[] args) {

        SparkSession spark = SparkSession.builder().master("local[4]").appName("test").getOrCreate();

        Dataset<Row> ds = spark.read().json("datas/person.json");

        //TODO 将数据注册成表
        ds.createOrReplaceTempView("person");

        //TODO sql方式编程
        spark.sql("select name,concat(name,'_',age) name_and_age from person where age>=25").show();

        //TODO DSL方式编程
        ds.where("age>=25").show();
        ds.selectExpr("name","concat(name,'_',age) name_and_age").show();
    }
}
```

### 2. UDF | UDAF

> UDF
>
> ```java
> public class $03_UserDeinedUDF {
> 
>     /**
>      * 自定义UDF函数
>      *          1、导入sparksql函数:
>      *                  import static org.apache.spark.sql.functions.*
>      *          2、使用udf方法自定义udf函数
>      *          3、注册: spark.udf().register("函数名",自定义udf函数对象)
>      *          4、使用: spark.sql(".....")
>      *
>      */
>     public static void main(String[] args) {
> 
>         SparkSession spark = SparkSession.builder().master("local[4]").appName("test").getOrCreate();
> 
>         Dataset<String> ds = spark.read().textFile("datas/test.txt");
> 
>         //ds.show();
>         //需求: 将员工id补全8位,在左侧以0补齐
>         //TODO 注册成表
>         ds.createOrReplaceTempView("person");
>         //TODO 自定义函数
>         // UDF3里面3代表函数有三个参数
>         // 第一个泛型代表函数第一个参数类型
>         // 第二个泛型代表函数第二个参数类型
>         // 第三个泛型代表函数第三个参数类型
>         // 第四个泛型代表函数返回值类型
>         // DataTypes.StringType指定返回值类型的序列化方式
>         UserDefinedFunction obj =  udf(new UDF3<String, Integer, String, String>() {
>             @Override
>             public String call(String value, Integer len, String pad) throws Exception {
>                 //获取id需要补齐的长度
>                 int padLen = len - value.length();
> 
>                 //补齐长度
>                 for(int i=1;i<=padLen;i++){
>                     value = pad + value;
>                 }
>                 return value;
>             }
>         }, DataTypes.StringType);
>         //TODO 将函数注册
>         spark.udf().register("myPad",obj);
>         //spark.sql("select value, lpad(value,8,'0') from person").show();
>         spark.sql("select value, myPad(value,8,'0') from person").show();
>     }
> }
> ```
>
> +++
>
> UDAF
>
> ```java
> /**
>  * 自定义UDAF函数
>  *      1、创建一个类继承Aggregator<IN,BUFF,OUT>
>  *                  IN: 代表聚合函数的参数类型
>  *                  BUFF: 代表聚合过程中中间变量类型
>  *                  OUT: 代表聚合函数最终的结果值类型
>  *      2、重写抽象方法
>  *      3、导入sparksql函数:
>  *          import static org.apache.spark.sql.functions.*
>  *      4、通过udaf方法转换自定义UDAF函数对象
>  *      5、注册
>  */
> //自定义中间变量BUFF类：
> public class AvgBuff implements Serializable {
>     //学科总分
>     private Integer sum;
>     //学科成绩个数
>     private Integer count;
>     //get set 方法
> }
> 
> //自定义UDAF函数：自定义类继承 Aggregator类，实现抽象方法
> public class AvgUDAF extends Aggregator<Integer,AvgBuff,Double> {
> 
>     /**
>      *  给中间变量赋予初始值
>      */
>     @Override
>     public AvgBuff zero() {
>         return new AvgBuff(0,0);
>     }
> 
>     /**
>      * combiner预聚合
>      * @param tmp 中间变量
>      * @param score 此次函数调用时的输入对象
>      * @return
>      */
>     @Override
>     public AvgBuff reduce(AvgBuff tmp, Integer score) {
> 
>         return new AvgBuff( tmp.getSum() + score, tmp.getCount() + 1 );
>     }
> 
>     /**
>      * reducer聚合
>      * @param b1 
>      * @param b2
>      * @return
>      */
>     @Override
>     public AvgBuff merge(AvgBuff b1, AvgBuff b2) {
> 
>         return new AvgBuff( b1.getSum() + b2.getSum() , b1.getCount() + b2.getCount() );
>     }
> 
>     /**
>      * 通过中间变量计算最终结果
>      * @param reduction
>      * @return
>      */
>     @Override
>     public Double finish(AvgBuff reduction) {
>         return reduction.getSum().doubleValue() / reduction.getCount();
>     }
> 
>     /**
>      * 指定中间变量类型的序列化方式
>      * @return
>      */
>     @Override
>     public Encoder<AvgBuff> bufferEncoder() {
>         return Encoders.bean(AvgBuff.class);
>     }
> 
>     /**
>      * 指定最终结果的序列化方式
>      * @return
>      */
>     @Override
>     public Encoder<Double> outputEncoder() {
>         return Encoders.DOUBLE();
>     }
> }
> 
> //使用自定义函数
> public class $04_UserDefinedUDAF {
> 
>     public static void main(String[] args) {
> 
>         SparkSession spark = SparkSession.builder().master("local[4]").appName("test").getOrCreate();
> 
>         Dataset<Row> ds = spark.read().option("sep", ",").csv("datas/score.txt");
> 
>         //TODO 可以通过toDF方法重定义列名[注意: 列名的个数必须与列的个数一致]
>         Dataset<Row> ds2 = ds.toDF("name", "score");
> 
>         ds2.createOrReplaceTempView("student");
> 
>         spark.sql("select name,avg(score) from student group by name").show();
> 
>         //TODO 通过udaf方法转换自定义UDAF函数对象
>         //TODO Encoders.INT() 指定udaf参数的序列化方式
>         UserDefinedFunction uf = udaf(new AvgUDAF(), Encoders.INT());
>         //TODO 注册
>         spark.udf().register("myAvg",uf);
>         
>         //使用自定义udaf函数
>         spark.sql("select name,myAvg(score) from student group by name").show();
>     }
> }
> ```

### 3. ReadData



### 4. WriteData



### 5. Spark SQL案例

> 需求：各区域热门商品 top3
>
> ![image-20220804221615692](http://ybll.vip/md-imgs/202208042216799.png)
>
> +++
>
> **数据准备**
>
> ```sql
> CREATE TABLE `user_visit_action`(
>   `date` string,
>   `user_id` bigint,
>   `session_id` string,
>   `page_id` bigint,
>   `action_time` string,
>   `search_keyword` string,
>   `click_category_id` bigint,
>   `click_product_id` bigint, --点击商品id，没有商品用-1表示。
>   `order_category_ids` string,
>   `order_product_ids` string,
>   `pay_category_ids` string,
>   `pay_product_ids` string,
>   `city_id` bigint --城市id
> )
> row format delimited fields terminated by '\t';
> 
> 
> CREATE TABLE `city_info`(
>   `city_id` bigint, --城市id
>   `city_name` string, --城市名称
>   `area` string --区域名称
> )
> row format delimited fields terminated by '\t';
> 
> 
> CREATE TABLE `product_info`(
>   `product_id` bigint, -- 商品id
>   `product_name` string, --商品名称
>   `extend_info` string
> )
> row format delimited fields terminated by '\t';
> 
> 
> -- 导入数据
> load data local inpath '/opt/module/data/user_visit_action.txt' into table user_visit_action;
> load data local inpath '/opt/module/data/product_info.txt' into table product_info;
> load data local inpath '/opt/module/data/city_info.txt' into table city_info;
> 
> -- 测试数据是否导入成功
> select * from user_visit_action limit 5;
> select * from product_info limit 5;
> select * from city_info limit 5;
> ```
>
> +++
>
> **代码实现**
>
> ```java
> //中间变量
> public class CityBuff implements Serializable {
> 
>     private Long total_num;
>     private Map<String,Long> cityNum;
> }
> 
> // 自定义UDAF函数（自定义类）
> public class CityRateUDAF extends Aggregator<String, CityBuff, String> {
> 
>     /**
>      * 初始化 中间变量
>      * @return
>      */
>     @Override
>     public CityBuff zero() {
>         return new CityBuff(0L,new HashMap<>());
>     }
> 
>     /**
>      * TODO 同一分区内，对数据进行聚合 combine合并
>      * @param cb  上一次聚合的结果
>      * @param curr_city 此次传入的数据
>      * @return
>      */
>     @Override
>     public CityBuff reduce(CityBuff cb, String curr_city) {
> 
>         //当前城市的点击次数进行 +1 操作
>         long city_num = cb.getCityNum().getOrDefault(curr_city, 0L) + 1;
>         //更新结果，记录进入cb，进行最后返回
>         cb.getCityNum().put(curr_city,city_num);
> 
>         return new CityBuff(cb.getTotal_num() + 1,cb.getCityNum());
>     }
> 
>     /**
>      * TODO 分区间，进行聚合操作  reduce合并
>      * @param b1
>      * @param b2
>      * @return
>      */
>     @Override
>     public CityBuff merge(CityBuff b1, CityBuff b2) {
> 
>         Map<String, Long> h1 = b1.getCityNum();
>         Map<String, Long> h2 = b2.getCityNum();
>         for (String city : h2.keySet()) {
>             long num = h1.getOrDefault(city, 0L) + h2.get(city);
>             h1.put(city,num);
>         }
>         Long all = b1.getTotal_num() + b2.getTotal_num();
>         return new CityBuff(all,h1);
>     }
> 
>     @Override
>     public String finish(CityBuff cityBuff) {
>         //TODO 1.得到当前key的访问总数
>         Long total_num = cityBuff.getTotal_num();
>         //TODO 2.转成List对象，并且进行排序
>         Map<String, Long> cityNum = cityBuff.getCityNum();
>         ArrayList<Map.Entry<String,Long>> list = new ArrayList(cityNum.entrySet());
>         Collections.sort(list, new Comparator<Map.Entry<String, Long>>() {
>             @Override
>             public int compare(Map.Entry<String, Long> o1, Map.Entry<String, Long> o2) {
>                 // TODO 降序排列
>                 return (int) (o2.getValue() - o1.getValue());
>             }
>         });
>         //TODO 3.对于排好序的list进行判断
> //        System.out.println(list);
>         String result = "";
>         if (list.size() <= 2){
>             //城市数不大于2，直接输出
>             for (Map.Entry<String, Long> entry : list) {
>                 String d = String.format("%.1f",(entry.getValue().doubleValue() / total_num)* 100);
>                 result += entry.getKey() + ":" + d + "%,";
>             }
>             result.substring(0,result.length()-1);
>         }else {
> 
>             String d1 = String.format("%.1f" ,(list.get(0).getValue().doubleValue()/total_num) * 100);
>             String d2 = String.format("%.1f", (list.get(1).getValue().doubleValue() / total_num) * 100);
>             Double o =  ((total_num-list.get(0).getValue().doubleValue() - list.get(1).getValue().doubleValue())/total_num) * 100;
>             String d3 = String.format("%.1f",o);
>             result +=list.get(0).getKey() + ":" + d1 + "%,";
>             result +=list.get(1).getKey() + ":" + d2+ "%,";
>             result += "其他：" + d3 + "%";
>         }
>         return result;
>     }
> 
>     //中间变量的序列化方式
>     @Override
>     public Encoder<CityBuff> bufferEncoder() {
>         return Encoders.bean(CityBuff.class);
>     }
>     //返回值的序列化
>     @Override
>     public Encoder<String> outputEncoder() {
>         return Encoders.STRING();
>     }
> }
> 
> 
> //入口类
> public class TestHive {
>     public static void main(String[] args) {
>         //设置操作hdfs的用户
>         System.setProperty("HADOOP_USER_NAME","atguigu");
>         //创建 sparkSession 连接对象
>         SparkSession spark = SparkSession
>                 .builder()
> //                .master("local[4]")
>                 .appName("sql")
>                 .enableHiveSupport() //开启hive支持
>                 .getOrCreate();
> 
>         //编写代码
> //读取数据 --本地测试时使用，从文件读取并注册成表
> //        spark.read()
> //                .option("sep","\t")
> //                .csv("spark/datas/sql/city_info.txt")
> //                .toDF("city_id","city_name","area")
> ////                .show();
> //                .createOrReplaceTempView("city_info");
> //
> //        spark.read()
> //                .option("sep","\t")
> //                .csv("spark/datas/sql/product_info.txt")
> //                .toDF("product_id","product_name","extend_info")
> ////                .show();
> //                .createOrReplaceTempView("product_info");
> //
> //        spark.read()
> //                .option("sep","\t")
> //                .csv("spark/datas/sql/user_visit_action.txt")
> //                .toDF("date","user_id","session_id","page_id","action_time","search_keyword"
> //                        ,"click_category_id","click_product_id","order_category_ids","order_product_ids"
> //                        ,"pay_category_ids","pay_product_ids","city_id")
> //                .filter("click_product_id !='-1'") //过滤点击商品
> ////                .show();
> //                .createOrReplaceTempView("user_visit_action");
> 
>         //注册udaf函数
>         UserDefinedFunction obj = udaf(new CityRateUDAF(), Encoders.STRING());
>         spark.udf().register("city_mark",obj);
> 
> 
>         spark.sql("select\n" +
>                 "\tarea,\n" +
>                 "\tproduct_name,\n" +
>                 "\tcount(1) num,\n" +
>                 "\tcity_mark(city_name) mark\n" +
>                 "from user_visit_action a join city_info b\n" +
>                 "on a.city_id = b.city_id\n" +
>                 "join product_info c\n" +
>                 "on a.click_product_id = c.product_id\n" +
>                 "group by area,product_name")
> //                .show();
>                 .createOrReplaceTempView("area_product_city");
> 
>         spark.sql("select\n" +
>                 "\tarea,\n" +
>                 "\tproduct_name,\n" +
>                 "\tnum,\n" +
>                 "\tmark,\n" +
>                 "\trow_number() over(partition by area order by num desc) rk\n" +
>                 "from area_product_city;")
> //                .show();
>                 .createOrReplaceTempView("area_product_city_rank");
> 
>         //hive中执行，创建result表用于存放数据
>         spark.sql("drop table if exists result");
>         spark.sql("create table result as \n" +
>                 "select \n" +
>                 "\tarea,\n" +
>                 "\tproduct_name,\n" +
>                 "\tnum,\n" +
>                 "\tmark\n" +
>                 "from area_product_city_rank\n" +
>                 "where rk <=3");
>         //本次测试时使用，写入文件write() | 打印输出show()
> //                .write().mode(SaveMode.Overwrite).json("spark/datas/output/json");
> //                .show();
> 
>         spark.close();
>     }
> }
> ```
>
> ```bash
> #执行的指令
> bin/spark-submit --master yarn --class com.atguigu.day07.TestHive ./spark-1.0-SNAPSHOT.jar
> ```
>
> +++
>
> 其他补充：
>
> + pom.xml引入相关依赖 ： 提交集群运行时，需要删除 `spark-core_2.12`，spark/jars中存在有
>
>   ```xml
>   <dependencies>
>           <dependency>
>               <groupId>org.apache.spark</groupId>
>               <artifactId>spark-sql_2.12</artifactId>
>               <version>3.1.3</version>
>               <scope>provided</scope>
>           </dependency>
>           <dependency>
>               <groupId>mysql</groupId>
>               <artifactId>mysql-connector-java</artifactId>
>               <version>5.1.27</version>
>           </dependency>
>           <dependency>
>               <groupId>org.apache.spark</groupId>
>               <artifactId>spark-hive_2.12</artifactId>
>               <version>3.1.3</version>
>               <scope>provided</scope>
>           </dependency>
>           <dependency>
>               <groupId>org.projectlombok</groupId>
>               <artifactId>lombok</artifactId>
>               <version>1.18.22</version>
>           </dependency>
>       </dependencies>
>   
>       <build>
>           <plugins>
>               <plugin>
>                   <groupId>org.apache.maven.plugins</groupId>
>                   <artifactId>maven-shade-plugin</artifactId>
>                   <version>3.1.1</version>
>                   <executions>
>                       <execution>
>                           <phase>package</phase>
>                           <goals>
>                               <goal>shade</goal>
>                           </goals>
>                           <configuration>
>                               <artifactSet>
>                                   <excludes>
>                                       <exclude>com.google.code.findbugs:jsr305</exclude>
>                                       <exclude>org.slf4j:*</exclude>
>                                       <exclude>log4j:*</exclude>
>                                       <exclude>org.apache.hadoop:*</exclude>
>                                   </excludes>
>                               </artifactSet>
>                               <filters>
>                                   <filter>
>                                       <!-- Do not copy the signatures in the META-INF folder.Otherwise, this might cause SecurityExceptions when using the JAR. -->
>                                       <!-- 打包时不复制META-INF下的签名文件，避免报非法签名文件的SecurityExceptions异常-->
>                                       <artifact>*:*</artifact>
>                                       <excludes>
>                                           <exclude>META-INF/*.SF</exclude>
>                                           <exclude>META-INF/*.DSA</exclude>
>                                           <exclude>META-INF/*.RSA</exclude>
>                                       </excludes>
>                                   </filter>
>                               </filters>
>   
>                               <transformers combine.children="append">
>                                   <!-- The service transformer is needed to merge META-INF/services files -->
>                                   <!-- connector和format依赖的工厂类打包时会相互覆盖，需要使用ServicesResourceTransformer解决-->
>                                   <transformer
>                                           implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
>                               </transformers>
>                           </configuration>
>                       </execution>
>                   </executions>
>               </plugin>
>           </plugins>
>       </build>
>   ```

### 6. Spark SQL 回顾

```txt
1、SparkSql概述
	1、什么是sparksql
		sparksql是处理结构化数据的spark模块
	2、hive on spark与spark on hive
		spark on hive: spark负责sql解析与优化,hive是负责存储元数据,后续spark可以从hive读写数据。
		hive on spark: hive负责解析sql与优化,spark是作为hive计算引擎
2、sparksql编程
	1、sparksession的创建: SparkSession spark = SparkSession.builder().master(...).appName(...).getOrCreate()
	2、编程方式[一般采用SQL方式]
		1、需要将数据集注册成表: ds.createOrRepleaceTempView(表名)
		2、通过sql操作数据: spark.sql("....")
	3、自定义UDF函数
		1、导入sparksql函数:
			import static org.apache.spark.sql.functions.*
		2、通过udf方法定义udf函数
			udf(new UDFN{.....})
		3、将第二步得到的对象注册到sparksession中
			spark.udf().register("函数名",第二步产生的对象)
	4、自定义UDAF函数
		1、创建一个class继承Aggregator<IN,BUFF,OUT>
			IN: 代表聚合函数的参数类型
			BUFF: 代表聚合过程中的中间变量类型
			OUT: 代表聚合函数的最终结果类型
		2、重新抽象方法
		3、导入sparksql函数:
			import static org.apache.spark.sql.functions.*
		4、创建自定义UDAF对象: new XXXX()
		5、通过udaf方法将自定义UDAF对象进行转换: udaf(第四步对象)
		6、将第二步得到的对象注册到sparksession中
			spark.udf().register("函数名",第五步转换的对象)
3、sparksql数据加载与保存
	1、读取文件: spark.read.[option(k,v)...].text/json/csv/parquet/orc(path)
		读取csv文件的时候常用的option：
			sep: 指定字段之间的分隔符
			header: 指定是否将文件的第一行作为表的列名
			infreSchema: 指定是否自动推断列的类型
	2、读取mysql
		String url = "jdbc:mysql://hadoop102:3306/库名"
		//String tableName = "person" //代表查询整表数据
		String tableName = "(select 列名,.... from 表名 where ... group by ...) 别名" //代表查询指定数据
		Properties props = new Properties()
		props.setProperty("user","mysql账号")
		props.setProperty("password","mysql密码")
		1、第一种读取方式: 生成的DataSet的分区数 = 1 【此种方式一般用于小数据量场景】
			spark.read.jdbc(url,tableName,props)
		2、第二种方式: 生成的DataSet的分区数 = conditions数组的长度 [此种方式一般不用]
			String[] conditions = {"sql where条件",....} //里面的每个元素代表每个分区拉取数据的where条件
			spark.read.jdbc(url,tableName,coditions,props)
		3、第三种方式:生成的DataSet的分区数 = (upperBound-lowerBound)>=numPartitions ? numPartitions : (upperBound-lowerBound) [一般用于大数据量场景]
			columName： 后续用于分区拉取数据的where条件中的列名,必须是数字、日期、时间戳类型[建议用主键]
			lowerBound: 用于决定每个分区拉取的数据的间距的最小值[建议用columName字段的最小值]
			upperBound: 用于决定每个分区拉取的数据的间距的最大值[建议用columName字段的最大值]
			numPartitions： 分区数
			spark.read.jdbc(url,tableName,columName,lowerBound,upperBound,numPartitions,props)
	3、写入数据到文件: ds.write.mode(SaveMode.XXX)[.option(k,v)....].json/text/jdbc/json/csv/parquet/orc(path)
		写入csv文件的时候常用的option：
			sep: 指定字段之间的分隔符
			header: 指定是否将列名作为文件第一行保存
		常用的写入模式:
			SaveMode.Append: 如果写入目录/表已经存在则追加数据[一般用于数据写入没有主键的mysql表中]
				如果想要将数据写入有主键的表中,此时不能用Append,因为Append底层使用的insert into语句,可能会出现主键冲突。
				解决方案: ds.rdd.foreachPartition + insert into 表名 values(?,...) ON DUPLICATE KEY UPDATE 字段名=?,字段名=?,....
			SaveMode.Overwrite: 如果写入目录/表已经存在则覆盖数据[一般用于数据写入HDFS]
	4、与hive的交互
		idea操作hive:
			1、引入spark-hive、mysql的依赖
			2、将hive-site.xml引入到resouces目录
			3、在创建sparksession对象的时候开启hive的支持: enableHiveSupport()
			4、读写数据
				读hive表: spark.sql("select ... from hive表...")
				写数据到hive表:
					spark.sql("load data inpath 'hdfs://...' overwrite into table 表名 ....")
					spark.sql("insert into 表名 select .....")
					spark.sql("insert overwrite 表名 select .....")
```

+++

+++

## Spark 内核

### 通信原理



### 提交流程



### 任务切分



### Task任务调度



### sortShuffle | bypassShuffle



### 统一内存管理



## Spark调优

1、查看执行计划: explain
2、执行计划处理流程
	sql -> 未决断的逻辑执行计划[此时没有判断语法、表名、字段是否正确]-> 已决断的逻辑执行计划[判断语法、表名、字段是否正确]-> 生成物理执行计划->评估->选择性能最优的物理执行计划->生成可执行代码
3、CBO优化
	1、CBO优化的前提,必须要对表、字段进行数据收集
		收集表信息: ANALYZE TABLE 表名 COMPUTE STATISTICS
		收集字段信息: ANALYZE TABLE 表名 COMPUTE STATISTICS FOR COLUMNS 列1,列2,列3
	2、CBO优化
		CBO能够调整join策略、调整join顺序等
		spark.sql.cbo.enabled: 是否开启cbo优化[前提表数据、字段数据已经收集好]
		spark.sql.cbo.joinReorder.enabled: 是否开启join顺序的调整
		spark.sql.cbo.joinReorder.dp.threshold: join顺序调整的时候,join表个数的限制。
	3、广播join
		适用场景: 大表join小表
		使用方式:
			1、通过参数控制小表大小的阈值: spark.sql.autoBroadcastJoinThreshold
			2、强制广播: select /*+Broadcast(表名/表名) */ B   from a join b ...
		好处: 能够避免shuffle操作,提高性能
	4、SMB join
		适用场景: 大表 join 大表
		好处: 能够减少join的扫描范围,提高性能
		前提: 
			1、join的两个表必须是分桶表
			2、join的两个表的桶的个数必须一样
			3、join的字段必须是分桶的字段
4、AQE优化
	动态合并分区: spark会根据分区数据量分析,将小数据量多个分区合并成一个分区,提高资源的利用率
	  spark.sql.adaptive.enabled： 是否开启AQE优化
      spark.sql.adaptive.coalescePartitions.enabled: 是否开启动态合并分区
      spark.sql.adaptive.coalescePartitions.initialPartitionNum: 初始分区数
      spark.sql.adaptive.coalescePartitions.minPartitionNum: 最小分区数
      spark.sql.adaptive.advisoryPartitionSizeInBytes: 合并分区之后分区数据量大小
	动态资源申请: spark会根据数据情况,动态的申请资源
		spark.sql.adaptive.enabled： 是否开启AQE优化
	  spark.dynamicAllocation.enabled：是否开启动态申请资源
      spark.dynamicAllocation.shuffleTracking.enabled:  是否开启shuffle动态跟踪
	动态切换join策略: 选择最优join策略
		spark.sql.adaptive.enabled： 是否开启AQE优化
        spark.sql.adaptive.localShuffleReader.enabled:在不需要进行shuffle重分区时，尝试使用本地shuffle读取器。将sort-meger join 转换为广播join
	数据倾斜的动态优化: 
	  spark.sql.adaptive.enabled： 是否开启AQE优化
      spark.sql.adaptive.skewJoin.enable: 是否开启join数据倾斜的优化
      spark.sql.adaptive.skewJoin.skewedPartitionFactor: N
      spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes:  代表判断数据倾斜分区数据量的阈值
		当分区处理的数据量>= 所有分区处理数据量的中位数 * skewedPartitionFactor
		当分区处理的数据量>= skewedPartitionThresholdInBytes
		以上两个条件都满足的时候,spark会自动判断该分区数据倾斜。
      spark.sql.adaptive.advisoryPartitionSizeInBytes: 数据倾斜拆分数据之后,分区处理的数据量大小。

# 知识小记

1. 只有Action算子会运行Job,调用 `runJob()`

2. Action算子调用时，从后往前查询依赖关系，遇到 `宽依赖` 【存在shuffle操作/或者活动算子都会切分为一个stage】则会切分为一个 `stage` ,每个stage的Task数量（线程数）由最后一个RDD分区决定

3. 基本概念

   > + Application: 应用，[一个SparkContext称之为一个Application]
   > + Job: 任务，[一个Action算子一般会产生一个job, `first()` `take()`可能产生两个]
   > + Stage: 阶段，[**一个job中stage的个数 = 宽依赖个数(shuffle个数)+ 1**]
   > + Task: 子任务，[**一个stage中task的个数 = 该stage种最后一个RDD的分区数**]

4. SparkContext **【Driver线程的对象】**

   > + SparkContext是spark功能的主要入口
   >
   > + 代表与spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。
   >
   > + 每个JVM里只能存在一个处于激活状态的SparkContext，在创建新的SparkContext之前必须调用stop()来关闭之前的SparkContext
   >
   > + SparkContext是Driver线程中的，负责与程序和spark集群进行交互，包括申请集群资源、创建RDD、accumulators及广播变量等。sparkContext与集群资源管理器、work节点交互，因此任务执行过程中，Driver线程不能停
   >
   >   ![image-20220925121656709](http://ybll.vip/md-imgs/202209251216811.png)
   >
   >   

5. SparkSession

   > + SparkSession 是 spark2.x 引入的新概念，SparkSession 为用户提供统一的切入点，字面理解是创建会话，或者连接 spark
   >
   > + 在 spark1.x 中
   >
   >   + SparkContext 是 spark 的主要切入点，
   >   +  RDD 作为主要的 API，我们通过 SparkContext 来创建和操作 RDD,
   >
   >   SparkContext 的问题在于：
   >
   >   + 不同的应用中，需要使用不同的 context，
   >     + 在 Streaming 中需要使用 StreamingContext
   >     + 在 sql 中需要使用 sqlContext
   >     + 在 hive 中需要使用 hiveContext，比较麻烦
   >
   > + 随着 DataSet 和 DataFrame API 逐渐成为标准 API，需要为他们创建接入点，即 **SparkSession**
   >   + SparkSession 实际上封装了 SparkContext，另外也封装了 SparkConf、sqlContext，随着版本增加，可能更多，
   >
   >   + 所以我们尽量使用 SparkSession ，如果发现有些 API 不在 SparkSession 中，也可以通过 SparkSession 拿到 SparkContext 和其他 Context 等
   >
   > 在 shell 操作中，原生创建了 SparkSession，故无需再创建，创建了也不会起作用
   >
   > 在 shell 中，SparkContext 叫 sc，SparkSession 叫 spark

