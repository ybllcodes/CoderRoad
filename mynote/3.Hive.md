# Hive

### 一. Hive基础

### 1.1基本概念

1. Hive : 由FaceBook开源用于解决海量结构化日志的数据存储工具，是基于Hadoop的一个数据仓库工具，将结构化数据文件映射为一张表，并提供类SQL操作

2. Hive本质:将HQL转化成MapReduce程序

   1. Hive处理的数据存储在HDFS

   2. Hive分析数据底层的实现是MapReduce

   3. 执行程序运行在Yarn上

3. Hive的优缺点

   1. 优点

      - 操作接口采用类SQL语法

      - 避免使用MapReduce，减少开发人员学习成本

      - Hive的执行延迟比较高，常用于数据分析，不适用于实时性场合

      - 适合处理大数据，对小数据处理没有优势

      - Hive支持用户自定义函数

   2. 缺点

      - HQL的表达能力有限，迭代算法无法表达；数据挖掘方面不擅长
      - Hive的效率比较低，自动生成的MapReduce作业，调优困难，粒度较粗
      - 不支持实时查询和行级别更新

4. Hive架构原理

   > ![image-20220918190904281](http://ybll.vip/md-imgs/202209181909462.png)
   >
   > +++
   >
   > + Hive运行架构
   >
   > ![image-20220918191247368](http://ybll.vip/md-imgs/202209181912532.png)
   >
   > > + CLI 命令行的方式连接HIVE客户端（command-line interface）
   > >
   > > + JDBC的方式连接HIVE客户端
   > >
   > >   +++
   > >
   > > + Metastore(元数据)：表名、表所属的数据库(default)、表的拥有者、列/分区字段、表的类型、表的数据所在目录等；默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore
   > >
   > > + 使用HDFS存储，使用MapReduce进行计算，Yarn进行资源调度（MapTask，ReduceTask)
   > >
   > >   > 数据分析计算：
   > >   >
   > >   > + 基于MapReduce ：shuffle操作多，稳定，但是速度慢，适合数据量特别大，需要跑好几天时使用
   > >   > + 基于Tez ：完全基于内存计算，容易OOM，相对不稳定，适合即席查询，适用于临时查询
   > >   > + 基于Spark ：内存和磁盘相结合，一般用于跑每天的定时同步任务
   > >
   > > + 驱动器：Dirver
   > >
   > >   > + 解析器：将SQL翻译成 抽象语法树
   > >   >
   > >   > + 逻辑计划/物理计划【生成器，优化器】
   > >   >
   > >   >   ![image-20220918192428529](http://ybll.vip/md-imgs/202209181924730.png)
   > >   >
   > >   >   +++
   > >   >
   > >   >   逻辑计划优化器：map端的预聚合【Group By Operator】 等等....
   > >   >
   > >   >   ![image-20220918192653851](http://ybll.vip/md-imgs/202209181926979.png)

5. Hive 和 数据库比较

   ![image-20220704110059333](http://ybll.vip/md-imgs/202207041100403.png)

### 1.2 Hive安装与部署

##### 1.2.1 安装MySQL

```bash
#检查是否存在自带Mysql
rpm -qa | grep -i mysql; 
rpm -qa | grep -i -E mysql\|mariadb;

#卸载mariadb
sudo rpm -e --nodes mysql.tar.gz/others;
rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps

#正式安装
tar -xvf mysql-5.7.tar.gz -C dir/ #解压压缩包

#目录列表，且依次顺序安装
mysql-community-common-xx.rpm; libs-5.7.28; libs-compat; client; server.rpm; 
#安装命令如下
rpm -ivh mysql-community-server-xx.rpm;

#删除my.cnf文件中，配置项datadir所指目录下的所有文件
#这是原来mysql数据所存储的目录，需要全部删除，保证卸载干净
/etc/my.cnf; datadir=/var/lib/mysql; rm -rf ./*;

#mysql安装后的初始化操作，创建mysql内部数据库和表
sudo mysqld --initialize --user=mysql
cat /var/log/mysqld.log | grep password  #查看临时密码

# erd5#kfzt<9W

#启动mysql, 并登录
sudo systemctl start mysqld
mysql -uroot -p

set password = password("newpw"); #修改root密码
#修改mysql库下user表中root用户允许任意ip连接
use mysql; select host,user from user;
update mysql.user set host='%' where user='root';
flush privileges;
```

***

##### 1.2.2 Hive安装部署

```bash
tar -zxvf /apache-hive-3.1.2-bin.tar.gz -C /opt/module #解压Hive
sudo vim /etc/profile.d/my_env.sh #修改环境变量
mv lib/log4j-slf4j-impl-2.10.0.jar lib/log4j.bak #修改Jar包冲突,$HIVE_HOME/lib下

#将mysql的JDBC驱动拷贝带Hive的lib下
cp mysql-connector-java-5.1.27-bin.jar hive_home/lib

#添加配置文件，配置Metastore到Mysql,添加内容在后面
vim $HIVE_HOME/conf/hive-site.xml

#登录mysql后，初始化元数据库
# 1.创建metastore元数据库
create database metastore; quit;
# 2.初始化Hive元数据库,linux下操作，不是mysql命令
schematool -initSchema -dbType mysql -verbose 
# 3.修改元数据库字符集
##字段注释
alter table metastore.COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
##表注释
alter table metastore.TABLE_PARAMS modify column PARAM_VALUE mediumtext character set utf8;

###启动hive,需要先启动hadoop集群
##启动方式一
hive #启动后可直接用HQL操作,show databases;show tables等

##方式二，使用元数据服务访问Hive，需修改hive-site.xml文件(添加配置,配置代码在后面)
hive --service metastore #启动metastore，启动后当前窗口不能操作，需要新打开一个窗口继续操作
hive #方式二启动

##方式三，使用JDBC访问Hive,需修改hive-site.xml文件(添加配置，配置代码在在后面)
hive --service hiveserver2; #启动hiveserver2
beeline -u jdbc:hive2://hadoop102:10000 -n atguigu #启动beeline客户端；if connection rejected (wait);
```

+ 方式一

  > 不需要安装mysql,直接安装hive,元数据用Derby存储
  >
  > ![image-20220704201324976](http://ybll.vip/md-imgs/202207042013040.png)
  >
  > + Derby使用Java编写，与应用程序共享一个JVM，不支持多客户端连接，因为部署方式为内嵌式
  >
  > + ```bash
  >   #在hive根目录下，进行初始化Derby元数据库(不用，直接改用mysql)
  >   bin/schematool -dbType derby -initSchema
  >   
  >   #解决Jar包冲突
  >   mv lib/log4j-slf4j-impl-2.10.0.jar /lib/....jar.bak
  >   
  >   #启动hive
  >   hive
  >   ```
  >
  
+ 方式二

  > ![image-20220704203456655](http://ybll.vip/md-imgs/202207042034736.png)

+ 方式三 HiveServer2模式

  > ![image-20220704203605078](http://ybll.vip/md-imgs/202207042036157.png)
  >
  > +++
  >
  > 启动后的jar进程情况：
  >
  > ![image-20220704141254246](http://ybll.vip/md-imgs/202207041412338.png)

***

hive-site.xml中相关配置如下：

```xml
<!-- 方式二 配置metastore到mysql时，需要添加配置-->
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- jdbc连接的URL -->
	<property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8
        </value>
	</property>

    <!-- jdbc连接的Driver-->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <!-- jdbc连接的username-->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <!-- jdbc连接的password -->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>1234</value>
    </property>
    <!-- Hive默认在HDFS的工作目录 -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    <!-- Hive元数据存储的验证 -->
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    <!-- 元数据存储授权  -->
    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>


    <!-- 使用元数据服务的方式访问Hive时，添加如下配置 -->
    <!-- 指定存储元数据要连接的地址 -->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hadoop102:9083</value>
    </property>


    <!-- 方式三 使用JDBC访问Hive时，继续添加如下配置 -->
    <!-- 指定hiveserver2连接的host -->
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hadoop102</value>
    </property>
    <!-- 指定hiveserver2连接的端口号 -->
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
</configuration>
```

***

###### shell命令介绍

```bash
nohup:放在命令开头，表示不挂起，关闭终端进程也继续保持运行状态
0:标准输入	1:标准输出	2:错误输出
2>&1:将标准错误重定向到标准输出上		&:放在命令结尾，表示后台运行
一般组合:nohup $linux命令>file 2>&1 &
nohup hive --service metastore 2>&1 &
nohup hive --service hiveserver2 2>&1 &
```

***

###### 启动脚本(myhive.sh)

```shell
#!/bin/bash
HIVE_LOG_DIR=$HIVE_HOME/logs
if [ ! -d $HIVE_LOG_DIR ]
then
	mkdir -p $HIVE_LOG_DIR
fi
#检查进程是否运行正常，参数1为进程名，参数2为进程端口
function check_process()
{
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

function hive_start()
{
    metapid=$(check_process HiveMetastore 9083)
    cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"
    cmd=$cmd" sleep 4; hdfs dfsadmin -safemode wait >/dev/null 2>&1"
    [ -z "$metapid" ] && eval $cmd || echo "Metastroe服务已启动"
    server2pid=$(check_process HiveServer2 10000)
    cmd="nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
    [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2服务已启动"
}

function hive_stop()
{
    metapid=$(check_process HiveMetastore 9083)
    [ "$metapid" ] && kill $metapid || echo "Metastore服务未启动"
    server2pid=$(check_process HiveServer2 10000)
    [ "$server2pid" ] && kill $server2pid || echo "HiveServer2服务未启动"
}

case $1 in
"start")
    hive_start
    ;;
"stop")
    hive_stop
    ;;
"restart")
    hive_stop
    sleep 2
    hive_start
    ;;
"status")
    check_process HiveMetastore 9083 >/dev/null && echo "Metastore服务运行正常" || echo "Metastore服务运行异常"
    check_process HiveServer2 10000 >/dev/null && echo "HiveServer2服务运行正常" || echo "HiveServer2服务运行异常"
    ;;
*)
    echo Invalid Args!
    echo 'Usage: '$(basename $0)' start|stop|restart|status'
    ;;
esac
```

***

##### 1.2.3 Hive简单操作

###### 常用交互命令

```bash
##linux命令操作
hive -help #查看命令合集
hive -e "select id from student;" #在linux下执行hql语句
#从文件中读取hql语句，结果输出到文件
hive -f /hivef.sql >/hive_result.txt 
#查看hive中的历史命令,在/root或/home/username
cat .hivehistory 

##hive客户端操作，退出
exit; #隐形提交数据，再退出
quit; #不提交数据，退出
dfs -ls /;#查看hdfs文件系统; dfs hdfs_command,操作hdfs
```

***

###### 常见属性配置

```bash
##1. Hive运行日志信息配置,重命名配置文件，修改配置项
mv $HIVE_HOME/conf/hive-log4j2.properties.template /conf/hive-log4j2.properties;
	#修改hive-log4j2.properties文件该配置项
	property.hive.log.dir=$HIVE_HOME/logs 

##2. Hive启动JVM堆内存配置
mv $HIVE_HOME/conf/hive-env.sh.template conf/hive-env.sh
	#修改下面这项，默认JVM堆内存是256M,设置成1024M，防止报错：OutOfMemoryError
	export HADOOP_HEAPSIZE=1024

##3. hive窗口打印默认库、表头
#在hive-site.xml中修改两个参数,设置为true
hive.cli.print.header
hive.cli.print.current.db

#############################################################

###参数配置方式
#hive-default.xml(默认配置文件) hive-site.xml(用户自定义配置文件)

#命令行参数设置(仅对本次hive启动有效)
hive -hiveconf mapred.reduce.tasks=10;
set mapred.reduce.tasks; #hive中查看该配置


#参数声明方式(仅对本次hive启动有效)
set mapred.reduce.tasks=100;

#有些参数的读取在会话建立前，则只能用前两种方式设置
```

+++

###### 数据类型

```bash
tinyint; smallint; int; bigint; boolean; float; double; string; timestamp; binary;
TINYINT; SMALLINT; INT; BIGINT; BOOLEAN; FLOAT; DOUBLE; STRING; TIMESTAMP; BINARY;

struct struct<name:string,age:int>
#struct.name
map map<string,int>
#map['key']
array array<string>
#array[index]

cast('1' as int) + 2;
```

![image-20220705101305108](http://ybll.vip/md-imgs/202207051013233.png)

+++

![image-20220705101504577](http://ybll.vip/md-imgs/202207051015643.png)

+ 实操案例

  > 1. json格式数据如下
  >
  > ```json
  > {
  >     "name": "songsong",
  >     "friends": ["bingbing" , "lili"], //列表Array, 
  >     "children": {                     //键值Map,
  >         "xiao song": 19 ,
  >         "xiaoxiao song": 18
  >     }
  >     "address": {                      //结构Struct,
  >         "street": "hui long guan" ,
  >         "city": "beijing"
  >     }
  > }
  > ```
  >
  > 2. 测试数据
  >
  > ```bash
  > songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing
  > yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing
  > ybllcodes,sanluo_dalao,xiao ou:10_xiao ming:22,bao an_shenzhen
  > 
  > #上传该文件只hdfs指定目录(personinfo表所对应的目录)
  > hadoop fs -put /opt/module/hive-3.1.2/datas/personinfo.txt /user/hive/warehouse/personinfo/
  > ```
  >
  > 3. ddl建表语句
  >
  > ```sql
  > create table personinfo(
  >     name string,
  >     friends array<string>,
  >     children map<string,int>,
  >     address struct<street:string,city:string>
  > )
  > row format delimited
  > fields terminated by ","
  > collection items terminated by "_"
  > map keys terminated by ":"
  > lines terminated by "\n";
  > ```
  >
  > 4. 测试HQL
  >
  > ```mysql
  > select name,friends[1] myFriend,children["xiao ming"] mySon,address.city myCity from personinfo;
  > 
  > -- friends[0]
  > -- children["xiao ming"]
  > -- address.city
  > ```
  >
  > ![image-20220705114157384](http://ybll.vip/md-imgs/202207051141476.png)
  >
  > +++

+ 类型转换

  > + Hive的基本数据类型可以隐式转换，类似于Java
  > + 隐式类型转换规则如下
  >   + 任何整数类型都可以隐式地转换为一个范围更广的类型，如INT可以转换成BIGINT。
  >   + 所有整数类型、FLOAT和`STRING类型`都可以隐式地转换成DOUBLE。
  >   + TINYINT、SMALLINT、INT都可以转换为FLOAT。
  >   + BOOLEAN类型不可以转换为任何其它的类型。
  > + CAST 操作可以显示进行数据类型转换
  >   + `cast('1' as int)` : 把字符串“1”转换成整型1
  >   + 如果强制类型转换失败，则表达式返回 `NULL`
  >
  > <img src="http://ybll.vip/md-imgs/202207051042604.png" alt="image-20220705104247496" style="zoom:80%;" />

+++

### 二. SQL(HQL)语句操作

##### 2.1 DDL语句

```sql
--回顾前面内容
beeline -u jdbc:hive2://hadoop102:10000 -n atguigu
netstat -anp | grep 10000
load data local inpath '/opt/module/hive-3.1.2/datas/test.txt' into table test;
friends[1],children['xiao song'],address.city from test
desc database default;

--数据库的ddl:
-- 创建数据库的语法格式
create database [if not exists] dbname
[comment database_comment]
[location hdfs_path]
[with dbproperties (property_name=property_value)]
--其他操作
create database db_hive; create database if not exists db_hive;--创建hive库
create database db_hive2 location '/dir/dblocation'; --再指定目录创建库

--展示hive数据库列表
show databases; show databases like 'db_hive*';
--展示数据库信息
desc database db_hive;
desc database extended db_hive;

use db_hive;
--修改数据库，只能修改属性信息，元数据信息(包括库名、库所在的hdfs目录位置等)都是不可修改
alter database db_hive set dbproperties('createtime'='p_value');

--删除库(空库)/删除库(不为空，强制删除-cascade)
drop database db_hive2; drop database db_hive cascade;
```
```sql
--表的ddl:
无 external: 内部表(管理表)，删除表时，元数据跟数据一起被删除
有 external: 创建外部表，删除表时，只删除元数据，不删除实际数据

--建表语句标准格式
create [external] table [if not exists] table_name
[(col_name data_type [comment col_comment],...)]
[comment table_comment]
[partitioned by (col_name data_type [comment col_comment],...)]
[clustered by (col_name, col_name,...)
	[sorted by (col_name [asc|desc],...)] into num_buckets]
[row format row_format]
[stored as file_format]
[location hdfs_path] 
[tblproperties (property_name=property_value,...)]
[as select_statement];
[like existing_table | view_name]

-- 查看建表语句
show create table student;
```
+ 字段解释说明

![image-20220705141929639](http://ybll.vip/md-imgs/202207051419750.png)

+ 内部表说明

  > a) 默认创建的表都是所谓的管理表，有时也被称为内部表。
  >
  > b) 管理表，Hive会控制着元数据和真实数据的生命周期。
  >
  > c) Hive默认会将这些表的数据存储在hive.metastore.warehouse.dir定义目录的子目录下。
  >
  > d) 当我们删除一个管理表时，Hive也会删除这个表中数据。
  >
  > e) **管理表不适合和其他工具共享数据。**

```sql
--表的创建，描述信息展示，删除表
create table if not exists student3 like students;
desc formatted student2; desc student2;
drop table dept;
-- 清空表，只能清空管理表(实际就是删除hdfs数据,因此外部表不能清空)
truncate table student;

--转换为外部表，FALSE:内部表(管理表)
alter table student set tblproperties('EXTERNAL'='TRUE');

-- 修改表名
-- 外部表：不会更改表中数据在hdfs的存储目录
-- 内部标：hdfs的存储目录也会跟着表名变化
alter table teacher rename to teacher2;

-- 修改列信息(增加、修改、替换)
alter table table_name change [column] col_old_name col_new _name column_type [comment col_comment] [first|after column_name];

alter table table_name add|replace column (col_name data_type [comment col_comment],...);

-- 举例-------------------
-- 更改列信息
alter table student change id stu_id int;
alter table student change stu_id stu_id bigint;
alter table student change stu_id id string;

alter table student change name name string first;

-- 增加列 替换列
alter table student add columns(age int);

alter table student change age age string after name;

--替换
alter table student replace columns(id double,name string);

-- 增加、修改和删除表分区 (后续补充)
```

##### 2.2 DML（数据导入导出）

```sql
--导入：
--1.load:
load data [local] inpath 'path' [overwrite] into table student [partition (partcoll=vall,...)];
/*
load data:表示加载数据
local:表示从本地加载到Hive中，否则就是从HDFS加载到Hive
	从hdfs文件中导入时，相当于剪切，会删除原本path下的文件
inpath:表示加载数据路径
overwrite:表示覆盖原有数据，不加overwrite则表示追加
into table:表示加载到哪一张表(student表)
partition:表示上传到指定分区
*/

--2.insert:
insert overwrite/into table student2 select id,name from student;
insert overwrite/into table student2 values(1,'wangwu')
create table if not exits student3 as select id,name from student;

--3.location:
create table if not exists student(id int,name string) 
row format delimited 
fields terminated by '\t' location '/hdfs_path'

--4.as select:一般不用，走MR程序创建表，存储的数据用SOH分割，而不是\t分割
create table student2 as (
    select * from student3 where id % 2 = 0
);


--导出：
--1.insert:
insert overwrite local directory 'local_path' select * from student; --导出到本地
insert overwrite directory 'hdfs_path' select * from student; --导出到HDFS
-- example
-- 查询结果导出到本地
insert overwrite local directory '/opt/module/hive/datas/export/student'
select * from student;

-- 查询结果格式化后导出到本地
insert overwrite local directory '/opt/module/hive/datas/export/student1'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
select * from student;

-- 查询结果导出到HDFS
insert overwrite directory '/export/student'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
select * from student;
-- insert导出时，hive会自动创建导出目录，但是由于是overwrite，所以存在误删数据的可能。

--2.hdfs_command
dfs -get /hdfs_file /local_file;
hive -e 'select * from default.student;' >/local_file

--3.Sqoop导出 后续补充
truncate table student; --清空表中数据，只能删除管理表，不能删除外部表
```

```sql
-- 数据迁移
-- export和import命令主要用于两个Hadoop平台集群之间Hive表迁移

-- export:导出(真实数据和元数据都会导出)
export table database.table_name to '/hdfs_path';

-- import:导入
import table student2 from 'export_path'
```

+++

##### 2.3 Select 查询

> + 基本查询
>
> ```sql
> set hive.exec.mode.local.auto=true; -- 开启本地模式
> 
> select * from emp;
> select empno,sal * 0.8 in_sal from emp;
> select 
> 	count(1) rows,
> 	max(sal) max_sal,
> 	min(sal) min_sal,
> 	avg(sal) avg_sal,
> 	sum(sal) sum_sal
> from emp;
> select * from emp limit 5;
> select * from emp limit 0,5;
> select empno,ename,sal from emp where sal > 1000;
> select 
> 	ename,
> 	NULL <=> job as case1, -- A B 同时为NULL返回ture,否则false
> 	"1" <> job as case2, --  <>等同于!= ,返回ture/fasle,  A B出现NULL时，返回NULL
> 	"1" != job as case3,
> 	job
> from emp;
> select * from emp where ename like '_红%';
> select * from emp where ename rkike '[红]';
> select * from emp where sal > 1000 and deptno = 30;
> ```
>
> + 分组
>
> > 1. Group By : 和聚合函数一起使用，按照一个或多个列数据进行分组，然后对每组执行聚合操作
> >
> >    ```sql
> >    select
> >    	e.deptno deptno,
> >    	avg(e.sal) avg_dept_sal
> >    from emp e
> >    group by e.deptno;
> >    ```
> >
> > 2. Hiving : 只用于group by分组统计，其后面可以使用聚合函数和列的别名
> >
> >    ```sql
> >    select 
> >    	e.deptno deptno,
> >    	avg(e.sal) avg_dept_sal
> >    from emp e
> >    group by e.deptno
> >    having avg_sal > 2000;
> >    ```
> >
> > +++
>
> + Join查询
> + 排序
>
> > 1. Order by : 全局排序，只有一个Reducer,数据规模大时不适用
> >
> >    ```sql
> >    select empno,sal from emp order by sal;
> >    select empno,sal * 2 sal_double from emp order by sal_double desc;
> >    -- 可以接列的别名
> >    -- asc|默认(升序排序)    desc(降序)
> >    ```
> >
> > 2. Sort by : 每个Reducer内部排序，全局不是有序
> >
> >    + 通过命令设置reduce个数：`set mapreduce.job.reduces=3`
> >
> >    ```sql
> >    select * from emp sort by deptno desc;
> >    
> >    -- 将查询结果导出至本地文件(导出到sortby/目录下，3个reduce，则会生成3个输出文件)
> >    insert overwrite local directory '/opt/module/hive-3.1.2/datas/sortby'
> >    row format delimited
> >    	fields terminated by '\t'
> >    select * from emp sort by deptno desc;
> >    ```
> >
> > 3. Distribute by : 跟sort by 搭配使用，指定如何分区，类似于MR程序中的partition(自定义分区)
> >
> >    ```sql
> >    -- 设置reduce数为3
> >    insert overwrite local directory '/opt/module/hive-3.1.2/datas/distributeby'
> >    row format delimited
> >    	fields terminated by '\t'
> >    select * from emp 
> >    distribute by deptno
> >    sort by sal desc;
> >    -- deptno % 3 的值，0(0号分区) 1(1号分区) 2(2号分区)
> >    -- reduce数为5，同样也是deptno % 5，根据其值判断分区
> >    ```
> >
> > 4. Cluster by : distribute by 和 sort by 排序字段相同，且是升序时，可使用 clusterby
> >
> >    ```sql
> >    select ename,empno from emp cluster by deptno;
> >    --导出到文件中观察
> >    insert overwrite local directory '/opt/module/hive-3.1.2/datas/clusterby'
> >    row format delimited 
> >    	fields terminated by '\t'
> >    select ename,empno from emp cluster by deptno;
> >    ```
> >
> >    +++

+ `<=>` `<>` `!=` 的测试结果 

<img src="http://ybll.vip/md-imgs/202207061846334.png" alt="image-20220706184647217" style="zoom:60%;" />

注意点：

+ 使用列别名时可加 `as` ,但是别名不能用 `""` 和 `''`
+ ...

+++

##### 2.4 分区表和分桶表

> 1. 分区表
>
> > + 分区表实际上就是对应HDFS文件系统上的独立文件夹
> > + 该文件夹是该分区所有的数据文件
> > + Hive中的分区就是分目录，把一个大的数据集根据业务分割成小的数据集
> > + 查询时通过where子句选择所需要的分区，使得查询效率提高
> > + ***`分区字段不能是已有字段，可以将分区字段看成表的伪列`***
> >
> > ```sql
> > -- 创建分区表
> > create table dept_partition(
> >  deptno int,
> >  dname string,
> >  loc string
> > )
> > partitioned by (day string)
> > row format delimited fields terminated by '\t';
> > 
> > -- 加载数据
> > -- 加载数据时必须指定分区 into table dept_partition partition(day='20200401');
> > load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200401.log' into table dept_partition partition(day='20200401');
> > load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200402.log' into table dept_partition partition(day='20200402');
> > load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200403.log' into table dept_partition partition(day='20200403');
> > 
> > insert overwrite table dept_partition values(90,'飞行部',2100,'20200409'); -- 会自动增加20200409分区
> > insert overwrite table dept_partition values(10,'飞行部',2100,'20200401'); -- 只会重写 20200401分区数据，其他分区数据不受影响
> > 
> > -- 查询
> > select * from dept_partition where day="20200401";
> > select * from dept_partition where day='20200401' or day='20200402';
> > select * from dept_partition where day in('20200401','20200403');
> > show partitions dept_partition; --查看dept_partition的分区情况，如果不是分区表会报错
> > 
> > -- 增加分区,增加多个分区，partition之间用空格间隔
> > alter table dept_partition add partition(day='20200404');
> > show partitions dept_partition;
> > alter table dept_partition add partition(day='20200405') partition(day="20200406");
> > show partitions dept_partition;
> > 
> > --删除分区，删除多个分区，partition之间用逗号分割 -- 连同数据一起删除
> > alter table dept_partition drop partition(day='20200406');
> > show partitions dept_partition;
> > alter table dept_partition drop partition(day='20200405'),partition(day='20200404');
> > show partitions dept_partition;
> > ```
> >
> > +++
> >
> > + 问题：分区表建立后，再添加字段，如何使分区表包含`新字段` 
> >
> > ```sql
> > -- 分区表建立后，再添加字段，如何使得新字段能够查询？
> > -- 分区表建立后，再新增字段，即使赋值，查询时也是NULL值
> > alter table dept_partition add columns(test string);
> > insert overwrite table dept_partition values(1000,'测试部',2100,'test','20200401'); --写入/重写数据后，test字段依然为NULL （如下图1）
> > 
> > --20200401分区的test字段值变为test,该分区包含test字段
> > alter table dept_partition partition(day='20200401') add columns(test string); 
> > 
> > insert into table dept_partition values(10,'开发部',2100,'ybLlcodes','20200401'); -- test字段显示为ybllcodes
> > insert into table dept_partition values(20,'开发部',2100,'ybLlcodes2','20200402'); -- test字段显示为NULL,因为是20200402分区
> > 
> > alter table dept_partition partition(day='20200402',day='20200403') add columns(test string); --此时，其他两个分区也包含test字段
> > ```
> >
> > + ![image-20220707102437918](http://ybll.vip/md-imgs/202207071024044.png)
> >
> > +++
>
> 2. 二级分区
>
> > + 相当于再多一层目录，用以分割大的数据集
> >
> > ```sql
> > -- 创建二级分区
> > create table dept_partition2(
> >     deptno int,
> >     dname string,
> >     loc string
> > )
> > partitioned by(day string,hour string)
> > row format delimited fields terminated by '\t';
> > 
> > -- 加载数据
> > load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200401.log' into table dept_partition2 partition(day='20200401',hour='11');
> > 
> > --查询数据
> > select * from dept_partition2 where day='20200401' and hour='11';
> > ```
> >
> > +++
>
> 3. 分区表与数据产生关联的三种方式
>
> ```sql
> -- 1.上传数据后修复
> dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=12;
> dfs -put /opt/module/hive-3.1.2/datas/dept_20200402.log /user/hive/warehouse/dept_partition2/day=20200401/hour=12;
> --数据上传完毕（指定目录、数据文件）
> -- 修复数据
> msck repair table dept_partition2;
> 
> 
> -- 2.上传数据后添加分区
> dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200402/hour=13;
> dfs -put /opt/module/hive-3.1.2/datas/dept_20200403.log /user/hive/warehouse/dept_partition2/day=20200402/hour=13;
> --数据上传完毕（指定目录、数据文件）
> --通过命令,给表添加分区
> alter table dept_partition2 add partition(day='20200402',hour='13');
> 
> 
> -- 3.创建文件夹后load数据
> dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200403/hour=14;
> -- load上传数据时，指定分区
> load data local inpath '/opt/module/hive-3.1.2/datas/dept_20200401.log' 
> into table dept_partition2 partition(day='20200403',hour='14');
> ```
>
> +++
>
> 4. 动态分区
>
> > 引言：关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。
> >
> > ```sql
> > set hive.exec.dynamic.partition=true; --开启动态分区功能，默认开启
> > set hive.exec.dynamic.partition.mode=nonstrict; --设置为非严格模式
> > set hive.exec.max.dynamic.partitions=1000; --所有mr节点，最大一共创建多少个动态分区
> > set hive.exec.max.dynamic.partitions.pernode=100;
> > set hive.exec.max.created.files=10000;
> > set hive.error.on.empty.partition=false;
> > 
> > --案例
> > create table dept_partition_dynamic(
> >     id int,
> >     name string
> > )
> > partitioned by (loc int)
> > row format delimited fields terminated by '\t';
> > 
> > -- 插入数据
> > insert into table dept_partition_dynamic partition(loc)
> > select deptno,dname,loc from dept;
> > ```
> >
> > +++
>
> 5. 分桶表(分区表更重要)
>
> > + 对于一张表或一个分区，Hive可以进一步阻止成桶，即更细粒度的数据范围划分
> > + 分桶是将数据集分解成更容易管理的若干部分的一个技术
> > + 分区针对的是数据的存储路径(分目录)；分桶这是针对数据文件(分文件存储)
> >
> > ```sql
> > -- 想要将表创建为4个桶，需要将hive中mapreduce.job.reduces参数设置为>=4或设置为-1;
> > set mapreduce.job.reduces=4;
> > -- 创建分桶表
> > create table stu_bucket(id int, name string)
> > clustered by(id) 
> > into 4 buckets
> > row format delimited fields terminated by '\t';
> > -- 查看表结构
> > desc formatted stu_bucket;
> > -- 导入数据
> > load data local inpath '/opt/module/../student.txt' into table stu_bucket; 
> > 
> > -- 分桶规则
> >   --Hive的分桶采用对分桶字段的值进行哈希
> >   --然后对桶的个数进行取模的方式决定该条记录放入哪个桶中
> > -- 注意事项
> >   --mapreduce.job.reduces=-1,(或者设置大于等于桶数)
> >   --最好从hdfs上load数据，因为会跑mr，避免本地文件找不到(或者开启本地模式)
> >   
> > -- insert的方式将数据导入分桶表
> > truncate table sut_bucket
> > insert into table stu_bucket select * from student;
> > ```
> >
> > 
>
> +++



##### 2.5 函数

> https://hive.apache.org/    ->   `User and Hive SQL documentation` -> `Hive SQL Language Manual`  ->  `Operators and User-Defined Functions (UDFs)`

```sql
show functions; --查看系统自带函数
desc function abs; --显示自带函数的用法
desc function extended abs; --详细显示自带函数的用法
--
--nvl()
select ename,comm,nvl(comm,0) from emp;

--case 和 if
-- case when then else end
select
    dept_id,
    sum(case sex when '男' then 1 else 0 end) as man,
    sum(case sex when '女' then 1 else 0 end) as woman
from emp_sex
group by dept_id;
-- if(p1,p2,p3)
select
    dept_id,
    sum(if(sex='男',1,0)) num_man,
    sum(if(sex='女',1,0)) num_woman
from emp_sex
group by dept_id;


--行转列：(UDAF:多行变一列)
select concat(1,2,3,4);
concat_ws("-",arr<string>)

select collect_set(friends) from personinfo;
-- [["bingbing","lili"],["caicai","susu"],["sanluo","dalao"]]
select collect_set(children) from personinfo;
-- [{"xiao song":18,"xiaoxiao song":19},{"xiao yang":18,"xiaoxiao yang":19},{"xiao ou":10,"xiao ming":22}]
select collect_set(address) from personinfo;
-- [{"street":"hui long guan","city":"beijing"},{"street":"chao yang","city":"beijing"},{"street":"bao an","city":"shenzhen"}]
select collect_list() -- 不去重，同样返回array类型
select collect_set() -- 去重，返回array类型


--列转行(UDTF：输入一列，输出多行的函数)
-- explode():将一个array|map转换成多行|多行多列
select explode(friends) from personinfo where name='ybllcodes';
+---------+
|   col   |
+---------+
| sanluo  |
| dalao   |
+---------+
select explode(children) from personinfo where name='ybllcodes';
+------------+--------+
|    key     | value  |
+------------+--------+
| xiao ou    | 10     |
| xiao ming  | 22     |
+------------+--------+
--出错，explode()的参数只能是map或者array,不能是struct
select explode(address) from personinfo where name='ybllcodes';

-- split(string str,string regex):按照regex字符串分割str
-- 分割后产生字符串数组 array<string>
select split('oneAtwoBthreeC', '[ABC]');
    -- ["one", "two", "three"]

-- lateral view:用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UDTF会将一字段拆分成一行或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。
SELECT movie,category_name 
FROM movie_info 
lateral VIEW
explode(split(category,",")) movie_info_tmp  AS category_name ;
```

窗口函数(开窗函数)

> + 输入多行，计算后得到一列数据，并可以将该列分配到每一行都拥有，相当于为每一行都新增了一个列
> + 聚合函数：可以让多行数据，根据规定形成1列，max(),min(),sum().......
> + 窗口函数：也是输入多行数据(一个窗口)，为每行数据进行一次计算，返回一个值，即，每一行都会计算得到一个值
> + 灵活运用窗口函数可以解决很多复杂的问题，如去重、排名、同比及和环比、连续登录等
>
> ```sql
> -- 语法
> function() over([partition by ...] [order by ...]) [<window_expression>])
> -- function():支持的函数-聚合函数【sum(),max(),avg()】,排序函数【ntile(),rank(),dens_rank(),row_number()】,统计比较函数【lead(),lag(),first_value()】...
> -- over():开窗函数(窗口随着行的变化而变化),[partition by:按字段分区],[order by:将各个分区排序]
> -- window_expression:窗口边界设置
> 	-- n preceding(往前N行)  n following(往后N行)  current row(当前行)
> 	-- unbounded preceding(从分区起点开始)  unbounded following(到分区终点结束)
> 	
> -- 说明：
> -- 1. 不指定partition by,则是不分区，所有数据集都看作在同一个分区
> 	 -- 默认使用分区内所有行
> -- 2. 不指定order by,不对各分区进行排序，通常搭配与顺序无关的函数，如sum()
>      -- 默认使用从起点到当前行
> ```
>
> ```sql
> -- 数据准备 business.txt
> jack,2017-01-01,10
> tony,2017-01-02,15
> jack,2017-02-03,23
> tony,2017-01-04,29
> jack,2017-01-05,46
> jack,2017-04-06,42
> tony,2017-01-07,50
> jack,2017-01-08,55
> mart,2017-04-08,62
> mart,2017-04-09,68
> neil,2017-05-10,12
> mart,2017-04-11,75
> neil,2017-06-12,80
> mart,2017-04-13,94
> -- 创建表并导入数据
> create table if not exists business(
> 	name string,
> 	orderdate string,
> 	cost int
> )
> row format delimited
> fields terminated by '\t';
> -- 导入
> load data local inpath "/opt/module/..." into table business;
> ```
>
> ```sql
> -- 案例实战
> --1.查询在2017年4月购买过的顾客及总人数
> select
>     name,
>     count(1) over()
> from business
> where substr(orderdate,1,7)='2017-04'
> group by name;
> 
> --2.查询顾客的购买明细及月购买总额
> select
>     name,
>     orderdate,
>     cost,
>     sum(cost) over(partition by name,month(orderdate)) as mouth_all_cost
> from business;
> 
> -- 3.将每个顾客的cost按照日期进行累加
> select 
>     name,
>     cost,
>     orderdate,
>     sum(cost) over(partition by name order by orderdate rows between unbounded preceding and current row) now_all_cost
> from business;
> 
> --4.计算每个人连续两天的消费总额
> select
>     name,
>     cost,
>     orderdate,
>     sum(cost) over(partition by name order by orderdate rows between 1 preceding and current row ) twodays_cost
> from business;
> 
> --5.查看顾客上次购买的时间
> -- lag() 必须跟开窗函数over() //往前走
> -- lead() //往后走
> select
>     name,
>     orderdate,
>     lag(orderdate,1,"0000-00-00") over (partition by name order by orderdate) as last_cost
> from business;
> 
> --6.查询前20%的订单信息
>   -- ntile()
> select
>     b1.name,
>     b1.orderdate,
>     b1.cost
> from (
>          select name,
>                 orderdate,
>                 cost,
>                 ntile(5) over (order by orderdate) flag
>          from business
>      ) b1
> where flag=1;
> ```
>

> +++

Rank函数

```sql
-- rank() over():排序相同会重复，总数不变
-- dense_rank() over():排序相同会重复，总数减少
-- row_number() over():根据顺序排序
-- 三个函数无参数，配合窗口函数over()使用

select
    name,
    subject,
    score,
    rank() over (partition by subject order by score desc) as rank,
    dense_rank() over (partition by subject order by score desc) as dense_rank,
    row_number() over (partition by subject order by score desc) as row_number
from score
where subject='英语';

+-------+----------+--------+-------+-------------+-------------+
| name  | subject  | score  | rank  | dense_rank  | row_number  |
+-------+----------+--------+-------+-------------+-------------+
| 宋宋   | 英语     | 84     | 1     | 1           | 1           |
| 大海   | 英语     | 84     | 1     | 1           | 2           |
| 婷婷   | 英语     | 78     | 3     | 2           | 3           |
| 孙悟空  | 英语     | 68     | 4     | 3           | 4           |
+-------+----------+--------+-------+-------------+-------------+
```

自定义函数

> ① **UDF**（User-Defined-Function）--> **一进一出**
>
> ② **UDAF**（User-Defined Aggregation Function） --> **聚合函数**，**多进一出**，类似：count/max/min
>
> ③ **UDTF**（User-Defined Table-Generating Functions）--> **炸裂函数**，**一进多出**，如：explode()
>
> +++
>
> ```java
> //自定义UDF函数
> 
> ```
> 
>




### 三. 压缩和存储

##### 2.1  压缩

> Hive不会强制要求将数据转换成特定的格式才能使用。利用Hadoop的InputFormat API可以从不同数据源读取数据，使用OutputFormat API可以将数据写成不同的格式输出。
>
> 对数据进行压缩虽然会增加额外的CPU开销，但是会节约客观的磁盘空间，并且通过减少载入内存的数据量而提高I/O吞吐量会更加提高网络传输性能。
>
> 原则上Hadoop的job时I/O密集型的话就可以采用压缩可以提高性能，如果job是CPU密集型的话，那么使用压缩可能会降低执行性能。
>
> +++
>
> 123
>
> 123
>
> 

+++

##### 2.2 存储

> 行式存储
>
> > 123
>
> 列式存储
>
> > 123
> >
> > Orc 格式
> >
> > Parquet格式
>
> 文件存储格式对比测试
>
> > TextFile
> >
> > ```sql
> > create table log_text (
> > track_time string,
> > url string,
> > session_id string,
> > referer string,
> > ip string,
> > end_user_id string,
> > city_id string
> > )
> > row format delimited fields terminated by '\t'
> > stored as textfile;
> > 
> > -- 上传文件
> > load data local inpath '/opt/module/hive/datas/log.data' into table log_text ;
> > 
> > -- 查看表中数据大小
> > dfs -du -h /user/hive/warehouse/log_text;
> > ```
> >
> > Orc格式(自带压缩，需要先取消，再对比)
> >
> > ```sql
> > create table log_orc(
> > track_time string,
> > url string,
> > session_id string,
> > referer string,
> > ip string,
> > end_user_id string,
> > city_id string
> > )
> > row format delimited fields terminated by '\t'
> > stored as orc
> > tblproperties("orc.compress"="NONE"); -- 由于ORC格式时自带压缩的，这设置orc存储不使用压缩
> > 
> > -- 上传数据(从log_text表中拿数据)
> > insert into table log_orc select * from log_text;
> > 
> > -- 查看表中数据大小
> > dfs -du -h /user/hive/warehouse/log_orc/
> > ```
> >
> > Parquet格式
> >
> > ```sql
> > create table log_parquet(
> > track_time string,
> > url string,
> > session_id string,
> > referer string,
> > ip string,
> > end_user_id string,
> > city_id string
> > )
> > row format delimited fields terminated by '\t'
> > stored as parquet;
> > 
> > -- 上传数据
> > insert into table log_parquet select * from log_text;
> > 
> > -- 查看表中数据大小
> > dfs -du -h /user/hive/warehouse/log_parquet/;
> > ```

+++

##### 2.3 存储和压缩结合

> > 在实际的项目开发当中：
> >
> > 1. hive表的数据存储格式一般选择：orc或parquet
> > 2. 压缩方式一般选择snappy，lzo
> >
>
> +++
>



### 四. 调优

##### Explain 查看执行计划

> Explain 呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job 或者 Spark Job
>
> +++
>
> + 一个Stage对应的一个MapReduce Job
>
> + Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述
>
> + Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作
>
> + 如TableScan Operator，Select Operator，Join Operator等
>
>   > ![image-20220919085847373](http://ybll.vip/md-imgs/202209190858492.png)
>   >
>   > +++
>   >
>   > ![image-20220919085918154](http://ybll.vip/md-imgs/202209190859259.png)
>
> +++
>
> **基本语法**
>
> ```sql
> explain [formatted | extended | dependency ] query_sql
> --FORMATTED：将执行计划以JSON字符串的形式输出
> --EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息
> --DEPENDENCY：输出执行计划读取的表及分区
> ```

+++

##### 优化思路

> 开启 MapJoin ,（默认打开），大表join小表【将小表缓存到内存中】
>
> +++
>
> 谓词下推：将过滤操作前移，减少后续操作的数据量【where 提到 join 前执行】
>
> ```sql
> set hive.opttimize.ppd = true;
> ```
>
> + CBO优化，会自动完成一部分谓词下推的优化工作
>
>   ```sql
>   set hive.cbo.enable = true;
>   ```
>
> +++
>
> 小文件处理
>
> 1. CombinehiveinputFormat :将多个文件放到一起，同一切片，减少MapTask的数量，进而减少资源的使用
> 2. JVM重用
> 3. merge => 如果只有maponly,默认打开，如果是map,reduce都存在,需要手动打开【单独开启一个mr,将小于16m的文件合并到256】
>
> +++
>
> 创建分区表，防止全表扫描
>
> +++
>
> 采用压缩【map输出端：snappy，快速，减少网络I/O】
>
> 采用列式存储
>
> +++
>
> 合理设置map个数，reduce个数

+++

##### 数据倾斜

> 大部分reducetask执行成功，个别reducetask一直在running
>
> 最后一个reducetask执行成功的时间是第一个执行成功的时间的20倍
>
> +++
>
> **出现场景**
>
> + key为null:控制空值分布
>
> + 单表：Groupby
>
>   > 1. map-side
>   >
>   >    > 开启map-side,相当于在map阶段提前预聚合
>   >    >
>   >    > ```sql
>   >    > set hive.map.aggr = true;
>   >    > ```
>   >
>   > 2. skewindata
>   >
>   >    > + 在key末尾追加随机数，从而打散key,后再进行二次聚合，将随机数去掉，再聚合
>   >    >
>   >    > + 开启 skewindata,底层默认实现
>   >    >
>   >    >   ```sql
>   >    >   set hive.groupby.skewindata = true;
>   >    >   ```
>
>   +++
>
> + 多表：Join
>
>   > 大表 Join 小表  => 使用mapjoin
>   >
>   > ```sql
>   > set hive.auto.convert.join = true;
>   > #如何确定小表的 阈值大小?
>   > set hive.auto.convert.join.noconditionaltask.size
>   > ```
>   >
>   > +++
>   >
>   > 大表 Join 大表
>   >
>   > + 开启 skewjoin：默认10万条相同key,进入同一个reduce
>   >
>   >   ```sql
>   >   set hive.optimize.skewjoin = true;
>   >   
>   >   --触发skew join的阈值，若某个key的行数超过该参数值，则触发
>   >   set hive.skewjoin.key = 100000;
>   >   ```
>   >
>   > + SMB Join : 分桶且有序的大表，桶与桶之间进行Join
>   >
>   > + 左表打散，右表扩容
>   >
>   >   ```sql
>   >   select
>   >       *
>   >   from(
>   >       select --打散操作
>   >           concat(id,'_',cast(rand()*2 as int)) id,
>   >           value
>   >       from A
>   >   )ta
>   >   join(
>   >       select --扩容操作
>   >           concat(id,'_',0) id,
>   >           value
>   >       from B
>   >       union all
>   >       select
>   >           concat(id,'_',1) id,
>   >           value
>   >       from B
>   >   )tb
>   >   on ta.id=tb.id;
>   >   ```
>   >
>   >   





