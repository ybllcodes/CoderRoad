

# **一、Hadoop**

## 1.大数据概述

#### 	1.1  大数据概念

- 无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。

#### 	1.2 大数据特点 : 

> - Volume (大量的)：TB级数据，EB级数据
> - Velocity (高速的)：处理海量数据(EB,ZB)的效率至关重要
> - Variety (多样的)：结构化数据(数据库表/文本等和非结构化数据(网络日志、音频、视频、图片、地理位置等)
> - Value(低密度价值)：价值密度的高低与数据总量的大小成反比

#### 1.3 大数据部门内部组织结构

![](http://ybll.vip/md-imgs/202203291218400.png)

## 2.Hadoop入门

#### 2.1 Hadoop是什么

> **Hadoop：**是由Apache基金会所开发的**分布式系统基础架构**，主要解决海量数据的**存储**和海量数据的**分析计算问题**;Hadoop通常指Hadoop生态圈
>
> + logo是 大象

#### 2.2 Hadoop发展史

1. Hadoop创始人：Doug Cutting

2. Google是Hadoop的思想之源

   GFS -> HDFS ;   Map-Reduce -> MR ;   BigTable -> HBase 

3. 三大发行版本：
   1. Apache版本(免费)，入门最合适  `hadoop.apache.org`
   2. Cloudera :收费，内部集成了大数据框架，对应产品 CDH
   3. Hortonworks :文档较好，对应产品 HDP; 现被Cloudera收购，新产品 CDP

#### 2.3 Hadoop优势

1. 高可靠性：维护多个数据副本，即使某个出现故障，也不会导致数据丢失
2. 高扩展性：在集群间分配任务数据，可以方便的扩展数以千计的节点
3. 高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务的处理速度
4. 高容错性：能够自动将失败的任务重新分配

#### 2.4 Hadoop的组成

+ 1.x时代 MapReduce(计算+资源调度) HDFS(数据存储) Common(辅助工具)
+ 2.x / 3.x 时代，MapReduce(计算) Yarn(资源调度) HDFS(数据存储) Common(辅助工具)

***

## 3.Hadoop架构

#### 3.1 Hadoop概述

Hadoop Distributed File System, 是一个分布式文件系统

1. NameNode(nm): 存储元数据，如文件名，文件目录结构，文件属性，以及每个文件的块列表和块所在的DataNode 等
2. DataNode(dn):在本地文件系统存储文件块数据，以及块数据的校验和
3. Secondary NameNode(2nn):每隔一段时间对NameNode元数据进行备份

#### 3.2 YARN概述

Yet Another Resource Negotiator(另一种资源协调者) ，是Hadoop的资源管理器

+ ResourceManager(RM)：整个集群资源(内存、CPU)的老大

+ NodeManager(NM)：单个节点服务器的资源老大

+ ApplicationMaster(AM)：单个任务运行的老大

+ Container:容器，相当一台独立的服务器，里面封装了任务运行时所需的资源（内存、CPU、磁盘、网络等）

![Snipaste_2022-03-15_11-10-16](http://ybll.vip/md-imgs/202204011548458.png)

#### 3.3 MapReduce概述

计算过程分为两个阶段：Map阶段(并行处理输入数据）和 Reduce阶段(对Map结果进行汇总)

#### 3.4 三者关系图：

![Snipaste_2022-03-15_11-13-32](http://ybll.vip/md-imgs/202204011547532.png)

#### 3.5 大数据技术生态体系

##### ![Snipaste_2022-03-15_11-16-02](http://ybll.vip/md-imgs/202204011545720.png)技术框架简述

> - Sqoop:一款开源的工具，主要用于在Hadoop、Hive 与传统的数据库(MySQL)间进行数据的传递
> - Flume:一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统；Flume支持在日志系统中定制各种数据发送，用于收集数据
> - Kafka:一种高吞吐量的分布式发布/订阅消息系统
> - Spark:当前最流行的开源大数据内存计算框架，可基于Hadoop上存储的大数据进行计算
> - Flink:当前最流行的开源大数据内存计算框架，用于实时计算的场景较多
> - Oozie:一个管理Hadoop作业(job) 的工作流程调度管理系统
> - Hbase:一个分布式、面向列的开源数据库。不同于一般的关系数据库，适用于非结构化数据存储的数据库
> - Hive:基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，提供简单的SQL查询功能
> - Zookeeper:针对大型分布式系统的可靠协调系统，提供的功能：配置维护、名字服务、分布式同步、组服务等

***

## 4.Hadoop运行环境搭建

> 搭建后效果 hadoop102  hadoop103  hadoop104
>
> ![image-20220424162309187](http://ybll.vip/md-imgs/202204241623257.png)

+++

#### 4.1 运行前准备

1. 安装虚拟机，能ping通外网（能上网），本笔记以CentOS-7.5-x86-1804

   ```bash
   #1. 修改ip 
   vi /etc/sysconfig/network-scripts/ifcfg-ens33
   	#修改内容
   	BOOTPROTO=static
   	ONBOOT=yes
   	#添加内容
   	IPADDR=192.168.10.102
   	GATEWAY=192.168.10.2
   	DNS1=114.114.114.114
   	DNS2=8.8.8.8
   	
   
   #2. 重启network服务
   systemctl restart network
   
   #3. 修改虚拟机主机名和hosts文件 hadoop102
   vi /etc/hostname
   vi /etc/hosts
   	#添加如下内容
   	192.168.10.100 hadoop100
   	192.168.10.101 hadoop101
   	192.168.10.102 hadoop102
   	192.168.10.103 hadoop103
   	192.168.10.103 hadoop104
   	...(可以添加到108)
   #4. 修改 windows 下的hosts文件
   C:\Windows\System32\drivers\etc #该目录下添加内容和上面一样
   
   reboot #重启
   ```

   

2. 虚拟机指令列表：

   ```shell
   ping www.baidu.com #正常上网
   #5. 安装epel-release,"红帽系"的操作系统提供的软件包，适用于CentOS、RHEL、等，相当于软件仓库
   yum install -y epel-release
   yum install -y net-tools #工具包集合 包含ifconfig等
   yum install -y vim	#安装vim
   #其他工具，可以不安装
   yum install -y psmisc nc rsync lrzsz ntp libzstd openssl-static tree iotop git
   
   #6. 关闭防火墙
   systemctl stop firewalld	#关闭防火墙
   systemctl disable firewalld.service #关闭防火墙开机自启
   
   #7. 创建用户
   useradd atguigu
   passwd atguigu
   vim /etc/sudoers #给atguigu用户赋予root权限
   	#在 root ALL=(ALL) ALL 下面添加如下内容
   	atguigu ALL=(ALL) NOPASSWD:ALL
   	
   #8. 安装Jdk,配置环境变量
   rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps #卸载虚拟机自带的JDK
   reboot #可以选择重启
   tar -zxvf jdk.tar.gz -C /opt/module/ #解压jdk目录
   sudo vim /etc/profile.d/my_env.sh  #配置环境变量
   	export JAVA_HOME=/opt/module/jdk1.8.0_212
   	export PATH=$PATH:$JAVA_HOME/bin
   source /etc/profile
   java -version #检查jdk是否安装且环境变量是否安装成功
   
   #9. 安装Hadoop,配置环境变量
   tar -zxvf hadoop.tar.gz -C /opt/module/
   sudo vim /etc/profile.d/my_env.sh
   	export HADOOP_HOME=/opt/module/hadoop-3.1.3
   	export PATH=$PATH:$HADOOP_HOME/bin
   	export PATH=$PATH:$HADOOP_HOME/sbin
   source /etc/profile
   hadoop version #检查是否安装成功，且环境变量配置成功
   sudo reboot #可以选择
   
   #10. 完成以上步骤后，再克隆 hadoop102 hadoop103 hadoop104
   #修改hadoop103 hadoop104的主机名，主机ip...
   ```

3. Hadoop目录结构

   ![](http://ybll.vip/md-imgs/202204011549171.png)

   重要目录：

   ​	bin : 存放Hadoop相关服务(hdfs,yarn,mapred)进行操作的脚本

   ​	etc : Hadoop的配置文件目录

   ​	lib : 存放 Hadoop 的本地库(对数据进行压缩、解压缩功能)

   ​	sbin : 存放启动或停止 Hadoop 相关服务的脚本

   ​	share : 存放 Hadoop 的依赖jar包、文档、和官方案例

4. Hadoop的运行模式

   本地模式：单机运行，生产环境不用

   伪分布式模式：单机运行，具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境

   完全分布式模式：多台服务器组成分布式环境，生产环境使用

5. 本地运行模式代码演示

```bash
mkdir wcinput #hadoop安装目录下，创建文件夹，存放输入文件
vim wcinput/word.txt #编辑输入文件
#运行本地模式
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput
#查看运行结果
cat wcoutput/part-r-00000
```

#### *4.2 完全分布式运行模式*

##### 4.2.1  配置集群前准备

0. 集群的每台机器都需要安装Hadoop

1. 编写集群分发脚本 

   ```bash
   #scp 实现服务器之间的数据拷贝 语法：scp -r $pdir/$fname $user@$host:$pdir/$fname
   scp -r /opt/module/jdk root@hadoop103:/opt/module
   scp -r root@hadoop102:/opt/module/* root@hadoop104:/opt/module
   #rsync 远程同步工具 主要用于备份和镜像，具有速度快、避免复制相同内容和支持符号链接的优点
   #语法：rsync -av $pdir/fname $user@$host:$pdir/$fname
   #-a:归档拷贝  -v:显示复制过程
   rsync -av hadoop-3.1.3/ root@hadoop103:/opt/module/hadoop-3.1.3/
   ```

2. shell脚本代码

   + 创建 `xsync` 脚本文件

   ```bash
   #!/bin/bash
   if [ $# -lt 1 ]
   then
   	echo Not Enough Arguement!
   	exit;
   fi
   #遍历所有机器
   for host in hadoop102 hadoop103 hadoop104
   do
   	echo === $host ===
   	#当前主机，遍历所有文件
   	for file in $@
   	do
   		if [ -e $file ]
   		then
   			#获取父目录
   			pdir=$(cd -P $(dirname $file), pwd)
   			#获取当前文件的名称
   			fname=$(basename $file)
   			ssh $host "mkdir -p $pdir"
   			rsync -av $pdir/$fname $host:$pdir
   		else
   			echo $file does not exists!
   		fi
   	done
   done
   ```

   ```bash
   #后续使用
   chmod +x xsync #赋予可执行权限
   xsync /home/atguigu/bin	#分发该脚本
   sudo cp xsync /bin/ #复制脚本到全局目录，以便全局调用
   sudo ./bin/xsync etc/profile.d/my_env.sh	#同步环境变量配置
   ```

3. 配置SSH免密登录

   1）免密登录原理

   

![](http://ybll.vip/md-imgs202203291214444.png)

​	2) linux命令操作

```bash
##注意：不同用户之间的ssh公钥/私钥不一样
#下面操作在root[/root/.ssh]和atguigu两个用户都要执行 

#生成公钥和私钥
cd /home/atguigu/.ssh
ssh-keygen -t rsa #连敲三个回车，会生成两个文件：id_rsa(私钥) id_rsa.pub(公钥)
#将公钥拷贝到目标机器 [自己本身这台机器也要拷贝]
ssh-copy-id hadoop102
ssh-copy-id hadoop103
ssh-copy-id hadoop104
#.ssh目录下文件解释
#known_hosts : 记录ssh访问过计算机的公钥(public key)
#authorized_keys : 存放授权过的无密登录服务器公钥
```

![image-20220424180946666](http://ybll.vip/md-imgs/202204241809765.png)

##### 4.2.2 集群配置与启动（重点)

1. 配置文件说明

   + 默认配置文件：

   ​	`core-default.xml`  / ` hdfs-default.xml`  / ` yarn-default.xml`  / ` mapred-default.xml`

   + 自定义配置文件：

   ​	`core-site.xml`  /  `hdfs-site.xml`  /  `yarn-site.xml`  /  `mapred-site.xml`

   ​	文件位置：`$HADOOP_HOME/etc/hadoop`

   + 配置代码：

   ```xml
   <!--  #####################   1.配置core-site.xml  ########################-->
   <configuration>
   	<!-- 指定NameNode的地址 -->
       <property>
           <name>fs.defaultFS</name>
           <value>hdfs://hadoop102:8020</value>
   	</property>
       
   	<!-- 指定hadoop数据的存储目录 -->
       <property>
           <name>hadoop.tmp.dir</name>
           <value>/opt/module/hadoop-3.1.3/data</value>
   	</property>
       
   	<!-- 配置HDFS网页登录使用的静态用户为atguigu -->
       <property>
           <name>hadoop.http.staticuser.user</name>
           <value>atguigu</value>
   	</property>
       
       <!-- 解决hive无法启动 beeline -->
   	<!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 -->
       <property>
           <name>hadoop.proxyuser.atguigu.hosts</name>
           <value>*</value>
   	</property>
   	<!-- 配置该atguigu(superUser)允许通过代理用户所属组 -->
       <property>
           <name>hadoop.proxyuser.atguigu.groups</name>
           <value>*</value>
   	</property>
   </configuration>
   
   <!-- ##################### 2.配置hdfs-site.xml ########################-->
   <configuration>
   	<!-- nn web端访问地址-->
   	<property>
           <name>dfs.namenode.http-address</name>
           <value>hadoop102:9870</value>
       </property>
       
   	<!-- 2nn web端访问地址-->
       <property>
           <name>dfs.namenode.secondary.http-address</name>
           <value>hadoop104:9868</value>
       </property>
   </configuration>
   
   <!-- #####################  3.配置yarn-site.xml  ########################-->
   <configuration>
   	<!-- 指定MR走shuffle -->
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
   	</property>
       
   	<!-- 指定ResourceManager的地址-->
       <property>
           <name>yarn.resourcemanager.hostname</name>
           <value>hadoop103</value>
   	</property>
       
   	<!-- 环境变量的继承 -->
       <property>
           <name>yarn.nodemanager.env-whitelist</name>
           <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
   	</property>
       
   	<!-- yarn容器允许分配的最大最小内存 -->
       <property>
           <name>yarn.scheduler.minimum-allocation-mb</name>
           <value>512</value>
       </property>
       <property>
           <name>yarn.scheduler.maximum-allocation-mb</name>
           <value>4096</value>
   	</property>
       
   	<!-- yarn容器允许管理的物理内存大小 -->
       <property>
           <name>yarn.nodemanager.resource.memory-mb</name>
           <value>4096</value>
   	</property>
       
   	<!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
       <property>
           <name>yarn.nodemanager.pmem-check-enabled</name>
           <value>false</value>
       </property>
       <property>
           <name>yarn.nodemanager.vmem-check-enabled</name>
           <value>false</value>
       </property>
   </configuration>
   
   <!-- #####################  4.配置mapred-site.xml ########################-->
   <configuration>
   	<!-- 指定MapReduce程序运行在Yarn上 -->
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ```

   然后，在集群上分发配置好的配置文件：

   ```
   xsync /opt/module/hadoop-3.1.3/etc/hadoop/
   ```

   + 完整截图（包括后续历史和日志的配置，上面代码部分未使用）

     > + 以下是我实际配置后的代码，可以正常运行
     > + 前面代码部分有些未使用，可能后续会加

     + cat core-site.xml

     ![image-20220424184734315](http://ybll.vip/md-imgs/202204241847398.png)

     + cat hdfs-site.xml

     ![image-20220424184837635](http://ybll.vip/md-imgs/202204241848703.png)

     + cat yarn-site.xml

     ![image-20220424184949633](http://ybll.vip/md-imgs/202204241849710.png)

     + cat mapred-site.xml

     ![image-20220424185323888](http://ybll.vip/md-imgs/202204241853966.png)

+++

2. 启动集群

1）配置works ：

```bash
vim /opt/module/hadoop-3.1.3/etc/hadoop/workers
	#新增内容
	#不允许有多余空格，包括结尾和空行
	hadoop102
	hadoop103
	hadoop104
xsync /opt/module/hadoop-3.1.3/etc/  #同步置集群其他机器
```

2）**启动步骤(重点操作)**

```bash
#1.第一次启动，需要在hadoop102节点上格式化 NameNode 
#注意：格式化NameNode会产生新的集群Id,会导致NameNode和DataNode的集群id不一致而报错
#解决：停止namenode和datanode进程，删除集群所有机器的data和logs目录，再重新格式化
hdfs namenode -format
#2.启动HDFS (hadoop102上)
sbin/start-dfs.sh
#3.启动YARN (在ResourceManager的节点上启动-hadoop103)
sbin/start-yarn.sh
#4.查看浏览器是否能连接
http://hadoop102:9870 #HDFS上存储的数据信息 (HDFS-NameNode)
http://hadoop103:8088 #YARN上运行的Job信息 (YARN-ResourceManager)
```

3）简单使用

```bash
hadoop fs -mkdir /input #hdfs上创建目录
#上传小文件
hadoop fs -put $HADOOP_HOME/wcinput/word.txt /input
	#查看上传后文件存放位置
	/opt/module/hadoop-1.3.3/data/dfs/data/current/BP.../current/finalized/...
	
#从hdfs上下载
hadoop fs -get /jdk-.tar.gz ./
#执行wordcount程序 
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /inout /output
```

4）配置历史服务器（查看历史运行情况）

```xml
<!-- mapred-site.xml 添加配置 -->
<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop102:10020</value>
</property>
<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
```

```bash
#分发配置到集群其他机器
xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml
#hadoop102上启动历史服务器
mapred --daemon start historyserver
#查看历史服务器是否启动
jps

http://hadoop102:19888/jobhistory
```

5）配置日志的聚集

![](http://ybll.vip/md-imgs/202204011549063.png)

开启日志聚集功能，需要重新启动NodeManager、ResourceManager、HistoryServer

```xml
<!-- 配置yarn-site.xml-->
<!-- 开启日志聚集功能 -->
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>
<!-- 设置日志聚集服务器地址 -->
<property>  
    <name>yarn.log.server.url</name>  
    <value>http://hadoop102:19888/jobhistory/logs</value>
</property>
<!-- 设置日志保留时间为7天 -->
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
</property>
```

```bash
#分发配置到集群其他机器
xsync $HADOOP_HOME/etc/hadoop/yarn-site.xml
#关闭 NodeManager、ResourceManager(YARN-hadoop103) / HistoryServer(hadoop102)
sbin/stop-yarn.sh
mapred --daemon stop historyserver
#启动 YARN(hadoop103) / HistoryServer(hadoop102)
start-yarn.sh
mapred --daemon start historyserver
#测试删除hdfs上的文件
hadoop fs -rm -r /output
#查看日志
http://hadoop102:19888/jobhistory
```

##### 4.2.3 集群脚本

1. 集群 启动/停止 命令总结（sbin 目录下）

   ```bash
   #HDFS
   start-dfs.sh  / stop-dfs.sh
   #YARN
   start-yarn.sh / stop-yarn.sh
   #HDFS各个组件
   hdfs --daemon start/stop namenode/datanode/secondarynamenode
   #YARN各个组件
   yarn --daemon start/stop resourcemanager/nodemanager
   #启动/停止 历史服务器(日志聚集是在历史服务器中查看运行日志)
   mapred --daemon start/stop historyserver
   
   #刷新namenode
   hdfs dfsadmin -refreshNodes
   ```

2. 编写启动脚本

   ```shell
   #!/bin/bash
   if [ $# -lt 1 ]
   then
       echo "No Args Input..."
       exit ;
   fi
   case $1 in
   "start")
   	echo " =================== 启动 hadoop集群 ==================="
       echo " --------------- 启动 hdfs ---------------"
       ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
       echo " --------------- 启动 yarn ---------------"
       ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
       echo " --------------- 启动 historyserver ---------------"
       ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
   ;;
   "stop")
       echo " =================== 关闭 hadoop集群 ==================="
       echo " --------------- 关闭 historyserver ---------------"
       ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
       echo " --------------- 关闭 yarn ---------------"
       ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
       echo " --------------- 关闭 hdfs ---------------"
       ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
   ;;
   *)
       echo "Input Args Error..."
   ;;
   esac
   ```

##### 4.2.4 端口号总结

| 端口名称                   | Hadoop2.x | Hadoop3.x          |
| -------------------------- | --------- | ------------------ |
| NameNode内部通信(集群内部) | 8020/9000 | **8020**/9000/9800 |
| NameNode(http外部访问)     | **50070** | **9870**           |
| MapReduce 查看执行任务端口 | 8088      | 8088               |
| 历史服务器通信端口         | 19888     | **19888**          |

##### 4.2.5 集群时间同步 (了解)

1. 服务器在公网环境，可以不采用集群时间同步，因为服务器会定时和公网时间进行校准
2. 如果服务器在内网环境，必须配置集群时间同步，因为时间久了，会产生时间偏差，导致集群执行任务不同步
3. 如何同步：选择集群中一台机器作为时间服务器，其他所有机器与时间服务器定时同步时间（根据任务进行周期同步）
4. 步骤

```bash
#时间服务器配置(必须root用户下)
#查看ntpd服务状态和开机自启命令(需要关闭该服务，关闭服务后再修改配置，修改配置后再启动和开机自启)
sudo systemctl status ntpd    # dead
sudo systemctl is-enabled ntpd  # 输出 disable
sudo systemctl stop ntpd #若开启，则要关闭服务
sudo systemctl disable ntpd #取消开机自启动
#先校准hadoop102时间，关闭ntpd后执行
sudo ntpdate ntp2.aliyun.com  #校准aliyun的时间服务器

#第一步 修改ntp.conf配置文件
sudo vim /etc/ntp.conf 
#修改1 授权某些网段内的所有服务器均能从该机器查询和同步时间 (取消下面1行代码的注释，并修改成如下内容)
restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap
#修改2 集群在局域网中，不使用其他互联网上的时间 (注释下面4行代码)
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
#修改3 当该服务器丢失网络连接，依然可以采用本地时间作为时间服务器为其他机器提供时间同步(添加下面两行代码)
server 127.127.1.0
fudge 127.127.1.0 stratum 10

#第二步 修改时间服务器的ntps文件
sudo vim /etc/sysconfig/ntpd
	SYNC_HWCLOCK=yes #添加该行代码：让硬件时间和系统时间一起同步

sudo systemctl start ntpd #重启ntpd服务
sudo systemctl enable ntpd #设置ntpd服务开机自启

#第三步 配置其他服务器
sudo systemctl stop ntpd
sudo systemctl disable ntpd  #关闭所有节点的ntp服务和自启动
sudo crontab -e
	*/10 * * * * sudo ntpdate hadoop102 #编写定时任务
```



## 5.HDFS详解

#### 5.1 HDFS概述

1. 背景：随着数据量越来越大，一台机器存储不下所有数据，存储到多台机器上却不方便管理和维护，因此需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统，HDFS只是分布式文件管理系统中的一种

2. 定义：一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，很多服务器联合实现功能，且集群中的服务器各有各的角色

3. 使用场景：一次写入，多次读出，不支持文件的修改，适合用来做数据分析，不适合网盘应用

4. 优点：1）高容错性  2）适合处理大数据  3）可构建在廉价机器上

5. 缺点：1）不适合低延时数据访问     2）无法高效对**大量小文件**进行存储     3）不支持**并发写入和随机修改**

6. ###### HDFS组成架构

   1. NameNode(nm) : Master，是管理者；

      1）管理HDFS的名称空间            2）配置副本策略

      3）管理数据块(Block)映射信息  4）处理客户端请求

   2. DataNode : 就是Slave,NameNode下达命令，DataNode执行命令；

      1）存储实际的数据块       2）执行数据块 读/写操作

   3. Client : 客户端；1）文件切分，将文件切分成一个个Block,然后上传；  2）与NameNode交互，获取文件实际位置信息(DataNode中)；  3）与DataNode交互，读取/写入数据；  4）提供一些命令管理HDFS（如NameNode格式化等） ； 5）通过一些命令访问HDFS（增删改查操作）

   4. Secondary NameNode(2nn) : 当NameNode挂掉时，不会马上替换NameNode并提供服务；1）辅助NN,定期合并Fsimages 和Edits，并推动给NN;  2）紧急情况下，可辅助恢复NN

   5. **HDFS的文件块（Block)**-------------面试

      1. HDFS的文件在物理上是分块存储（Block），由 `dfs.blocksize` 配置决定，默认为128M(hadoop2.x)

      2. Block不能太大，也不能太小？（和磁盘传输速率(单位s)基本一致最好）

         1）设置太小，会增加寻址时间

         2） 设置太大，传输时间会明显大于定位时间，导致处理是会变慢

         3） ***HDFS块的大小设置取决于磁盘传输速率***

#### 5.2 HDFS的Shell操作（开发）

Shell命令合集

```shell
#基本语法
hadoop fs ...  或者   hdfs dfs ...
hadoop fs #展示所有命令
hadoop fs -help rm #查看rm该如何使用，展示该命令的参数
##上传
# -moveFromLocal 从本地剪切到HDFS 
touch test1.txt  #本地目录创建文件
hadoop fs -moveFromLocal ./text1.txt /sanguo/shuguo # 顺序:本地文件 Hdfs目录
# -copyFromLocal 从本地复制到HDFS
hadoop fs -copyFromLocal ./text2.txt /sanguo/shuguo
# -appendToFile 追加一个文件到已存在文件的末尾
touch test3.txt #在文件中输入些许内容
hadoop fs -appendToFile ./test3.txt /sanguo/shuguo.test2.txt
# -put 等同于copyFromLocal,复制文件到HDFS


##下载
# -copyToLocal 从HDFS中拷贝到本地
hadoop fs -copyToLocal /sanguo/shuguo/text1.txt ./dir #顺序:HDFS文件 本地目录
# -get 等同于-copyToLocal
hadoop fs -get /sanguo/shuguo/text2.txt ./dir
# -getmerge 合并下载多个文件
hadoop fs -getmerge /sanguo/shuguo/* ./merge.txt # HDFS目录 本地文件


##HDFS的直接操作  mkdir -ls cat cp mv tail rm rmdir du
#-chmod -chown -chgrp  修改权限，和Linux中用法一样
hadoop fs -ls / #显示目录信息
hadoop fs -mkdir #在HDFS上创建目录
hadoop fs -cat /sanguo/shuguo/test1.txt #查看文件
hadoop fs -cp /sanguo/shuugo/test2.txt /textn2.txt
hadoop fs -mv /sanguo/shuguo/text3.txt /
hadoop fs -tail /text3.txt #显示一个文件末尾1KB的数据
hadoop fs -rm -r /textn2.txt
hadoop fs -rmdir /sanguo/shuguo
hadoop fs -du -s -h /text3.txt #统计文件夹大小信息（***后续查询更多***）
hadoop fs -setrep 10 /test3.txt #设置文件副本数
#记录在NameNode元数据中，若集群只有3台机器，则只有3副本，若集群增加到10台，则副本数更新到10


hadoop fs -D dfs.replication=1 -copyFromLocal ./text2.txt /sanguo/shuguo
```

#### 5.3 HDFS的客户端操作(IDEA)

1. Windows下安装Hadoop开发环境

   1）下载安装包    2）配置环境变量    3）创建maven项目，导入依赖    4）写代码

   + 双击 `/bin` 目录下的 `winutils.exe` 
   + 若没有报错，可在 cmd 输入 `hadoop` 看是否配置成功(注意配好环境变量)
   + 若报错 `找不到MSVCR120.dll` ，则要安装微软运行库

2. 具体操作

   + pom.xml 引入依赖

     ```xml
     <dependencies>
         <!-- hadoop 客户端 -->
         <dependency>
             <groupId>org.apache.hadoop</groupId>
             <artifactId>hadoop-client</artifactId>
             <version>3.1.3</version>
         </dependency>
         <!-- 单元测试 -->
         <dependency>
             <groupId>junit</groupId>
             <artifactId>junit</artifactId>
             <version>4.12</version>
         </dependency>
         <!-- log4j(日志管理) 打印日志信息 -->
         <dependency>
             <groupId>org.slf4j</groupId>
             <artifactId>slf4j-log4j12</artifactId>
             <version>1.7.30</version>
         </dependency>
     </dependencies>
     ```

   + resources 目录下添加日志配置，`log4j.properties

     ```log
     log4j.rootLogger=INFO, stdout  
     log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
     log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
     log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
     log4j.appender.logfile=org.apache.log4j.FileAppender  
     log4j.appender.logfile.File=target/spring.log  
     log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
     log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
     ```

3. 具体代码实现

   + 创建客户端对象

     ```java
     Configuration conf = new Configuration(); //配置core-site.xml中配置项可通过此对象,set()方法
         //get(final URI uri,final Configuration conf,final String user)
         //uri:HDFS的地址（NameNode的地址)dfs
         //conf:在conf中设置参数
         //user:指定操作集群的用户
         URI uri = new URI("hdfs://hadoop102:8020");
         fs = FileSystem.get(uri,conf,"auguigu")
     ```
     
   + 具体操作(上传，下载 ... )

     ```java
     fs.mkdirs(new Path("/xiyouji/huaguoshan/"));
     
     //文件上传
     fs.copyFromLocalFile(false,  //delSrc是否删除源文件
          true,  //true:覆盖; false:不覆盖，要求hdfs不能存在，否则报错
          //本地路径,可以是文件或者目录
          new Path("D:\\sunwukong.txt"), 
          // hdfs存储目录,一般是目录；若是文件，则本地路径必须是文件，然后要么覆盖，要么创建
          new Path("/xiyou/huaguoshan"));  
     
     //文件下载
     fs.copyToLocalFile(false, //是否删除hdfs上的源文件
     	new Path("/xiyou/huaguoshan"), //hdfs中的文件或目录
         new Path("D://"), //本地路径
         //false时会生成crc文件(校验),ture时不进行校验                  
         true);
     
     //文件删除
     fs.delete(new Path("/jdk-8u212-linux-x64.tar.gz"),false)
     //文件重命名
     fs.rename(new Path("/xiyou/huaguoshan/swk.txt"),
         new Path("/yb.txt"));
         //fs.rename(new Path("/xiyou/huaguoshan/sunwukong.txt"),new Path("/xiyou/huaguoshan/swk.txt"));
     ```
     
   + 关闭资源

     ```java
     fs.close();
     ```

   4. 示例代码

      ```java
      /**
       * 客户端代码常用套路
       * 1、获取一个客户端对象
       * 2、执行相关的操作命令
       * 3、关闭资源
       * HDFS  zookeeper
       */
      public class HdfsClient {
          private FileSystem fs;
          @Before
          public void init() throws URISyntaxException, IOException, InterruptedException {
              // 连接的集群nn地址
              URI uri = new URI("hdfs://hadoop102:8020");
              // 创建一个配置文件
              Configuration configuration = new Configuration();
              configuration.set("dfs.replication", "2");
              // 用户
              String user = "atguigu";
              // 1 获取到了客户端对象
              fs = FileSystem.get(uri, configuration, user);
          }
      
          @After
          public void close() throws IOException {
              // 3 关闭资源
              fs.close();
          }
      
          // 创建目录
          @Test
          public void testmkdir() throws URISyntaxException, IOException, InterruptedException {
              // 2 创建一个文件夹
              fs.mkdirs(new Path("/xiyou/huaguoshan1"));
          }
      
          // 上传
          /**
           * 参数优先级
           * hdfs-default.xml => hdfs-site.xml=> 在项目资源目录下的配置文件 =》代码里面的配置
           *
           * @throws IOException
           */
          @Test
          public void testPut() throws IOException {
              // 参数解读：参数一：表示是否删除原数据； 参数二：是否允许覆盖；参数三：原数据本地路径； 参数四：目的地hdfs路径
              fs.copyFromLocalFile(false, true, new Path("D:\\sunwukong.txt"), new Path("hdfs://hadoop102/xiyou/huaguoshan"));
          }
      
          @Test
          public void testPut2() throws IOException {
              FSDataOutputStream fos = fs.create(new Path("/input"));
              fos.write("hello world".getBytes());
          }
      
          // 文件下载
          @Test
          public void testGet() throws IOException {
              // 参数的解读：参数一：原文件是否删除；参数二：原文件路径HDFS； 参数三：目标地址路径Win ; 参数四：
              //fs.copyToLocalFile(true, new Path("hdfs://hadoop102/xiyou/huaguoshan/"), new Path("D:\\"), true);
              fs.copyToLocalFile(false, new Path("hdfs://hadoop102/a.txt"), new Path("D:\\"), false);
          }
      
          // 删除
          @Test
          public void testRm() throws IOException {
      
              // 参数解读：参数1：要删除的路径； 参数2 ： 是否递归删除
              // 删除文件
              //fs.delete(new Path("/jdk-8u212-linux-x64.tar.gz"),false);
      
              // 删除空目录
              //fs.delete(new Path("/xiyou"), false);
      
              // 删除非空目录
              fs.delete(new Path("/jinguo"), true);
          }
      
          // 文件的更名和移动
          @Test
          public void testmv() throws IOException {
              // 参数解读：参数1 ：原文件路径； 参数2 ：目标文件路径
              // 对文件名称的修改
              //fs.rename(new Path("/input/word.txt"), new Path("/input/ss.txt"));
      
              // 文件的移动和更名
              //fs.rename(new Path("/input/ss.txt"),new Path("/cls.txt"));
      
              // 目录更名
              fs.rename(new Path("/input"), new Path("/output"));
      
          }
      
          // 获取文件详细信息
          @Test
          public void fileDetail() throws IOException {
      
              // 获取所有文件信息 true:遍历获取
              RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);
      
              // 遍历文件
              while (listFiles.hasNext()) {
                  LocatedFileStatus fileStatus = listFiles.next();
      
                  System.out.println("==========" + fileStatus.getPath() + "=========");
                  System.out.println(fileStatus.getPermission());
                  System.out.println(fileStatus.getOwner());
                  System.out.println(fileStatus.getGroup());
                  System.out.println(fileStatus.getLen());
                  System.out.println(fileStatus.getModificationTime());//上次修改时间
                  System.out.println(fileStatus.getReplication());
                  System.out.println(fileStatus.getBlockSize());
                  System.out.println(fileStatus.getPath().getName());
      
                  // 获取块信息
                  BlockLocation[] blockLocations = fileStatus.getBlockLocations();
                  System.out.println(Arrays.toString(blockLocations));
              }
          }
      
          // 判断是文件夹还是文件
          @Test
          public void testFile() throws IOException {
      
              FileStatus[] listStatus = fs.listStatus(new Path("/"));
      
              for (FileStatus status : listStatus) {
                  if (status.isFile()) {
                      System.out.println("文件：" + status.getPath().getName());
                  } else {
                      System.out.println("目录：" + status.getPath().getName());
                  }
              }
          }
      }
      ```

      

#### 5.4 HDFS的读写流程

##### 5.4.1 写数据流程

![image-20220426221512227](http://ybll.vip/md-imgs/202204262215451.png)

```
说明
1. 客户端通过 DistributedFileSystem(分布式文件系统对象,不能选本地) 模块向NameNode请求上传文件(/user/atguigu/ss.avi)，NameNode检查权限、目标文件是否存在、父目录是否存在等
2. NameNode响应，向客户端回复信息
3. 客户端向NameNode请求第一个Block发送哪里(哪一个DataNode)
4. NameNode返回第一个Block存储的节点(包括多个副本节点),dn1,dn2,dn3
5. 客户端创建数据流(如 FSDataOutputStream)请求dn1上传数据,dn1收到请求会请求dn2,dn2会请求dn3,最后通信管道建立完成
6. 通信管道建立后，逐级应答客户端;dn3应答dn2,dn2应答dn1,dn1应答客户端
7. 客户端向dn1上传第一个Block(先从磁盘读取数据放到本地内存缓存),以Packet(64k-[chunk 512b;chunksum 4b; 收集到64k时以packet为单位开始传送])为单位,dn1收到一个Packet就会传给dn2，dn2收到后就会传给dn3,dn1每传一个packet，会放一个应答队列等待应答(逐级应答) {客户端有一个数据缓冲队列(发送数据) 和 一个应答队列(收到应答后删除，否则重发)}
8. 当第一个Block传输完成后，客户端会再次请求NameNode上传下一个Block(步骤3-7)
```

> 自我理解：
>
> 1. 客户端向 NameNode 请求写操作(指定目录，文件)，NameNode检查权限，目标文件是否存在，父目录是否存在等；然后响应客户端请求
> 2. 客户端请求上传Block块（第一个文件块），请求NameNode返回 DataNode 地址；【NameNode选择文件块的存储副本（机架感知）,记录元数据，然后返还给客户端 DataNode 节点】
> 3. 客户端向其中一个节点(dn1)请求建立block传输通道，dn1收到请求后，主动请求另一个节点(dn2),dn2再请求dn3，直到通信管道建立完成【通信管道建立完成后，逐级进行应答，有dn1最终应答给客户端】
> 4. 客户端向 dn1 上传第一个Block【以Packet(64k)为单位开始传送】，dn1每收到一个Packet,就会传送给dn2,dn2再传送给dn3【同样采用逐级传送，逐级应答，客户端最后收到dn1的应答信息（有一个数据缓存队列-发送数据，一个应答队列-接受应答，删除Packet,否则重发）】
> 5. 第一个Block传输完成，客户端通知NameNode,然后再请求下一个Block

##### 5.4.2 网络拓扑-节点计算

1. HDFS写数据过程中，NameNode会选择距离上传数据**最近距离的DataNode**接受数据
2. 节点距离：两个节点到达最近的共同祖先的距离总和

##### 5.4.3 机架感知(副本节点选择)

+ 源码：Crtl + n 查找BlockPlacementPolicyDefault类，在该类中查找chooseTargetInOrder方法

![image-20220628155243218](http://ybll.vip/md-imgs/202206281552339.png)

##### 5.4.4 读数据流程

![image-20220628155335773](http://ybll.vip/md-imgs/202206281553884.png)

```
解释说明：
1,2.客户端创建客户端对象(如 DistributedFileSystem)向 NameNode 请求下载文件，NameNode通过查询元数据，找到文件所在块的DataNode地址,返回给客户端
3.创建输入流对象(如 FSDataInputStream),挑选一台DataNode服务器(就近原则，负载均衡),请求读取数据
4.DataNode开始向客户端传送数据(从磁盘读取数据,以Packet为单位来做校验),串行读取数据(读完blk1,再读blk2)
5.客户端以Packet为单位接收，先在本地缓存，后再写入目标文件
```

> 自我理解：
>
> 1. 客户端请求从 HDFS 下载一个文件（文件路径，文件名），向 NameNode发送请求，NameNode应答客户端目标文件的元数据（NameNode节点等）
> 2. 客户端从应答中，挑选一台 DataNode(就近原则，负载均衡)，请求读取数据
> 3. DataNode向客户端传送数据【以Packet为单位作校验】
> 4. 客户端以Packet为单位接受，先在本地缓存，再写入目标文件（串行读取每个Block文件块）

#### 5.5 NN和2NN的工作机制(了解)

##### 5.5.1 工作机制

+ ![image-20220426230927522](http://ybll.vip/md-imgs/202204262309594.png)

+ > 1. NameNode启动,会加载`edits_inprogress_n1` 和 `fsimage`文件，二者合并就是最新数据(元数据)
  >
  > 2. 若客户端进行写操作(元数据发生变动)，会先将操作记录到`edits_inprogress_n1` ,然后再对内存数据进行修改
  >
  >    +++
  >
  > 3. 2nn会请求是否需要CheckPoint (1. 定时会请求执行 ,默认1h ; 2. `edits_inprogress_n1` 中数据较多时会请求执行)；而后开始执行CheckPoint
  >
  > 4. CheckPoint执行前，namenode会生成新的`edits_inprogress_n2`文件(后续新的更新操作记录在该文件)，然后原来的`edits_inprogress_n1`更名为 `edits_n1`
  >
  > 5. 2nn再从nn中拉取(拷贝) `fsimage` 和 `edits_n1` ；将其加载到内存并合并，得到`fsimage.chkpoint`文件(合并后的最新数据)；然后将其拷贝回nn
  >
  > 6. nn中，将`fsimage.chkpoint`更名为 `fsimage`（覆盖操作); 
  >
  > 7. 此时，`fsimage` 和 `edits_inprogress_n2`再合并即为最新数据(回到步骤1)
  >
  > + 由上得，nn会比2nn多一个`edits_inprogress_n2`

##### 5.5.2 Fsimage 和 Edits 

+ namenode中的位置：`/opt/module/hadoop-3.1.3/data/dfs/name/current`
  
  + ![image-20220426235952011](http://ybll.vip/md-imgs/202204262359103.png)
+ 2nn中的位置：`/opt/module/hadoop-3.1.3/data/dfs/namesecondary/current`
  
+ ![image-20220427000143062](http://ybll.vip/md-imgs/202204270001155.png)
  
+ 具体说明

  > 1. Fsimage文件 是HDFS文件系统元数据的一个**永久性检查点**，包含HDFS文件系统的**`所有目录和文件inode的序列化信息`**
  > 2. Edits文件 存放HDFS文件系统的**`所有更新操作的路径`**、客户端的所有写操作都会先被记录当中
  > 3. nn中有seen_txid文件，保存最新的edits文件名末尾的数字；2nn没有该文件
  > 4. NameNode每次启动时，都会合并Fsimage 和 Edits ,确保内存中的元数据是最新的，同步的
  > 5. nn 和 2nn 都有 `VERSION` ，内容都相同 ；namenode和datanode靠集群ID(`clusterID`)进行通信，所以必须保持一致

+ 查看 Fsimage 文件

  + ```shell
    hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后的输出路径
    #实操
    hdfs oiv -p XML -i fsimage_0000000000000005254 -o /home/atguigu/hdfs_fsimage.xml
    ```

  + 查看hdfs_fsimag.xml

  ```xml
  	<INodeSection>
  		<lastInodeId>18208</lastInodeId>
  		<numInodes>57</numInodes>
  		<inode>
  			<id>16385</id>
  			<type>DIRECTORY</type>
  			<name/>
  			<mtime>1650871921537</mtime>
  			<permission>atguigu:supergroup:0755</permission>
  			<nsquota>9223372036854775807</nsquota>
  			<dsquota>-1</dsquota>
  		</inode>
  		<inode>
  			<id>16494</id>
  			<type>DIRECTORY</type>
  			<name>user</name>
  			<mtime>1639210624027</mtime>
  			<permission>atguigu:supergroup:0755</permission>
  			<nsquota>-1</nsquota>
  			<dsquota>-1</dsquota>
  		</inode>
  		<inode>
  			<id>16659</id>
  			<type>FILE</type>
  			<name>000000_0</name>
  			<replication>3</replication>
  			<mtime>1639215535991</mtime>
  			<atime>1641617098360</atime>
  			<preferredBlockSize>134217728</preferredBlockSize>
  			<permission>atguigu:supergroup:0644</permission>
  			<blocks>
  				<block>
  					<id>1073741879</id>
  					<genstamp>1056</genstamp>
  					<numBytes>6</numBytes>
  				</block>
  			</blocks>
  			<storagePolicyId>0</storagePolicyId>
  		</inode>
          ...
  	</INodeSection>
  	...
  	<INodeDirectorySection>
  		<directory>
  			<parent>16385</parent>
  			<child>18192</child>
  			<child>18193</child>
  			<child>18194</child>
  			<child>18186</child>
  			<child>16494</child>
  		</directory>
  		<directory>
  			<parent>16494</parent>
  			<child>16495</child>
  		</directory>
  		<directory>
  			<parent>16740</parent>
  			<child>17040</child>
  			<child>18171</child>
  			<child>17053</child>
  			<child>17049</child>
  			<child>17039</child>
  			<child>16916</child>
  			<child>17030</child>
  			<child>17354</child>
  			<child>16741</child>
  			<child>16742</child>
  			<child>16743</child>
  		</directory>
          ...
  	</INodeDirectorySection>
  	...
  ```

  + <inode>里面的id标记每个文件或目录，记录 父/子层次关系等
  + Fsimage文件没有记录 `block块` 所对应的DataNode，因为集群启动后，是DataNode主动上报数据块信息

+ 查看Edits文件

  + 执行操作 ：创建 /edits_dir ,上传文件 

    ![image-20220427001437430](http://ybll.vip/md-imgs/202204270014540.png)

  + ```shell
    hdfs oev -p 文件类型 -i 编辑日志 -o 转换后的输出路径
    #实操
    hdfs oev -p XML -i  -o /hong/atguigu/hdfs_edits.xml
    ```

  + 查看 hdfs_edits.xml 

  ```xml
  <?xml version="1.0" encoding="UTF-8" standalone="yes"?>
  <EDITS>
    <EDITS_VERSION>-64</EDITS_VERSION>
    <RECORD>
      <OPCODE>OP_START_LOG_SEGMENT</OPCODE>
      <DATA>
        <TXID>5258</TXID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_MKDIR</OPCODE>
      <DATA>
        <TXID>5259</TXID>
        <LENGTH>0</LENGTH>
        <INODEID>18209</INODEID>
        <PATH>/edits_dir</PATH>
        <TIMESTAMP>1650989571144</TIMESTAMP>
        <PERMISSION_STATUS>
          <USERNAME>atguigu</USERNAME>
          <GROUPNAME>supergroup</GROUPNAME>
          <MODE>493</MODE>
        </PERMISSION_STATUS>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_ADD</OPCODE>
      <DATA>
        <TXID>5260</TXID>
        <LENGTH>0</LENGTH>
        <INODEID>18210</INODEID>
        <PATH>/edits_dir/hadoop-3.1.3.tar.gz</PATH>
        <REPLICATION>3</REPLICATION>
        <MTIME>1650989629800</MTIME>
        <ATIME>1650989629800</ATIME>
        <BLOCKSIZE>134217728</BLOCKSIZE>
        <CLIENT_NAME>DFSClient_NONMAPREDUCE_1260773612_30</CLIENT_NAME>
        <CLIENT_MACHINE>192.168.10.103</CLIENT_MACHINE>
        <OVERWRITE>false</OVERWRITE>
        <PERMISSION_STATUS>
          <USERNAME>atguigu</USERNAME>
          <GROUPNAME>supergroup</GROUPNAME>
          <MODE>420</MODE>
        </PERMISSION_STATUS>
        <ERASURE_CODING_POLICY_ID>0</ERASURE_CODING_POLICY_ID>
        <RPC_CLIENTID>4c7eecd3-2f5a-49af-88d3-6e69d252b92c</RPC_CLIENTID>
        <RPC_CALLID>852</RPC_CALLID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_ALLOCATE_BLOCK_ID</OPCODE>
      <DATA>
        <TXID>5261</TXID>
        <BLOCK_ID>1073742248</BLOCK_ID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_SET_GENSTAMP_V2</OPCODE>
      <DATA>
        <TXID>5262</TXID>
        <GENSTAMPV2>1425</GENSTAMPV2>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_ADD_BLOCK</OPCODE>
      <DATA>
        <TXID>5263</TXID>
        <PATH>/edits_dir/hadoop-3.1.3.tar.gz</PATH>
        <BLOCK>
          <BLOCK_ID>1073742248</BLOCK_ID>
          <NUM_BYTES>0</NUM_BYTES>
          <GENSTAMP>1425</GENSTAMP>
        </BLOCK>
        <RPC_CLIENTID/>
        <RPC_CALLID>-2</RPC_CALLID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_ALLOCATE_BLOCK_ID</OPCODE>
      <DATA>
        <TXID>5264</TXID>
        <BLOCK_ID>1073742249</BLOCK_ID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_SET_GENSTAMP_V2</OPCODE>
      <DATA>
        <TXID>5265</TXID>
        <GENSTAMPV2>1426</GENSTAMPV2>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_ADD_BLOCK</OPCODE>
      <DATA>
        <TXID>5266</TXID>
        <PATH>/edits_dir/hadoop-3.1.3.tar.gz</PATH>
        <BLOCK>
          <BLOCK_ID>1073742248</BLOCK_ID>
          <NUM_BYTES>134217728</NUM_BYTES>
          <GENSTAMP>1425</GENSTAMP>
        </BLOCK>
        <BLOCK>
          <BLOCK_ID>1073742249</BLOCK_ID>
          <NUM_BYTES>0</NUM_BYTES>
          <GENSTAMP>1426</GENSTAMP>
        </BLOCK>
        <RPC_CLIENTID/>
        <RPC_CALLID>-2</RPC_CALLID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_ALLOCATE_BLOCK_ID</OPCODE>
      <DATA>
        <TXID>5267</TXID>
        <BLOCK_ID>1073742250</BLOCK_ID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_SET_GENSTAMP_V2</OPCODE>
      <DATA>
        <TXID>5268</TXID>
        <GENSTAMPV2>1427</GENSTAMPV2>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_ADD_BLOCK</OPCODE>
      <DATA>
        <TXID>5269</TXID>
        <PATH>/edits_dir/hadoop-3.1.3.tar.gz</PATH>
        <BLOCK>
          <BLOCK_ID>1073742249</BLOCK_ID>
          <NUM_BYTES>134217728</NUM_BYTES>
          <GENSTAMP>1426</GENSTAMP>
        </BLOCK>
        <BLOCK>
          <BLOCK_ID>1073742250</BLOCK_ID>
          <NUM_BYTES>0</NUM_BYTES>
          <GENSTAMP>1427</GENSTAMP>
        </BLOCK>
        <RPC_CLIENTID/>
        <RPC_CALLID>-2</RPC_CALLID>
      </DATA>
    </RECORD>
    <RECORD>
      <OPCODE>OP_CLOSE</OPCODE>
      <DATA>
        <TXID>5270</TXID>
        <LENGTH>0</LENGTH>
        <INODEID>0</INODEID>
        <PATH>/edits_dir/hadoop-3.1.3.tar.gz</PATH>
        <REPLICATION>3</REPLICATION>
        <MTIME>1650989635955</MTIME>
        <ATIME>1650989629800</ATIME>
        <BLOCKSIZE>134217728</BLOCKSIZE>
        <CLIENT_NAME/>
        <CLIENT_MACHINE/>
        <OVERWRITE>false</OVERWRITE>
        <BLOCK>
          <BLOCK_ID>1073742248</BLOCK_ID>
          <NUM_BYTES>134217728</NUM_BYTES>
          <GENSTAMP>1425</GENSTAMP>
        </BLOCK>
        <BLOCK>
          <BLOCK_ID>1073742249</BLOCK_ID>
          <NUM_BYTES>134217728</NUM_BYTES>
          <GENSTAMP>1426</GENSTAMP>
        </BLOCK>
        <BLOCK>
          <BLOCK_ID>1073742250</BLOCK_ID>
          <NUM_BYTES>69640404</NUM_BYTES>
          <GENSTAMP>1427</GENSTAMP>
        </BLOCK>
        <PERMISSION_STATUS>
          <USERNAME>atguigu</USERNAME>
          <GROUPNAME>supergroup</GROUPNAME>
          <MODE>420</MODE>
        </PERMISSION_STATUS>
      </DATA>
    </RECORD>
  </EDITS>
  ```

##### 5.5.3 CheckPoint时间设置

> 1.  hdfs-default.xml中的配置项(以下是默认)
>
>    ```xml
>    <!--  单位:秒 -->
>    <property>
>      <name>dfs.namenode.checkpoint.period</name>
>      <value>3600s</value>
>    </property>
>    ```
>
>    ```xml
>    <!-- 1分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次-->
>    <property>
>      <name>dfs.namenode.checkpoint.txns</name>
>      <value>1000000</value>
>    <description>操作动作次数</description>
>    </property>
>    
>    <property>
>      <name>dfs.namenode.checkpoint.check.period</name>
>      <value>60s</value>
>    <description> 1分钟检查一次操作次数</description>
>    </property>
>    ```
>
> 2. 企业用得少，后续会搭建2个 nn ，实现高可用；而nn和2nn之间企业配置得少

+++

#### 5.6 DataNode的工作机制(理解)

> block存储路径 ： `/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-639904764-192.168.10.102-1629864286538/current/finalized/...`
>
> ![image-20220427160200554](http://ybll.vip/md-imgs/202204271602635.png)
>
> > blk_n1 : 是实际存储的数据
> >
> > blk_n1.meta : 存储数据长度，校验和，时间戳等...
>
> +++
>
> 工作机制：
>
> ![image-20220427155858659](http://ybll.vip/md-imgs/202204271558812.png)
>
> > 1. DataNode启动后，会主动向nn注册，报告自己存储的`block块`一切正常；nn收到注册请求后会记录在自己的元数据中，并回应DataNode
> > 2. 之后每隔周期时间(默认6h)向nn上报块信息，确保block块数据可靠性
> > 3. NN和DataNode之间会频繁通信(默认3s一次)，称为心跳，心跳会带有NN给DataNode的命令等
> > 4. 若未按时收到心跳，且超过 10min+ 30s 还未收到DataNode的心跳，则nn会认为该节点不可用(挂掉)
> >
> > + 6h周期汇报，是汇报 块信息，告诉nn哪些 块信息 是完整、正确的
> >
> >   + ```xml
> >     <!-- DN 周期汇报块信息的配置项 - hdfs-default.xml -->
> >     <!--  单位:毫秒 -->
> >     <property>
> >     	<name>dfs.blockreport.intervalMsec</name>
> >     	<value>21600000</value>
> >     	<description>
> >     		Determines block reporting interval in milliseconds.
> >     	</description>
> >     </property>
> >     ```
> >
> >   + ```xml
> >     <!-- DN 同时会周期扫描自己节点的块信息列表，默认也是6h(确保汇报地正确性) -->
> >     <!--  单位:秒 -->
> >     <property>
> >     	<name>dfs.datanode.directoryscan.interval</name>
> >     	<value>21600</value>
> >     	<description>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.
> >     	Support multiple time unit suffix(case insensitive), as described
> >     	in dfs.heartbeat.interval.
> >     	</description>
> >     </property>
> >     ```
> >
> > + 心跳是告诉nn，该DataNode还活着，没有挂掉(心跳值的配置项在后面-掉线时限参数设置 )
> >
> > + 集群可以安全地加入或者退出 机器
>
> +++
>
> 数据完整性
>
> > HDFS采用`crc校验方式`,确保数据的正确性 ===表情是个意外，哈哈哈![img](http://ybll.vip/md-imgs/202204271628155.PNG)
>
> +++
>
> 掉线时限参数设置
>
> > ![image-20220427164032633](http://ybll.vip/md-imgs/202204271640765.png)
> >
> > + ```xml
> >   <!-- 单位 ms -->
> >   <property>
> >       <name>dfs.namenode.heartbeat.recheck-interval</name>
> >       <value>300000</value>
> >   </property>
> >   <!-- 心跳时间周期 单位 s -->
> >   <property>
> >       <name>dfs.heartbeat.interval</name>
> >       <value>3</value>
> >   </property>
> >   ```

+++



## 6. MapReduce详解

#### 6.1 MapReduce概述

1. 定义

   MapReduce 是一个分布式运算程序的**编程框架**，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的**分布式运算程序**，并发运行在一个Hadoop集群上

2. 优点

   易于编程(用户只需要编写业务逻辑代码,然后结合自带默认组件)；

   有良好的扩展性(动态增加服务器，解决计算资源不够的问题)；

   高容错性；

   适合TB/PB级以上的海量数据的离线处理

3. 缺点

   不擅长实时计算；不擅长流式计算；不擅长有向无环图(DAG)计算

4. 核心思想

   ![image-20220427174354063](http://ybll.vip/md-imgs/202204271743255.png)

> 一个完整的MapReduce程序在分布式运行时有三个实例进程:
>
> 1. MrAppMaster:负责整个程序的过程调度以及状态协调 (是YARN中，ApplicationMaster的子进程)
> 2. MapTask:负责Map阶段的整个数据处理流程
> 3. ReduceTask:负责Reduce阶段的整个数据处理流程
>
> +++
>
> 一个MapReduce程序中Map阶段和Reduce阶段都只有一个，如果逻辑复杂，有多个MapReduce程序，则会串行运行
> 一个Map阶段中，能并发运行MapTask,互不干扰
> 一个Reduce阶段，也能并发ReduceTask,互不干扰，但是数据依赖于Map阶段所有MapTask并发实例的输出

+++

#### 6.2 MapReduce编程规范

##### 6.2.1 常用数据序列化类型

![](http://ybll.vip/md-imgs/202206281554867.png)

```
数据序列化类型：
BooleanWritable		ByteWritable
IntWritable		LongWritable
FloatWritable	DoubleWritable 
Text	MapWritable		ArrayWritable	NullWritable
```

##### 6.2.2 WordCount案例实操

```
用户编写的程序分为三个部分：Mapper、Reducer、Driver
Mapper阶段
1) 用户自定义Mapper要继承提供的父类
2) Mapper的输入数据/输出数据都是KV对的形式(K、V的类型可自定义)
3) Mapper中的业务逻辑写在map()方法中，每个<K,v>都会调用一次map()方法
Reducer阶段
1) 用户自定义Reducer要继承提供的父类
2) Reducer的输入类型对应Mapper的输出类型，也是<K,V>对
3) Reducer的业务逻辑写在reduce()方法中，每一组相同k的<K,V>组会调用一次reduce()方法
Driver阶段
相当于YARN集群的客户端，用于提交整个程序到YARN集群，提交的是封装了MapReducer程序相关运行参数的job对象
```

```xml
<!--准备阶段 添加依赖(hadoop-client)-->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.1.3</version>
</dependency>
<dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>4.12</version>
</dependency>
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-log4j12</artifactId>
    <version>1.7.30</version>
</dependency>

<!--log4j.properties 文件 -->
log4j.rootLogger=INFO, stdout  
log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
log4j.appender.logfile=org.apache.log4j.FileAppender  
log4j.appender.logfile.File=target/spring.log  
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```

+ 以下是在本地运行

```java
//Mapper类
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
public class WordCountMapper extends Mapper<LongWritable, Text, Text,IntWritable> {

    private Text outK = new Text(); //作为类属性，只创建一个对象，提高效率
    private IntWritable outV = new IntWritable(1);
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        //自定义逻辑代码
        String line = value.toString(); //得到一行数据
        String[] words = line.split(" "); //切割一行数据，得到每个单词数组
        for (String word : words) {
            outK.set(word);
            context.write(outK,outV ); //输出 <K,V>
        }
    }
}
//Reducer类
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
public class WordCountReducer extends Reducer<Text, IntWritable,Text,IntWritable> {
    private IntWritable outV = new IntWritable();
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        //用户自定义逻辑
        int sum=0;
        for (IntWritable value : values) {
            sum+=value.get();
        }
        outV.set(sum);
        //写出<K,V>
        context.write(key,outV);
    }
}
//Driver驱动类
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
public class WordCountDriver{
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        //1. 获取Job
        Configuration conf = new Configuration();
        Job wc_job = Job.getInstance(conf);
        //2. 设置jar包路径
        wc_job.setJarByClass(WordCountDriver.class);
        //3. 关联mapper和reducer
        wc_job.setMapperClass(WordCountMapper.class);
        wc_job.setReducerClass(WordCountReducer.class);
        //4. map输出的 <K,V> 类型
        wc_job.setMapOutputKeyClass(LongWritable.class);
        wc_job.setMapOutputValueClass(Text.class);
        //5. 设置最终输出的 <K,V> 类型
        wc_job.setMapOutputKeyClass(Text.class);
        wc_job.setMapOutputValueClass(IntWritable.class);
        //6. 设置输入/输出路径
        FileInputFormat.setInputPaths(wc_job,new Path("D:\\input\\inputword"));
        FileOutputFormat.setOutputPath(wc_job,new Path("D:\\output\\inputword_debug"));
        //7. 提交job
        boolean result = wc_job.waitForCompletion(true);
        System.exit(result?0:1);
    }
}
```

+ 以下是在集群下测试

```xml
<!-- 在pom.xml添加插件-->
<build>
    <plugins>
        <plugin>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>3.8.1</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
            </configuration>
        </plugin>
        <!-- 打包时将项目所需要的依赖也一并打包 -->
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <configuration>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
            </configuration>
            <executions>
                <execution>
                    <id>make-assembly</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

```bash
#启动集群，运行程序
sbin/start-dfs.sh #hadoop102上
sbin/start-yarn.sh #hadoop103上
hadoop jar wc.jar com.ybllcodes.mapreduce.wordcount.WordCountDriver /input /output
```

##### 6.2.3 Hadoop序列化

> 序列化，就是把内存中的对象转换成字节序列(或者其他数据传输协议)，以便于存储到磁盘和网络传输
>
> 反序列化，就是将收集到的字节序列(或其他数据传输协议) 或者是磁盘的持久化数据，转换成内存中的对象。
>
> hadoop序列化特点：
> + 紧凑 ：高效使用存储空间
> + 快速 ：读写数据的额外开销小
> + 互操作：支持多种语言的交互(java进行序列化，反序列化时可用其他语言)
>
> +++

+ 自定义bean对象实现序列化接口(Writable)

  > 1. 定义bean类，实现 `Writable` 接口
  > 2. 必须有空参构造器（反序列化时，会通过反射调用空参构造器）
  > 3. 重写序列化方法 `write()` 
  > 4. 重写反序列化方法 `readFields()`
  >
  > + 注意 
  >   + 类各个属性的序列化和反序列化的顺序必须一致；
  >   + 需要重写toString(),否则传输后默认得到的是地址值
  >   + 自定义Bean放在<K,V> 中的K时，需要实现Comparable接口(key必须支持排序)

+ 序列化案例实操

  > 统计每一个手机![img](http://ybll.vip/md-imgs/202205061228514.PNG)（意外）号耗费的总上行流量、总下行流量、总流量
  >
  > ![image-20220506122917109](http://ybll.vip/md-imgs/202205061229186.png)
  >
  > + Bean对象 `FlowBean.java`
  >
  >   ```java
  >   //1. 实现writable接口
  >   public class FlowBean implements Writable {
  >       private long upFlow;
  >       private long downFlow;
  >       private long sumFlow;
  >   
  >       //2. 空参构造
  >       public FlowBean(){}
  >       //3. 重写序列化和反序列化方法
  >       @Override
  >       public void write(DataOutput out) throws IOException {
  >           out.writeLong(upFlow);
  >           out.writeLong(downFlow);
  >           out.writeLong(sumFlow);
  >       }
  >   
  >       @Override
  >       public void readFields(DataInput in) throws IOException {
  >           this.upFlow = in.readLong();
  >           this.downFlow = in.readLong();
  >           this.sumFlow = in.readLong();
  >       }
  >       // 4. 重写toString
  >       @Override
  >       public String toString() {
  >           return upFlow + "\t" + downFlow + "\t" + sumFlow;
  >       }
  >   
  >       public long getUpFlow() {
  >           return upFlow;
  >       }
  >   
  >       public void setUpFlow(long upFlow) {
  >           this.upFlow = upFlow;
  >       }
  >   
  >       public long getDownFlow() {
  >           return downFlow;
  >       }
  >   
  >       public void setDownFlow(long downFlow) {
  >           this.downFlow = downFlow;
  >       }
  >   
  >       public long getSumFlow() {
  >           return sumFlow;
  >       }
  >   
  >       public void setSumFlow(long sumFlow) {
  >           this.sumFlow = sumFlow;
  >       }
  >       public void setSumFlow() {
  >           this.sumFlow = this.downFlow+ this.upFlow;
  >       }
  >   }
  >   ```
  >
  > + MR程序
  >
  >   ```java
  >   //FlowMapper
  >   public class FlowMapper extends Mapper<LongWritable, Text,Text,FlowBean> {
  >       private Text outK= new Text();
  >       private FlowBean outV = new FlowBean();
  >   
  >       @Override
  >       protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
  >           //1. 获取一行信息
  >           String line = value.toString();
  >           //2. 切割
  >           String[] words = line.split("\t");
  >           //3. 从数组中获取需要的数据
  >           //手机号
  >           String phone = words[1];
  >           //上行流量和下行流量
  >           String up = words[words.length - 3];
  >           String down = words[words.length - 2];
  >           //4. 封装 <K,V>,并写出
  >           outK.set(phone);
  >           outV.setUpFlow(Long.parseLong(up));
  >           outV.setDownFlow(Long.parseLong(down));
  >           outV.setSumFlow();
  >           context.write(outK,outV);
  >       }
  >   }
  >   
  >   //FlowReducer
  >   public class FlowReducer extends Reducer<Text,FlowBean,Text,FlowBean> {
  >       private FlowBean outV = new FlowBean();
  >       @Override
  >       protected void reduce(Text key, Iterable<FlowBean> values, Context context) throws IOException, InterruptedException {
  >           //1. 迭代器遍历
  >           long totalUp = 0;
  >           long totalDown = 0;
  >           for (FlowBean flowBean : values) {
  >               totalUp += flowBean.getUpFlow();
  >               totalDown += flowBean.getDownFlow();
  >           }
  >           //2. 封装 <K,V> , 并写出
  >           outV.setUpFlow(totalUp);
  >           outV.setDownFlow(totalDown);
  >           outV.setSumFlow();
  >           context.write(key,outV);
  >       }
  >   }
  >   
  >   //FlowDriver
  >   public class FlowDriver {
  >       public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
  >           // 1. 获取job对象
  >           Configuration conf = new Configuration();
  >           Job job = Job.getInstance(conf);
  >           // 2. 设置jar包
  >           job.setJarByClass(FlowDriver.class);
  >           // 3. 关联Mapper/Reducer 类
  >           job.setMapperClass(FlowMapper.class);
  >           job.setReducerClass(FlowReducer.class);
  >           // 4. 设置 Mapper 输出<K,V>
  >           job.setMapOutputKeyClass(Text.class);
  >           job.setMapOutputValueClass(FlowBean.class);
  >           // 5. 设置最终输出 <K,V>
  >           job.setOutputKeyClass(Text.class);
  >           job.setOutputValueClass(FlowBean.class);
  >           // 6. 输入、输出路径
  >           FileInputFormat.setInputPaths(job,new Path("D:\\input\\inputflow"));
  >           FileOutputFormat.setOutputPath(job,new Path("D:\\output\\inputflow"));
  >           // 7. 提交 job
  >           //true:控制台打印的日志信息更多 ，false:日志信息少
  >           boolean result = job.waitForCompletion(true);
  >           System.exit(result?0:1);
  >       }
  >   }
  >   ```

+++

#### 6.3 MR核心框架原理(重要)

![image-20220506132342704](http://ybll.vip/md-imgs/202205061323793.png)

##### MapReduce工作流程

> +++
>
> ![image-20220508161726245](http://ybll.vip/md-imgs/202205081617471.png)
>
> > 1. Driver提交job后，会先向HDFS上传任务执行信息（切片信息，执行jar包，配置xml【请求RM，返还提交路径-包含application_id】）
> >
> > 2. ResourceManager启动MrAppMaster,由这些执行信息计算出MapTask的数量；默认采用TextInputFormat读取输入数据，在map()方法中进行处理，输出<K,V>对
> >
> >    **————Shuffle开始————**
> >
> > 3. map()方法后，将<K,V>对写入环形缓存区【默认100M,80%后方向写入，准备溢写磁盘】；溢写磁盘前，默认由Key进行排序分区（快排），使得环形缓冲区每次溢写时，每个分区都会有一个溢写文件，且分区内有序
> >
> > 4. 同一个MapTask的可能有多次溢写操作，使得同一个分区也有多个溢写文件，再将这些溢写按分区进行合并（归并排序）
> >
> > 5. 可以选择使用Combiner进行预聚合（和Reduce类似，减少I/O传输，提高速率），最终，同一个MapTask针对每个分区，只产生一个文件写到磁盘
>
> +++
>
> ![image-20220508161820391](http://ybll.vip/md-imgs/202205081618573.png)
>
> > 1. 每个ReduceTask会从MapTask的输出结果中，拷贝对应同分区的溢写文件置于ReduceTask的本地磁盘【先拷贝置内存缓存，内存不够写入磁盘】
> >
> > 2. 对于同分区的文件，进行归并排序【使得相同的Key紧挨在一块】
> >
> > 3. 针对key进行分组，相同的Key在同一组，reduce()方法，每次读取一组的数据
> >
> >    **————Shuffle结束————**
> >
> > 4. reduce()方法中，针对相同的key，多个value放入Iterable(可遍历对象),进行处理，默认由TextOutputFormat进行输出，每个分区形成一个分区文件
> >
> > +++
> >
> > + Iterable 和 Iterator的区别
> >
> >   > + iiterator是迭代器对象（接口），iterable接口中定义了返回iterator的方法，相当于对iterator的封装
> >   > + 实现了iterable接口的类才支持for each循环，【List接口实现了iterable，而非实现iterator】
> >   > + 集合类实现iterable接口，而非iterator,因为集合类可能有多种遍历方式，可以在Interable中返回不同的Iterator(hasNext()和next()方法的不同实现)，从而实现不同的遍历方式
> >   > + https://blog.csdn.net/u012871914/article/details/107942018

+++

##### 6.3.1 输入的数据InputFormat

> + 默认是 FileInputFormat抽象类中的 `TextInputFormat` 一行一行地读取
>
>   ![image-20220506141647593](http://ybll.vip/md-imgs/202205061416690.png)
>
>   +++
>
>   + TextInputFormat
>
>     > 是默认的FileInputFormat实现类，按行读取每条记录 `<LongWritable,Text>` ,键是存储该行在整个文件中的偏移量，值是该行的内容，不包括行终止符(换行符和回车符)
>
> + MapTask并行度决定机制
>
>   + 数据块：Block 是HDFS上物理上把数据分成一块一块，数据块是HDFS存储数据单位。
>   + 数据切片：只在逻辑上对数据进行分片，是MR程序`输入数据的单位`,一个切片对应启动一个MapTask。
>   + ![image-20220506135245775](http://ybll.vip/md-imgs/202205061352970.png)
>
> + 切片源码分析
>
>   ![image-20220506144446003](http://ybll.vip/md-imgs/202205061444180.png)
>
>   +++
>
>   ![image-20220508152500892](http://ybll.vip/md-imgs/202205081525051.png)
>
>   +++
>
>   ![image-20220508152601857](http://ybll.vip/md-imgs/202205081526027.png)
>
>   +++
>
> + CombineTextInputFormat 切片机制
>
>   > + 使用场景：小文件过多的场景，可以将 `多个小文件` 从 `逻辑上` 规划到 `一个切片`中,这样，多个小文件就可以交给 `一个MapTask` 处理
>  > + 虚拟存储切片最大值设置(根据实际的小文件大小情况来设置具体的值)：
>   >   + `CombineTextInputFormat.setMaxInputSplitSize(job,4194304)`  // 4M
>  >   + ![image-20220508154511045](http://ybll.vip/md-imgs/202205081545234.png)
>   > + CombineTextInputFormat 案例实操
>   >
>   > > 需求：一个切片处理4个小文件
>   > >
>   > > + 在wordcount案例的 `Driver类` 添加配置代码
>   > > + ![image-20220508160010944](http://ybll.vip/md-imgs/202205081600025.png)
> 
> +++

##### 6.3.2 Shuffle

> + map()方法后，reduce()方法前的数据处理过程称作Shuffle
>
> + ![image-20220508162814835](http://ybll.vip/md-imgs/202205081628037.png)
>
> + Partition分区
>
>   + 自定义Partition步骤
>   1. 自定义类继承 `Partitioner` ，重写 `getPartition()` 方法 （控制分区逻辑代码）
>     2. 在 `Job驱动` 中设置自定义Partitioner，`job.setPartitionerClass(SelfPC.class)`
>     3. 设置ReduceTask的个数，`job.setNumReduceTasks(num)`
>
>   +++
>
>   Partition案例实操
>
>   + 分区 无序    
>
>     > 1.自定义Partitioner类,重写
>     >
>     > ```java
>     > public class PhonePartitioner extends Partitioner<Text,FlowBean> {
>     >     @Override
>     >     public int getPartition(Text text, FlowBean flowBean, int numPartitions) {
>     >         int partition;
>     >         String phone = text.toString();
>     >         String prePhone = phone.substring(0,3);
>     >         switch (prePhone){
>     >             case "136":
>     >                 partition = 0;
>     >                 break;
>     >             case "137":
>     >                 partition = 1;
>     >                 break;
>     >             case "138":
>     >                 partition = 2;
>     >                 break;
>     >             case "139":
>     >                 partition = 3;
>     >                 break;
>     >             default:
>     >                 partition = 4;
>     >         }
>     >         return partition;
>     >     }
>     > }
>     > ```
>     >
>     > 2. Driver中配置Job属性
>     >
>     > ```java
>     >   // 指定自定义分区器
>     >         job.setPartitionerClass(PhonePartitioner.class);
>     >         job.setNumReduceTasks(5);
>     > ```
>
>     +++
>
>   + 全局排序（一个分区，一个ReduceTask)
>
>     > 3. Key对象实现`WritableComparable`，重写comparaTo()方法，调用其进行比较，由此排序
>     >
>     > ```java
>     > public class FlowBean implements WritableComparable<FlowBean> {
>     >     @Override
>     >     public int compareTo(FlowBean o) {
>     >         return Long.compare(this.sumFlow,o.sumFlow);
>     >     }
>     > }
>     > ```
>
>   + 分区 有序
>
>     > ```java
>     > public class FlowBean extends Partitioner<Text,FlowBean> implements WritableComparable<FlowBean> {}
>     > ```
>     >
>     > + 根据Key分区，必须保证Key能够排序，自定义对象需要重写compareTo()方法
>
> +++

##### 6.3.3 输出的数据OutputFormat

> + OutputFormat是MapReduce输出的基类，默认是`TextOutputFormat`
>
> + ![image-20220629183413852](http://ybll.vip/md-imgs/202206291834923.png)
>
> + 自定义OutputFormat (输出数据到Mysql/HBase/Elasticsearch等等存储框架中)
>
>   + 步骤
>
>     > 1. 自定义类继承FileOutputFormat
>     > 2. 改写RecordWriter（自定义一个类，重写write()方法）
>
>   + 实操
>
>     > 1. 过滤log日志，包含atguigu的写入一个文件，不包含的输出到另一个文件
>     > 2. ![image-20220629183913524](http://ybll.vip/md-imgs/202206291839616.png)
>     > 3. 代码
>     >
>     > > + 自定义FileOutputFormat类
>     > >
>     > > ![image-20220629193128859](http://ybll.vip/md-imgs/202206291931935.png)
>     > >
>     > > + 自定义RecordWriter类，重写了write()方法
>     > >
>     > > ```java
>     > > public class myRecordWriter extends RecordWriter<Text, NullWritable> {
>     > > 
>     > >     private FSDataOutputStream guiguFOS;
>     > >     private FSDataOutputStream otherFOS;
>     > >     private FileSystem fs;
>     > >     //创建流对象，用于输出到文件，需要两个输出流
>     > >     public myRecordWriter(){ }
>     > > 
>     > >     public myRecordWriter(TaskAttemptContext job)  {
>     > >         Configuration conf = job.getConfiguration();
>     > >         try {
>     > >             //通过conf对象，得到两个输出流
>     > >             fs = FileSystem.get(conf);
>     > >             Path outputPath = FileOutputFormat.getOutputPath(job);
>     > >             guiguFOS = fs.create(new Path(outputPath,"guigu.txt"));
>     > >             otherFOS = fs.create(new Path(outputPath, "other.txt"));
>     > >         } catch (IOException e) {
>     > >             e.printStackTrace();
>     > >         }
>     > >     }
>     > >     /*
>     > >     重写该方法，往输出流中传输数据
>     > >      */
>     > >     @Override
>     > >     public void write(Text key, NullWritable value) throws IOException, InterruptedException {
>     > >         String line = key.toString() + "\n";
>     > >         boolean flag = line.contains("atguigu");
>     > >         if (flag){
>     > >             guiguFOS.write(line.getBytes());
>     > >         }else{
>     > >             otherFOS.write(line.getBytes());
>     > >         }
>     > >     }
>     > >     /*
>     > >     关闭资源
>     > >      */
>     > >     @Override
>     > >     public void close(TaskAttemptContext context) throws IOException, InterruptedException {
>     > >         if (guiguFOS != null){
>     > >             guiguFOS.close();
>     > >         }
>     > >         if (otherFOS != null) {
>     > >             otherFOS.close();
>     > >         }
>     > >     }
>     > > }
>     > > ```
>     > >
>     > > + Mapper | Reducer | Driver
>     > >
>     > > ```java
>     > > public class myOutputFormatMapper extends Mapper<LongWritable, Text,Text, NullWritable> {
>     > > 
>     > >     @Override
>     > >     protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
>     > > 
>     > >         context.write(value,NullWritable.get());
>     > >     }
>     > > }
>     > > 
>     > > public class myOutputFormatReducer extends Reducer<Text, NullWritable,Text,NullWritable> {
>     > >     @Override
>     > >     protected void reduce(Text key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
>     > >         for (NullWritable value : values) {
>     > >             context.write(key,value);
>     > >         }
>     > >     }
>     > > }
>     > > 
>     > > public class myOutputFormatDriver {
>     > >     public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
>     > > 
>     > >         Job job = Job.getInstance(new Configuration());
>     > > 
>     > >         job.setJarByClass(myOutputFormatDriver.class);
>     > >         job.setMapperClass(myOutputFormatMapper.class);
>     > >         job.setReducerClass(myOutputFormatReducer.class);
>     > > 
>     > >         job.setMapOutputKeyClass(Text.class);
>     > >         job.setMapOutputValueClass(NullWritable.class);
>     > >         job.setOutputKeyClass(Text.class);
>     > >         job.setOutputValueClass(NullWritable.class);
>     > > 
>     > >         FileInputFormat.setInputPaths(job,new Path("D:\\input\\inputoutputformat"));
>     > >         FileOutputFormat.setOutputPath(job,new Path("D:\\output\\inputoutputformat1"));
>     > >         //this case:设置自定义OutputFormat
>     > >         job.setOutputFormatClass(myOutputFormat.class);
>     > > 
>     > >         job.waitForCompletion(true);
>     > >     }
>     > > }
>     > > ```
>
> +++

##### 6.3.4 Join

> Map Join
>
> > + Map Join适用于一张表十分小、一张表很大的场景
> > + Map端缓存小表，进行业务逻辑处理，减少Reduce端的数据压力，尽可能减少数据倾斜
>
> Reduce Join
>
> > + 合并操作在Reduce阶段完成，Reduce端处理压力打，Map节点的运算负载低，资源利用率不高
> > + 容易产生数据倾斜

##### 6.3.5 MapReduce开发总结

```
1.输入数据接口：InputFormat
	1) 默认使用的实现类是：TextInputFormat
	2) TextInputFormat的功能逻辑是:一次读一行文本;然后将该行的起始偏移量作为key;行内容作为value返回。
	3) CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。
2.逻辑处理接口：Mapper 
	用户根据业务需求实现其中三个方法：map() setup() cleanup () 
3.Partitioner 分区
	1) 有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号;key.hashCode()&Integer.MAXVALUE % numReduces
	2) 如果业务上有特别的需求，可以自定义分区。
4.Comparable 排序
	1) 当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。
	2) 部分排序：对最终输出的每一个文件进行内部排序。
	3) 全排序：对所有数据进行排序，通常只有一个 Reduce。 （4）二次排序：排序的条件有两个。
5.Combiner 合并
	Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。
6.逻辑处理接口：Reducer
	用户根据业务需求实现其中三个方法：reduce() setup() cleanup () 
7.输出数据接口：OutputFormat
	1) 默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。
	2) 用户还可以自定义 OutputFormat
```

#### 6.4 压缩

`hadoop checknative` :查看所支持的压缩格式

![image-20220701212027927](http://ybll.vip/md-imgs/202207012120886.png)

> + 压缩好处和坏处
>   + 优点：减少磁盘IO,减少磁盘存储空间
>   + 缺点：增加CPU开销
> + 压缩原则
>   + 运算密集型Job,少用压缩
>   + IO密集型Job,多用压缩

+++

MR支持的压缩格式如下：

![image-20220701212335634](http://ybll.vip/md-imgs/202207012123720.png)

+++

性能比较（Snappy速度最快）

![image-20220701212415289](http://ybll.vip/md-imgs/202207012124373.png)

+++

+ 压缩方式选择：压缩/解压缩速度、压缩率、压缩后是否支持切片

+ 压缩位置选择：

  ![image-20220701212615685](http://ybll.vip/md-imgs/202207012126774.png)

+ 压缩配置

  > + ![image-20220701212706809](http://ybll.vip/md-imgs/202207012127885.png)
  > + ![image-20220701212736935](http://ybll.vip/md-imgs/202207012127029.png)

  +++

+ map端使用压缩

  > + ![image-20220701212903006](http://ybll.vip/md-imgs/202207012129088.png)
  > + ![image-20220630093953083](http://ybll.vip/md-imgs/202206300941355.png)

  +++

+ reduce端输出压缩开启

![image-20220630094157459](http://ybll.vip/md-imgs/202206300941560.png)

+++



## 7.YARN详解

#### 7.1 工作流程理解（重要）

> ![image-20220630183339096](http://ybll.vip/md-imgs/202206301833216.png)
>
> + Yarn工作机制
>
> ![image-20220630183430691](http://ybll.vip/md-imgs/202206301834803.png)
>
> > + 说明
> >
> >   > ![image-20220630190120600](http://ybll.vip/md-imgs/202206301901712.png)
> >
> > + **自我理解**
> >
> >   > 1. Driver提交job后，向RM申请一个Application,RM应答，返回给客户端资源提交路径以及application_id
> >   > 2. 客户端执行submit()方法，生成任务执行信息文件【jar包，job.xml，job.split ..】,并根据应答信息，将这些资源上传至hdfs
> >   > 3. 上传完毕，再次向RM请求运行MrApplicationMaster；RM将请求转换为一个Task，放入任务队列中，等待调度
> >   > 4. NodeManager领取到该Task，创建Container容器，启动MrApplicationMaster进程【此时可以在web界面查看该job的运行情况】，并从hdfs上下载执行资源到本地
> >   > 5. AppMaster以轮询的方式，通过RPC协议向RM申请资源（启动MaskTask 或者 启动ReduceTask)，申请得到资源后，与对应的NodeManager进行通信，创建容器
> >   > 6. 根据客户端提交的运行资源,将任务启动命令写入脚本，并通过脚本启动MapTask任务；或者根据MapTask的溢写输出文件，运行ReduceTask
> >   > 7. 该MR运行完毕后，AppMaster会向RM申请注销自己
> >
> > + 网络解释
> >
> > ![image-20220630185301619](http://ybll.vip/md-imgs/202206301853732.png)
> >

+++

#### 7.2 Job提交详解

> ![img](http://ybll.vip/md-imgs/202206301911061.png)
>
> 作业提交全过程详解
>
> （1）作业提交
>
> 第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。
>
> 第2步：Client向RM申请一个作业id。
>
> 第3步：RM给Client返回该job资源的提交路径和作业id。
>
> 第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。
>
> 第5步：Client提交完资源后，向RM申请运行MrAppMaster。
>
> （2）作业初始化
>
> 第6步：当RM收到Client的请求后，将该job添加到容量调度器中。
>
> 第7步：某一个空闲的NM领取到该Job。
>
> 第8步：该NM创建Container，并产生MRAppmaster。
>
> 第9步：下载Client提交的资源到本地。
>
> （3）任务分配
>
> 第10步：MrAppMaster向RM申请运行多个MapTask任务资源。
>
> 第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。
>
> （4）任务运行
>
> 第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。
>
> 第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。
>
> 第14步：ReduceTask向MapTask获取相应分区的数据。
>
> 第15步：程序运行完毕后，MR会向RM申请注销自己。
>
> （5）进度和状态更新
>
> YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。
>
> （6）作业完成
>
> 除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。

+++

#### 7.3 Yarn调度器和调度算法

> `FIFO` 、`容量调度器(Capacity Scheduler)`、`公平调度器(Fair Scheduler)`
>
> Apache Hadoop3.1.3默认是容量调度器
>
> ```xml
> <property>
>     <description>The class to use as the resource scheduler.</description>
>     <name>yarn.resourcemanager.scheduler.class</name>
> <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
> </property>
> ```
>
> CDH框架默认是公平调度器
>
> +++

+ FIFO : 单队列，根据作业提交顺序，先来先服务

  > ![image-20220704205938144](http://ybll.vip/md-imgs/202207042059252.png)

+ 容量调度器 : Yahoo开发的多用户调度器

  > ![image-20220704210034934](http://ybll.vip/md-imgs/202207042100037.png)
  >
  > +++
  >
  > ![image-20220704210106813](http://ybll.vip/md-imgs/202207042101907.png)
  >
  > +++

+ 公平调度器 : Facebook开发的多用户调度器

  > ![image-20220704210220389](http://ybll.vip/md-imgs/202207042102486.png)
  >
  > +++
  >
  > ![img](http://ybll.vip/md-imgs/202207042102112.png)
  >
  > +++
  >
  > ![image-20220704210259658](http://ybll.vip/md-imgs/202207042102763.png)
  >
  > +++
  >
  > ![img](http://ybll.vip/md-imgs/202207042103731.png)
  >
  > 
  >
  > +++
  >
  > ![img](http://ybll.vip/md-imgs/202207042103671.png)
  >
  > +++
  >
  > ![img](http://ybll.vip/md-imgs/202207042103223.png)
  >

  +++

+++

#### 7.4 Yarn案例实操

##### 7.4.1 核心参数配置

```xml
<!-- 选择调度器，默认容量 -->
<property>
	<description>The class to use as the resource scheduler.</description>
	<name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

<!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） -->
<property>
	<description>Number of threads to handle scheduler interface.</description>
	<name>yarn.resourcemanager.scheduler.client.thread-count</name>
	<value>8</value>
</property>


<!--
是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 
-->
<property>
	<description>Flag to determine if logical processors(such as
	hyperthreads) should be counted as cores. Only applicable on Linux
	when yarn.nodemanager.resource.cpu-vcores is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true.
	</description>
	<name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
	<value>false</value>
</property>

<!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 -->
<property>
	<description>Enable auto-detection of node capabilities such as
	memory and CPU.
	</description>
	<name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
	<value>false</value>
</property>


<!--
Core转成Vcore的个数（虚拟核数和物理核数乘数，默认是1.0） 
hadoop中的vcore不是真正的core，通常vcore的个数设置为逻辑cpu个数的1~5倍。
-->
<property>
	<description>Multiplier to determine how to convert phyiscal cores to vcores. This value is used if 
yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and
yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The	number of vcores will be calculated as	number of CPUs * multiplier.
	</description>
	<name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
	<value>1.0</value>
</property>

<!-- NodeManager使用内存数，默认8G，修改为4G内存 -->
<property>
	<description>Amount of physical memory, in MB, that can be allocated 
	for containers. If set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically calculated(in case of Windows and Linux).
	In other cases, the default is 8192MB.
	</description>
	<name>yarn.nodemanager.resource.memory-mb</name>
	<value>4096</value>
</property>

<!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 -->
<property>
	<description>Number of vcores that can be allocated
	for containers. This is used by the RM scheduler when allocating
	resources for containers. This is not used to limit the number of
	CPUs used by YARN containers. If it is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically determined from the hardware in case of Windows and Linux.
	In other cases, number of vcores is 8 by default.</description>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
	<value>4</value>
</property>

<!-- 容器最小内存，默认1G -->
<property>
	<description>The minimum allocation for every container request at the RM	in MBs. Memory requests lower than this will be set to the value of this	property. Additionally, a node manager that is configured to have less memory	than this value will be shut down by the resource manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-mb</name>
	<value>1024</value>
</property>

<!-- 容器最大内存，默认8G，修改为2G -->
<property>
	<description>The maximum allocation for every container request at the RM	in MBs. Memory requests higher than this will throw an	InvalidResourceRequestException.
	</description>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>2048</value>
</property>

<!-- 容器最小CPU核数，默认1个 -->
<property>
	<description>The minimum allocation for every container request at the RM	in terms of virtual CPU cores. Requests lower than this will be set to the	value of this property. Additionally, a node manager that is configured to	have fewer virtual cores than this value will be shut down by the resource	manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-vcores</name>
	<value>1</value>
</property>

<!-- 容器最大CPU核数，默认4个，修改为2个 -->
<property>
	<description>The maximum allocation for every container request at the RM	in terms of virtual CPU cores. Requests higher than this will throw an
	InvalidResourceRequestException.</description>
	<name>yarn.scheduler.maximum-allocation-vcores</name>
	<value>2</value>
</property>

<!-- 虚拟内存检查(限制)，默认打开，修改为关闭 -->
<property>
	<description>Whether virtual memory limits will be enforced for
	containers.</description>
	<name>yarn.nodemanager.vmem-check-enabled</name>
	<value>false</value>
</property>

<!-- 虚拟内存和物理内存设置比例,默认2.1 -->
<property>
	<description>Ratio between virtual memory to physical memory when	setting memory limits for containers. Container allocations are	expressed in terms of physical memory, and virtual memory usage	is allowed to exceed this allocation by this ratio.
	</description>
	<name>yarn.nodemanager.vmem-pmem-ratio</name>
	<value>2.1</value>
</property>
```



##### 7.4.2 多队列提交案例配置

> 1. 说明
>
> ![image-20220630191953851](http://ybll.vip/md-imgs/202206301919966.png)
>
> 2. 需求 ：default队列占总内存40%，最大资源容量为60%；hive队列占总内存的60%,最大资源容量为80%
>
> + `capacity-scheduler.xml` 中修改配置
>
> ```xml
> <!-- 指定多队列，增加hive队列 -->
> <property>
>     <name>yarn.scheduler.capacity.root.queues</name>
>     <value>default,hive</value>
>     <description>
>       The queues at the this level (root is the root queue).
>     </description>
> </property>
> 
> <!-- 降低default队列资源额定容量为40%，默认100% -->
> <property>
>     <name>yarn.scheduler.capacity.root.default.capacity</name>
>     <value>40</value>
> </property>
> 
> <!-- 降低default队列资源最大容量为60%，默认100% -->
> <property>
>     <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
>     <value>60</value>
> </property>
> 
> <!-- ---------------------------------------------------------------------------- -->
> <!-- ---------------------------------------------------------------------------- -->
> <!-- ---------------------------------------------------------------------------- -->
> 
> <!-- 添加如下配置，使得多出一条hive队列 -->
> <!-- 指定hive队列的资源额定容量 -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.capacity</name>
>     <value>60</value>
> </property>
> 
> <!-- 用户最多可以使用队列多少资源，1表示所有 -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
>     <value>1</value>
> </property>
> 
> <!-- 指定hive队列的资源最大容量 -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
>     <value>80</value>
> </property>
> 
> <!-- 启动hive队列 -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.state</name>
>     <value>RUNNING</value>
> </property>
> 
> <!-- 哪些用户有权向队列提交作业 -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
>     <value>*</value>
> </property>
> 
> <!-- 哪些用户有权操作队列，管理员权限（查看/杀死） -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
>     <value>*</value>
> </property>
> 
> <!-- 哪些用户有权配置提交任务优先级 -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
>     <value>*</value>
> </property>
> 
> <!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout
> 参考资料：https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ -->
> 
> <!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 
> -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
>     <value>-1</value>
> </property>
> 
> <!-- 如果application没指定超时时间，则用default-application-lifetime作为默认值 -->
> <property>
>     <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
>     <value>-1</value>
> </property>
> ```
>
> 3. 分发配置文件，并刷新队列
>
> ```bash
> xsyc capacity-scheduler.xml
> yarn rmadmin -refreshQueues #刷新队列
> #或者可以重启yarn
> stop-yarn.sh
> start-yarn.sh
> ```

+++

##### 7.4.3 案例实操

> 采用默认调度器：容量调度器，每个队列是FIFO

1. hive队列单独提交一个job

```bash
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output1
```

![image-20220630193908476](http://ybll.vip/md-imgs/202206301939562.png)

![image-20220630194034249](http://ybll.vip/md-imgs/202206301940519.png)

2. default队列和hive队列同时提交job

```bash
# hadoop102: 
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output21

# hadoop103:
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=default /input /output22
```

![image-20220630194717818](http://ybll.vip/md-imgs/202206301947909.png)

+++

![image-20220630194822073](http://ybll.vip/md-imgs/202206301948229.png)

+ 两个任务同时运行，因为不同队列的任务可以并发运行

+++

3. hive队列向default队列借用资源

   > + 更改`capacity-scheduler.xml`配置（注意，设置成10%后，default必须设置成90%-否则刷新队列时报错）
   >
   > ```xml
   > <!-- 指定hive队列的资源额定容量 -->
   > <property>
   >     <name>yarn.scheduler.capacity.root.hive.capacity</name>
   >     <value>10</value>
   > </property>
   > <property>
   >     <name>yarn.scheduler.capacity.root.default.capacity</name>
   >     <value>90</value>
   >     <description>Default queue target capacity.</description>
   > </property>
   > ```
   >
   > + 分发配置，并刷新队列
   > + hadoop指令
   >
   > ```bash
   > # hadoop102: 
   > hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output31
   > ```
   >
   > ![image-20220630200123416](http://ybll.vip/md-imgs/202206302001500.png)
   >
   > + 结果
   >
   > ![image-20220630200308824](http://ybll.vip/md-imgs/202206302003939.png)

4. default队列提交两个Job是否能同时执行？

   > + 配置，分发，刷新队列
   >
   > ```xml
   > <property>
   >     <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
   >     <value>0.9</value>
   >     <description>
   >       Maximum percent of resources in the cluster which can be used to run 
   >       application masters i.e. controls number of concurrent running
   >       applications.
   >     </description>
   > </property>
   > <property>
   >     <name>yarn.scheduler.capacity.root.hive.capacity</name>
   >     <value>80</value>
   > </property>
   > <property>
   >     <name>yarn.scheduler.capacity.root.default.capacity</name>
   >     <value>20</value>
   >     <description>Default queue target capacity.</description>
   > </property>
   > ```
   >
   > + hadoop指令
   >
   > ```bash
   > # hadoop102:
   > hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output41
   > 
   > # hadoop103:
   > hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output42
   > ```
   >
   > ![image-20220630201721922](http://ybll.vip/md-imgs/202206302017012.png)
   >
   > +++
   >
   > + 结果：同一队列也可以并发执行（资源足够分配时)
   >
   > ![image-20220630201513942](http://ybll.vip/md-imgs/202206302015050.png)
   >
   > +++
   >
   > +++
   >
   > ![image-20220630201559945](http://ybll.vip/md-imgs/202206302016074.png)

## 8.优化

#### 8.1 HDFS 优化

##### 安全模式

> 文件只接受读数据请求，而不接受删除、修改等变更请求
>
> + NameNode加载`镜像文件`和`编辑日志`期间处于安全模式
>
> + NameNode接收DataNode注册时，处于安全模式
>
> + 刚开启Hadoop集群时，等待30s后退出安全模式
>
>   +++
>
> + 退出安全模式条件
>
> ![image-20220701164447932](http://ybll.vip/md-imgs/202207011644033.png)
>
> +++
>
> + 关于安全模式的命令操作：
>
> ```shell
> hdfs dfsadmin -safemode get    #查看安全模式状态
> hdfs dfsadmin -safemode enter  #进入安全模式
> hdfs dfsadmin -safemode leave  #退出安全模式
> 
> #进入等待，直到安全模式退出(已退出的无效果)
> hffs dfsadmin -safemode wait
> ```
>
> +++
>
> + 模拟等待安全模式
>
> ```bash
> hdfs dfsadmin -safemode get
> #进入安全模式
> hdfs dfsadmin -safemode enter
> #创建并执行脚本 safemode.sh
> #!/bin/bash
> hdfs dfsadmin -safemode wait
> hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /
> 
> #赋予执行权限,并执行
> chmod 777 safemode.sh
> ./safemode.sh
> #另启一个窗口，执行
> hdfs dfsadmin -safemode leave
> #观察原窗口
> Safe mode is OFF
> #且集群中上传README.txt成功
> ```
>
> +++

##### HDFS故障处理

> 注意：只是让HDFS恢复正常运行，不是找回丢失数据
>
> + NameNode元数据丢失：hdfs dfsadmin -safemode forceExit
>
> ```bash
> #故障模拟
> kill -9 NameNode的进程号
> #删除NameNode所有元数据信息
> rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*
> 
> #拷贝2NN的元数据信息置于NN(包括镜像文件和编辑日志，用于重启NN时，加载至内存)
> scp -r atguigu@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./name/
> 
> #重新启动NameNode
> hdfs --daemon start namenode
> ```
>
> + NameNode数据块丢失：删除对应元数据
>
> ```bash
> #1. 故障模拟：统一删除集群中某两个块信息（所有副本都要删除，否则会自动恢复）
> #2. 重启集群
> #3. 安全模式一直处于开启状态，因为块的数量没有达到要求
> #4. 离开安全模式
> hdfs dfsadmin -safemode get
> hdfs dfsadmin -safemode leave
> #5. 将元数据删除（缺少的Block块，在Web端访问该文件时会报错）
> #6. 集群恢复正常
> ```
>
> +++
>

##### DataNode多目录

> + 配置(配置多个目录，每个目录存储的数据不一样-是数据，不是副本)
> + 最好配置成：不同目录，在不同磁盘
>
> ```xml
> <!-- hdfs-site.xml 中添加如下内容 -->
> <!-- ${hadoop.tmp.dir}:HDFS数据在本地存储的路径，（core-site.xml中配置） -->
> <property>
>      <name>dfs.datanode.data.dir</name>
>      <value>file://${hadoop.tmp.dir}/dfs/data,file://${hadoop.tmp.dir}/dfs/data2</value>
> </property>
> ```
>
> ```bash
> 此时集群中有两个目录，都用以存储数据，且内容不一致
> data/
> data2/
> ```
>
> + 磁盘间数据均衡
>
> ```
> 生产环境，由于硬盘空间不足，往往需要增加硬盘，刚加载的硬盘没有数据，可以执行磁盘数据均衡命令。(Hadoop3.x新特性)
> ```
>
> ```bash
> lsblk #查看磁盘情况
> #生成均衡计划
> hdfs diskbalancer -plan hadoop102
> #执行均衡计划
> hdfs diskbalancer -execute hadoop102.plan.json
> #查看当前均衡任务执行情况
> hdfs diskbalancer -query hadoop102
> #取消均衡任务
> hdfs diskbalancer -cancel hadoop102.plan.json
> ```
>
> +++
>

##### HDFS扩容及缩容

> ```bash
> 1. 克隆一台 hadoop105主机
> 2. 修改IP地址、主机名称
> 3. 删除hadoop105上 /opt/module/hadoop/data 以及 /logs目录（历史数据）
> 4. 配置ssh无密登录
> ssh-copy-id hadoop105
> ```
>
> + 新增节点
>
> > 1. 直接启动，即可关联到集群（拷贝过来的etc/下的配置文件一致）
> >
> > ```bash
> > #在hadoop105上执行，启动datanode,nodemanager
> > hdfs --daemon start datanode
> > yarn --daemon start nodemanager
> > #观察web页面，105已经加入集群了
> > ```
> >
> > 2. 添加白名单
> >
> > ```bash
> > cd /${HADOOP_HOME}/etc/hadoop
> > vim whitelist #创建白名单文件，添加如下内容
> > 	hadoop102
> > 	hadoop103
> > 	hadoop104
> > 	hadoop105
> > xsync whitelist #分发白名单
> > 
> > #不需要重启集群，刷新NameNode即可
> > hdfs dfsadmin -refreshNodes
> > 
> > #白名单的方式，使用start-dfs.sh可以集体启动
> > start-dfs.sh  #连同NameNode一起启动
> > #未添加白名单，需要单独使用命令启动
> > hdfs --daemon start datanode
> > ```
>
> + 节点间的数据均衡
>
> ```bash
> #开启数据均衡命令
> start-balancer.sh -threshold 10 #利用率不超过10%
> #停止数据均衡命令
> stop-balancer.sh
> ```
>
> + 添加白名单和黑名单退役服务器
>
> > 白名单：白名单的主机IP地址可以用来存储数据；配置白名单，可以尽量防止黑客恶意访问攻击
> >
> > 配置步骤如下：
> >
> > ```bash
> > #1. etc/hadoop 目录下创建whitelist和blacklist文件
> > vim whitelist  #添加下列内容
> > 	hadoop102
> > 	hadoop103
> > vim touch blacklist #不添加数据，黑名单为空
> > ```
> >
> > ```xml
> > <!-- 2. 在hdfs-site.xml文件中添加dfs.hosts配置参数-->
> > <!-- 白名单 -->
> > <property>
> >      <name>dfs.hosts</name>
> >      <value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value>
> > </property>
> > 
> > <!-- 黑名单 -->
> > <property>
> >      <name>dfs.hosts.exclude</name>
> >      <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value>
> > </property>
> > ```
> >
> > ```bash
> > #3. 分发配置文件whitelist,hdfs-site.xml
> > xsync hdfs-site.xml whitelist
> > #4. 刷新NameNode节点即可(第一次添加白名单时，必须重启集群)
> > # web页面查看 http://hadoop102:9870
> > #5. 修改白名单
> > vim whitelist
> > 	hadoop102
> > 	hadoop103
> > 	hadoop104
> > 	hadoop105
> > #6. 刷新NameNode
> > hdfs dfsadmin -refreshNodes
> > #查看web页面变化
> > ```
> >
> > +++
>
> > 黑名单退役服务器 : 黑名单的主机IP地址，不可以用来存储数据，配置黑名单，用来退役服务器
> >
> > 配置步骤如下：(跟白名单类似)
> >
> > ```bash
> > # 1.创建blacklist文件，存入黑名单地址
> > # 2.在hdfs-site.xml中添加dfs.hosts.exclude配置项
> > # 3.第一次添加必须重启集群，否则，只需要刷新NameNode节点
> > myhadoop.sh stop
> > myhadoop.sh start
> > hdfs dfsadmin -refreshNodes
> > # 4.检测Web页面，观察节点状态
> > #退役时，等待所有块复制完成，状态由 Decommissioning -> Decommissioned
> > #若副本数是3，服役的节点小于3，则无法退役成功，需要先修改副本数才能退役
> > # 5.退役后，可以关闭datanode/nodemanager进程
> > hdfs --daemon stop datanode
> > yarn --daemon stop nodemanager
> > ```
> >
> > + 节点状态
> >
> >   ![image-20220705203027866](http://ybll.vip/md-imgs/202207052030989.png)

+++

##### 小文件处理

> + 小文件弊端
>
>   > + 每个文件，都会对应一个Block块，都会在NameNode元数据中占用存储空间(150B)左右
>   > + 小文件过多，会大量占用NameNode的内存空间
>   > + 元数据过多，寻址索引的速度变慢
>
> + 解决方案
>
>   > + har归档（本质上减少 NN 的内存压力）
>   >
>   >   > HDFS归档文件,对内还是一个个独立文件
>   >   >
>   >   > 但对NameNode而言却是一个整体，减少NameNode的元数据存储内存
>   >   >
>   >   > +++
>   >   >
>   >   > ```bash
>   >   > #归档文件
>   >   > hadoop archive -archiveName input.har -p /input /output
>   >   > #将 input/目录下的文件归档为 input.har,并存放在 output/ 目录下
>   >   > 
>   >   > #查看归档
>   >   > hadoop fs -ls har://output/input.har
>   >   > 
>   >   > #解归档文件
>   >   > hadoop fs -cp har://output/input.har/* /input2
>   >   > ```
>   >
>   > + CombineTextInputFormat（计算方向，减少MapTask的数量）
>   >
>   >   > 将多个小文件，在切片过程中，生成一个单独的切片
>   >
>   > + 实现JVM重用 （开启 uber 模式）-计算方向
>   >
>   >   > 默认每个Task任务，都会启动一个JVM来运行，如果Task任务计算的数据量很小，这可设置同一个Job的多个Task运行在一个JVM中
>   >   >
>   >   > +++
>   >   >
>   >   > ```xml
>   >   > <!-- mapred-site.xml -->
>   >   > <!--  开启uber模式，默认关闭 -->
>   >   > <property>
>   >   >   	<name>mapreduce.job.ubertask.enable</name>
>   >   >   	<value>true</value>
>   >   > </property>
>   >   > 
>   >   > <!-- uber模式中最大的mapTask数量，可向下修改  --> 
>   >   > <property>
>   >   >   	<name>mapreduce.job.ubertask.maxmaps</name>
>   >   >   	<value>9</value>
>   >   > </property>
>   >   > <!-- uber模式中最大的reduce数量，可向下修改 -->
>   >   > <property>
>   >   >   	<name>mapreduce.job.ubertask.maxreduces</name>
>   >   >   	<value>1</value>
>   >   > </property>
>   >   > <!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 -->
>   >   > <property>
>   >   >   	<name>mapreduce.job.ubertask.maxbytes</name>
>   >   >   	<value></value>
>   >   > </property>
>   >   > ```
>   >   >
>   >   > + 分发配置,再执行任务时，同一个Application_Job 的所分配的容器数量会减少
>   >
>   > +++
>
> +++

##### 回收站

> 开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。
>
> ***1) 回收站参数设置及工作机制***
>
> ![img](http://ybll.vip/md-imgs/202207052054318.png) 
>
>
> ***2) 启用回收站***
>
> 修改core-site.xml，配置垃圾回收时间为1分钟。
>
> ```xml
> <property>
>  <name>fs.trash.interval</name>
>  <value>1</value>
> </property>
> ```
>
> ***3) 查看回收站***
>
> 回收站目录在hdfs集群中的路径：`/user/atguigu/.Trash/….`
>
> ***4) 通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站***
>
> ```java
> Configuration conf = new Configuration();
> 
> //设置HDFS的地址
> conf.set("fs.defaultFS","hdfs://hadoop102:8020");
> //因为本地的客户端拿不到集群的配置信息 所以需要自己手动设置一下回收站
> conf.set("fs.trash.interval","1");
> conf.set("fs.trash.checkpoint.interval","1");
> //创建一个回收站对象
> Trash trash = new Trash(conf);
> 
> //将HDFS上的/input/wc.txt移动到回收站
> trash.moveToTrash(new Path("/input/wc.txt"));
> ```
>
> ***5) 通过网页上直接删除的文件也不会走回收站。***
>
> ***6) 只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站。***
>
> ```bash
> hadoop fs -rm -r /user/atguigu/input
> ```
>
> ***7) 恢复回收站数据***
>
> ```bash
> hadoop fs -mv
> /user/atguigu/.Trash/Current/user/atguigu/input   /user/atguigu/input
> ```
>
> +++

##### 调优参数

> + hadoop-env.sh【NameNode内存，DataNode内存】
>
>   ```shell
>   export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"
>   
>   export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
>   ```
>
> + hdfs-site.xml
>
>   ```xml
>   <!-- NameNode有处理DataNode心跳的工作线程池，默认值是10 -->
>   <property>
>     <name>dfs.namenode.handler.count</name>
>     <value>21</value>
>   </property>
>   ```
>
> + core-site.xml
>
>   ```xml
>   <!-- 配置垃圾回收时间为60分钟 -->
>   <property>
>     <name>fs.trash.interval</name>
>     <value>60</value>
>   </property>
>   ```
>
> +++



#### 8.2 MR优化

> MapReduce慢的原因：
>
> + 计算机性能 - CPU，内存，磁盘，网络
> + I/O操作优化 - 数据倾斜，Map运行时间太长使得Reducer等待，小文件过多
>
> ```txt
> MapReducer优化，从下面五个方面考虑，另外还有常用的调优参数
> ```

##### 1-数据输入(小文件处理)

> + 合并小文件（har）
> + 采用CombineTextInputFormat作为输入
> + jVM重用

##### 2-Map阶段

>  Shuffle前半段：map()方法执行后
>
>  ![image-20220918015828356](http://ybll.vip/md-imgs/202209180158592.png)
>
>  +++
>
>  

##### 3-Reduce阶段

> Shuffle后半段 : reduce()方法执行前
>
> ![image-20220918020115458](http://ybll.vip/md-imgs/202209180201660.png)
>
> +++
>
> 

##### 4-I/O传输

> + 采用数据压缩的方式，snappy,LZO...
> + 使用SequenceFile二进制文件

##### 5-数据倾斜问题

> 减少数据倾斜的方法:
>
> ![image-20220918020647401](http://ybll.vip/md-imgs/202209180206542.png)
>
> 

+++

##### MR参数调优

> + mapred-site.xml
>
> ```xml
> <!-- 环形缓冲区大小，默认100m -->
> <property>
>   <name>mapreduce.task.io.sort.mb</name>
>   <value>100</value>
> </property>
> 
> <!-- 环形缓冲区溢写阈值，默认0.8 -->
> <property>
>   <name>mapreduce.map.sort.spill.percent</name>
>   <value>0.80</value>
> </property>
> 
> <!-- merge合并次数，默认10个 -->
> <property>
>   <name>mapreduce.task.io.sort.factor</name>
>   <value>10</value>
> </property>
> 
> <!-- maptask内存，默认1g； maptask堆内存大小默认和该值大小一致mapreduce.map.java.opts -->
> <property>
>   <name>mapreduce.map.memory.mb</name>
>   <value>-1</value>
>   <description>The amount of memory to request from the scheduler for each    map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.
>   </description>
> </property>
> 
> <!-- matask的CPU核数，默认1个 -->
> <property>
>   <name>mapreduce.map.cpu.vcores</name>
>   <value>1</value>
> </property>
> 
> <!-- matask异常重试次数，默认4次 -->
> <property>
>   <name>mapreduce.map.maxattempts</name>
>   <value>4</value>
> </property>
> 
> <!-- 每个Reduce去Map中拉取数据的并行数。默认值是5 -->
> <property>
>   <name>mapreduce.reduce.shuffle.parallelcopies</name>
>   <value>5</value>
> </property>
> 
> <!-- Buffer大小占Reduce可用内存的比例，默认值0.7 -->
> <property>
>   <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
>   <value>0.70</value>
> </property>
> 
> <!-- Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。 -->
> <property>
>   <name>mapreduce.reduce.shuffle.merge.percent</name>
>   <value>0.66</value>
> </property>
> 
> <!-- reducetask内存，默认1g；reducetask堆内存大小默认和该值大小一致mapreduce.reduce.java.opts -->
> <property>
>   <name>mapreduce.reduce.memory.mb</name>
>   <value>-1</value>
>   <description>The amount of memory to request from the scheduler for each    reduce task. If this is not specified or is non-positive, it is inferred
>     from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.
>     If java-opts are also not specified, we set it to 1024.
>   </description>
> </property>
> 
> <!-- reducetask的CPU核数，默认1个 -->
> <property>
>   <name>mapreduce.reduce.cpu.vcores</name>
>   <value>2</value>
> </property>
> 
> <!-- reducetask失败重试次数，默认4次 -->
> <property>
>   <name>mapreduce.reduce.maxattempts</name>
>   <value>4</value>
> </property>
> 
> <!-- 当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05 -->
> <property>
>   <name>mapreduce.job.reduce.slowstart.completedmaps</name>
>   <value>0.05</value>
> </property>
> 
> <!-- 如果程序在规定的默认10分钟内没有读到数据，将强制超时退出 -->
> <property>
>   <name>mapreduce.task.timeout</name>
>   <value>600000</value>
> </property>
> ```

+++



#### 8.3 YARN调优参数(重要)

> + Resourcemanager相关【7.4节】
>
>   ```bash
>   yarn.resourcemanager.scheduler.client.thread-count #ResourceManager处理调度器请求的线程数量
>   
>   yarn.resourcemanager.scheduler.class #配置调度器
>   ```
>
> + Nodemanager相关
>
>   ```bash
>   yarn.nodemanager.resource.memory-mb #NodeManager使用内存数--重要
>   yarn.nodemanager.resource.system-reserved-memory-mb #NodeManager为系统保留多少内存，和上一个参数二者取一即可
>   
>   yarn.nodemanager.resource.cpu-vcores #NodeManager使用CPU核数
>   yarn.nodemanager.resource.count-logical-processors-as-cores #是否将虚拟核数当作CPU核数
>   yarn.nodemanager.resource.pcores-vcores-multiplier 		#虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2
>   yarn.nodemanager.resource.detect-hardware-capabilities 	#是否让yarn自己检测硬件进行配置
>   
>   yarn.nodemanager.pmem-check-enabled #是否开启物理内存检查限制container
>   yarn.nodemanager.vmem-check-enabled #是否开启虚拟内存检查限制container
>   yarn.nodemanager.vmem-pmem-ratio 	#虚拟内存物理内存比例
>   ```
>
> + Container容器相关
>
>   ```bash
>   yarn.scheduler.minimum-allocation-mb	#容器最小内存
>   yarn.scheduler.maximum-allocation-mb	#容器最大内存 --重要
>   yarn.scheduler.minimum-allocation-vcores	#容器最小核数
>   yarn.scheduler.maximum-allocation-vcores	#容器最大核数
>   ```

+++





# 二、 Zookeeper

## 1. Zookeeper入门

#### 1.1 概述

Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。基于观察者模式，接受观察者的注册，一旦数据发生变化，Zookeeper就会通知已经注册的观察者。

Zookeeper = 文件系统 + 通知机制

#### 1.2 特点

> 1. Zookeeper:一个Leader(领导者)，多个Follower(跟随者)组成的集群
>
> 2. 集群存活一半以上的节点，Zookeeper就能运行
>
> 3. 全局数据一致，每个Server保存一份相同的数据副本
>
> 4. 更新请求顺序进行，来自同一个Client的更新请求，会该Client的按照发送顺序，依次执行
>
> 5. 数据更新原子性，一次数据更新，要么成功，要么失败
>
> 6. 实时性，在一定时间范围内，Client能读到最新数据

#### 1.3 数据结构

Zookeeper的数据模型的结构是树形结构，每一个节点称做一个ZNode,每个ZNode默认存储1MB的数据，每个ZNode都可以通过其路径唯一表示

![image-20220319151654748](http://ybll.vip/md-imgs/202203291224821.png)

#### 1.4 应用场景

统一命名服务、统一配置管理、统一集群管理、服务器动态上下线、软负载均衡等

图示：

![image-20220319152424419](http://ybll.vip/md-imgs/202203291224760.png)

![image-20220319152137837](http://ybll.vip/md-imgs/202203291224082.png)

![image-20220319152320366](http://ybll.vip/md-imgs/202203291224013.png)

#### 1.5 Zookeeper本地安装

0. 集群中的每一台服务器都需要安装 Zookeeper

1. 需要安装JDK

2. 正式安装

   ```bash
   #解压到指定目录
   tar -zxvf zookeeper-3.5.7.tar.gz -C /opt/module/
   #修改配置
   mv zoo_sample.cfg zoo.cfg
   mkdir zkData
   vim zoo.cfg
   	#在zoo.cfg中修改该行
   	dataDir=/opt/module/zookeeper-3.7.5/zkData
   #操作Zookeeper
   bin/zkServer.sh start #启动Zookeeper
   jps #查看进程状态
   	#zkServer.sh服务器启动后的进程名： QuorumPeerMain
   	#zkCli.sh客户端启动后的进程名： ZooKeeperMain
   bin/zkServer.sh status #查看状态
   bin/zkCli.sh #启动客户端
   quit #退出客户端
   bin/zkServer.sh stop #停止Zookeeper
   ```

3. 配置参数解读

   ```bash
   #Zookeeper中配置文件zoo.cfg 中参数含义
   tickTime=2000 #单位ms,通信心跳数，Zookeeper服务器与客户端心跳时间
   initLimit=10 #LF初始通信时限，Follower与Leader初始连接时，允许的最多心跳数
   syncLimit=5 #Leader与Follower之间最大响应时间单位
   dataDir=/dir #数据文件目录 + 数据持久化路径 ；用于保存Zookeeper中的数据
   clientPort=2181 #客户端连接端口，一般不修改
   ```

   

## 2. Zookeeper实战

#### 2.1 分布式安装部署

##### 	2.1.1 安装与启动

```bash
#解压zookeeper安装包
tar -zxvf zookeeper-3.5.7.tar.gz -C /opt/module
xsync zookeeper-3.5.7.tar.gz/ #同步到集群的其他服务器
#配置服务器编号
mkdir -p zkData
touch zkData/myid   #文件名必须是myid，源码中可见
vim myid#hadoop102,103,104 的myid文件填写不同myid(如：2,3,4)
#修改zoo.cfg
mv zoo_sample.cfg zoo.cfg
vim zoo.cfg
	dataDir=/opt/module/zookeeper-3.5.7/zkData #修改该行
	server.2=hadoop102:2888:3888
	server.A=B:C:D #增加该行，集群多少台机器，就添加多少行
	A:就是myid的值,一个数字
    B:服务器的地址，host或者ip
	C:Follower与Leader交换信息的端口；2888
	D:执行选举新Leader时服务器互相通信的端口；3888
#分发配置到集群的其他机器 xsync

bin/zkServer.sh start #启动集群；每台服务器均需输入该命令启动，可写脚本
```

##### ***2.1.2 客户端命令行操作***

```shell
节点类型
持久(客户端与服务器断开连接后，节点不删除)
短暂(断开连接后，节点自己删除)  `-e`

带序号 `-s` | 不带序号  #不同节点下的序号不统一，都是从0开始，创建一个就加1

监听 `-w` get:监听节点的内容 ls:监听当前节点的子节点是否变化
```

```bash
bin/zkCli.sh #启动客户端
bin/zkCli.sh -server hadoop102:2181 #指定启动哪台服务器的zk客户端

#以下是启动客户端后，操作zookeeper的命令列表
help #显示所有操作命令
ls /path #查看当前路径节点的子节点，-w:监听子节点变化，-s:附加次级信息(详细信息)
create /path [xx] #普通创建 -s:含有序列 -e:临时节点(重启或者超时就消失)
get /path #获取节点的值 -w:监听节点的变化 -s:附加次级信息
set xx #设置节点的具体值
stat #查看节点的状态
delete #删除节点,节点有值则删除不了
deleteall #递归删除节点，节点有值也能删

#具体操作
ls /  #[zookeeper]
ls -s / #ls + stat

#创建节点时，需要赋值,不赋值则为null (永久节点，不带序号)
create /sanguo "diaochan"
create /sanguo/shuguo "loubei"

get /sanguo # diaochan
get -s /sanguo/shuguo # get + stat

create -e /sanguo/wuguo "zhouyu"
ls /sanguo # [wuguo,shuguo]  (重启后，wuguo会被删除)

##创建带序号的节点,序号从0开始递增(-s带序号，可重复创建，不带序号不能重复创建)
create /sanguo/w "c" # 创建成功
create /sanguo/w "c" # 创建失败 Node already exists:/sanguo/w
create -s /sanguo/w "c" # 创建成功 /sanguo/0000000000
create -s /sanguo/w "c" # 创建成功 /sanguo/0000000001
create -s /sanguo/w "c" # 创建成功 /sanguo/0000000002
##修改节点数据值
set /sanguo/w "newc"


##节点的内容变化监听(调用一次，就只能监听一次；想要再次监听值的变化，需要再次调用)
get -w /sanguo  #hadoop104键入该命令，监听/sanguo
set /sanguo "xishi" #hadoop103上修改/sanguo的值
#hadoop104主机会收到数据变化的监听

##节点的子节点变化监听（路径变化；调用一次，监听一次，同上）
ls -w /sanguo #hadoop104主机注册监听/sanguo
create /sanguo/jin "simayi" #hadoop103上创建子节点
#hadoop104主机会收到子节点变化的监听

delete /sanguo/jin #删除节点
deleteall /sanguo #递归删除节点
stat /sanguo #查看节点状态
```

> + ![image-20220702092611371](http://ybll.vip/md-imgs/202207020928825.png)
> + czxid ：值越大，说明数据越完整
> + ![image-20220702092911793](http://ybll.vip/md-imgs/202207020929877.png)

+++

##### 2.1.3 API应用（IDEA环境）

> ```xml
> <!-- pom.xml-->
> <dependencies>
> 	<dependency>
> 		<groupId>junit</groupId>
> 		<artifactId>junit</artifactId>
> 		<version>RELEASE</version>
> 	</dependency>
> 
> 	<dependency>
> 		<groupId>org.apache.logging.log4j</groupId>
> 		<artifactId>log4j-core</artifactId>
> 		<version>2.8.2</version>
> 	</dependency>
> 
> 	<dependency>
> 		<groupId>org.apache.zookeeper</groupId>
> 		<artifactId>zookeeper</artifactId>
> 		<version>3.5.7</version>
> 	</dependency>
> </dependencies>
> 
> <!--log4j.properties-->
> log4j.rootLogger=INFO, stdout  
> log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
> log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
> log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
> log4j.appender.logfile=org.apache.log4j.FileAppender  
> log4j.appender.logfile.File=target/spring.log  
> log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
> log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 
> ```
>
> 代码示例
>
> ```java
> public class zkClient {
> 
>     private String connectString="hadoop102:2181,hadoop103:2181,hadoop104:2181";
>     private int sessionTimeout = 2000;
>     private ZooKeeper zkCli;
> 
>     @Before
>     public void init() throws IOException {
>          zkCli = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
>             @Override
>             public void process(WatchedEvent watchedEvent) {
>                 //写在Watcher中，每次变化后都会执行该方法
>                 List<String> children = null;
>                 System.out.println("===============");
>                 try {
>                     children = zkCli.getChildren("/sanguo", true);
>                 } catch (KeeperException e) {
>                     e.printStackTrace();
>                 } catch (InterruptedException e) {
>                     e.printStackTrace();
>                 }
>                 for (String child : children) {
>                     System.out.println(child);
>                 }
>                 System.out.println("==============");
>             }
>         });
>     }
>     @Test
>     public void create() throws InterruptedException, KeeperException {
>         String nodeCreated = zkCli.create("/sanguo/sg", "sg123".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
>     }
>     @Test
>     public void getChildren() throws InterruptedException, KeeperException {
> //        List<String> children = zkCli.getChildren("/sanguo", true);
> //        for (String child : children) {
> //            System.out.println(child);
> //        }
>         //上述代码全部写在匿名Watcher的process()方法中
>         //延时操作，进行监听
>         Thread.sleep(Long.MAX_VALUE);
>     }
>     @Test
>     public void isExists() throws InterruptedException, KeeperException {
>         Stat exists = zkCli.exists("/sanguo", false);
>         System.out.println(exists.getDataLength());
>         System.out.println(exists==null?"not exists":"exists");
>         //执行完后，仍然会执行匿名Watcher中的process()方法
>     }
> }
> ```

+++

#### 2.2 选举机制

> 第一次启动
>
> > ![image-20220501212755188](http://ybll.vip/md-imgs/202205012127373.png)
>
> 非第1次启动
>
> > ![image-20220501212601197](http://ybll.vip/md-imgs/202205012126454.png)

+++

#### 2.3 监听器原理

> 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、节点删除、子目录节点增加删除）时，ZooKeeper会通知客户端。监听机制保证ZooKeeper保存的任何的数据的任何改变都能快速的响应到监听了该节点的应用程序。
>
> ![image-20220501231825466](http://ybll.vip/md-imgs/202205012318640.png)

+++

#### 2.4 写数据流程

> ![image-20220501231221887](http://ybll.vip/md-imgs/202205012312014.png)
>
> ![img](http://ybll.vip/md-imgs/202207021119933.png)

+++

#### 2.5 ZK案例

##### 2.5.1 服务器动态上下线监听

> ![image-20220501232458687](http://ybll.vip/md-imgs/202205012324856.png)
>
> 1. Zookeeper中创建 /mytest/servers 节点
>
>    > ```shell
>    > /bin/zkCli.sh
>    > create /mytest/servers "servers"
>    > ```
>
> 2. 服务器向zookeeper注册
>
>    > ```java
>    > public class DistributeServer {
>    >     private String connectString="hadoop102:2181,hadoop103:2181,hadoop104:2181";
>    >     private int sessionTimeout=2000;
>    >     private ZooKeeper zk;
>    > 
>    >     public static void main(String[] args) throws InterruptedException, KeeperException {
>    >         DistributeServer server = new DistributeServer();
>    >         //1. 获取zk连接
>    >         server.getConnection();
>    >         //2. 注册服务器到zk服务器
>    >         server.regist(args[0]);
>    >         //3. 启动业务逻辑(sleep)
>    >         server.business();
>    >     }
>    > 
>    >     private void business() {
>    >         try {
>    >             Thread.sleep(Long.MAX_VALUE);
>    >         } catch (InterruptedException e) {
>    >             e.printStackTrace();
>    >         }
>    >     }
>    > 
>    >     private void regist(String hostname) throws InterruptedException, KeeperException {
>    >         //创建临时带序号的节点
>    >         zk.create("/mytest/servers/"+hostname,hostname.getBytes(),
>    >                 ZooDefs.Ids.OPEN_ACL_UNSAFE,
>    >                 CreateMode.EPHEMERAL_SEQUENTIAL);
>    >         System.out.println(hostname + "is online");
>    >     }
>    > 
>    >     private void getConnection() {
>    >         try {
>    >             zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
>    >                 @Override
>    >                 public void process(WatchedEvent watchedEvent) {
>    > 
>    >                 }
>    >             });
>    >         } catch (IOException e) {
>    >             e.printStackTrace();
>    >         }
>    >     }
>    > }
>    > ```
>
> 3. 客户端向zookeeper注册
>
>    > ```java
>    > public class DistributeClient {
>    >     private String connectString="hadoop102:2181,hadoop103:2181,hadoop104:2181";
>    >     private int sessionTimeout=2000;
>    >     private ZooKeeper zk;
>    > 
>    >     public static void main(String[] args) throws IOException, InterruptedException, KeeperException {
>    >         DistributeClient zkCli = new DistributeClient();
>    >         //1. 获取zk连接
>    >         zkCli.getConnection();
>    >         //2. 监听 /mytest/servers下的子节点的增加和删除
>    >         zkCli.getServerList();
>    >         //3. 业务逻辑(sleep)
>    >         zkCli.business();
>    >     }
>    > 
>    >     private void business() throws InterruptedException {
>    >         Thread.sleep(Long.MAX_VALUE);
>    >     }
>    > 
>    >     private void getServerList() throws InterruptedException, KeeperException {
>    >         System.out.println("=========================");
>    >         List<String> children = zk.getChildren("/mytest/servers", true);
>    > 
>    >         ArrayList<String> servers = new ArrayList<>();
>    >         for (String child : children) {
>    >             byte[] data = zk.getData("/mytest/servers/" + child, false, null);
>    >             servers.add(new String(data));
>    >         }
>    >         System.out.println(servers);
>    >         System.out.println("=========================");
>    >     }
>    > 
>    >     private void getConnection() throws IOException {
>    >         zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
>    >             @Override
>    >             public void process(WatchedEvent watchedEvent) {
>    >                 try {
>    >                     getServerList();
>    >                 } catch (InterruptedException e) {
>    >                     e.printStackTrace();
>    >                 } catch (KeeperException e) {
>    >                     e.printStackTrace();
>    >                 }
>    >             }
>    >         });
>    >     }
>    > }
>    > ```
>

+++

>  注：下列案例中的pom.xml引入的maven依赖在 `API应用(IDEA环境)` 中

+++

##### 2.5.2 ZooKeeper分布式锁

> ![image-20220502122726513](http://ybll.vip/md-imgs/202205021227812.png)
>
> 1. `DistributeLock.java`
>
>    > ![carbon](http://ybll.vip/md-imgs/202205031720957.png)
>
> 2. `DistributedLockTest.java`
>
>    > ![carbon (1)](http://ybll.vip/md-imgs/202205031722365.png)

+++

##### 2.5.3 Curator框架实现分布式锁

> 官网：https://curator.apache.org/index.html
>
> 1. pom.xml 中引入依赖
>
>    ```xml
>    <dependency>
>        <groupId>org.apache.curator</groupId>
>        <artifactId>curator-framework</artifactId>
>        <version>4.3.0</version>
>    </dependency>
>    <dependency>
>        <groupId>org.apache.curator</groupId>
>        <artifactId>curator-recipes</artifactId>
>        <version>4.3.0</version>
>    </dependency>
>    <dependency>
>        <groupId>org.apache.curator</groupId>
>        <artifactId>curator-client</artifactId>
>        <version>4.3.0</version>
>    </dependency>
>    ```
>
> 2. 代码示例
>
>    ```java
>    public class CuratorLockTest {
>    
>        public static void main(String[] args) {
>            //创建分布式锁1
>            InterProcessMutex lock1 = new InterProcessMutex(getCuratorFramework(), "/mytest/locks");
>            //创建分布式锁2
>            InterProcessMutex lock2 = new InterProcessMutex(getCuratorFramework(), "/mytest/locks");
>            new Thread(new Runnable() {
>                @Override
>                public void run() {
>                    try {
>                        lock1.acquire();
>                        System.out.println("线程1 获取到锁");
>                        lock1.acquire();
>                        System.out.println("线程1 再次获取锁");
>    
>                        Thread.sleep(5000);
>    
>                        lock1.release();
>                        System.out.println("线程1 释放锁");
>                        lock1.release();
>                        System.out.println("线程1 再次释放锁");
>                    } catch (Exception e) {
>                        e.printStackTrace();
>                    }
>                }
>            }).start();
>    
>            new Thread(new Runnable() {
>                @Override
>                public void run() {
>                    try {
>                        lock2.acquire();
>                        System.out.println("线程2 获取到锁");
>                        lock2.acquire();
>                        System.out.println("线程2 再次获取锁");
>    
>                        Thread.sleep(5000);
>    
>                        lock2.release();
>                        System.out.println("线程2 释放锁");
>                        lock2.release();
>                        System.out.println("线程2 再次释放锁");
>                    } catch (Exception e) {
>                        e.printStackTrace();
>                    }
>                }
>            }).start();
>    
>        }
>    
>        private static CuratorFramework getCuratorFramework() {
>            ExponentialBackoffRetry policy = new ExponentialBackoffRetry(3000, 3);
>            CuratorFramework client = CuratorFrameworkFactory.builder().connectString("hadoop102:2181,hadoop103:2181,hadoop104:2181")
>                    .connectionTimeoutMs(2000)
>                    .retryPolicy(policy).build();
>            client.start();
>            System.out.println("zookeeper启动成功！");
>            return client;
>        }
>    }
>    ```

+++

## 3. 企业面试真题

> ##### 3.1 选举机制
>
> 半数机制，超过半数的投票通过，即通过。
>
> 1. 第一次启动选举规则：
>    
> + 投票过半数时，服务器id大的胜出
>   
> 2. 第二次启动选举规则：
>
>    ① EPOCH大的直接胜出
>
>    ② EPOCH相同，事务id大的胜出
>
>    ③ 事务id相同，服务器id大的胜出
>
> ##### 3.2 生产集群安装多少zk合适？
>
> > 安装奇数台。
> >
> > 生产经验：
> >
> > + 10台服务器：3台zk；
> > + 20台服务器：5台zk；
> > + 100台服务器：11台zk；
> > + 200台服务器：11台zk
> >
> > zk服务器台数多：好处，提高可靠性；坏处：提高通信延时
>
> ##### 3.3 常用命令
>
> > ls、get、create、delete

+++

## 4. Zookeeper 源码分析(后续学习)

> 

# 三、HA

## 1. HA高可用概述

## 2.HA高可用配置

#### 2.1 准备工作

#### 2.2 HDFS-HA 配置

> + ![image-20220702151049847](http://ybll.vip/md-imgs/202207021510955.png)
> + ![image-20220702151310345](http://ybll.vip/md-imgs/202207021513446.png)
>
> +++
>
> 手动故障转移的配置
>
> > + `core-site.xml`  `hdfs-site.xml` 中原有配置需要删除
> > + core-site.xml 添加配置：
> >
> > ```xml
> > <configuration>
> >   <!-- 把多个NameNode的地址组装成一个集群mycluster -->
> >   <property>
> >     <name>fs.defaultFS</name>
> >     <value>hdfs://mycluster</value>
> >   </property>
> > 
> >   <!-- 指定hadoop运行时产生文件的存储目录 -->
> >   <property>
> >     <name>hadoop.tmp.dir</name>
> >     <value>/opt/ha/hadoop-3.1.3/data</value>
> >   </property>
> > </configuration>
> > ```
> >
> > + hdfs-site.xml 添加配置：
> >
> > ```xml
> > <!-- NameNode数据存储目录 -->
> >   <property>
> >     <name>dfs.namenode.name.dir</name>
> >     <value>file://${hadoop.tmp.dir}/name</value>
> >   </property>
> > 
> >   <!-- DataNode数据存储目录 -->
> >   <property>
> >     <name>dfs.datanode.data.dir</name>
> >     <value>file://${hadoop.tmp.dir}/data</value>
> >   </property>
> > 
> >   <!-- JournalNode数据存储目录 -->
> >   <property>
> >     <name>dfs.journalnode.edits.dir</name>
> >     <value>${hadoop.tmp.dir}/jn</value>
> >   </property>
> > 
> >   <!-- 完全分布式集群名称 -->
> >   <property>
> >     <name>dfs.nameservices</name>
> >     <value>mycluster</value>
> >   </property>
> > 
> >   <!-- 集群中NameNode节点都有哪些 -->
> >   <property>
> >     <name>dfs.ha.namenodes.mycluster</name>
> >     <value>nn1,nn2,nn3</value>
> >   </property>
> > 
> >   <!-- NameNode的RPC通信地址 -->
> >   <property>
> >     <name>dfs.namenode.rpc-address.mycluster.nn1</name>
> >     <value>hadoop102:8020</value>
> >   </property>
> >   <property>
> >     <name>dfs.namenode.rpc-address.mycluster.nn2</name>
> >     <value>hadoop103:8020</value>
> >   </property>
> >   <property>
> >     <name>dfs.namenode.rpc-address.mycluster.nn3</name>
> >     <value>hadoop104:8020</value>
> >   </property>
> > 
> >   <!-- NameNode的http通信地址 -->
> >   <property>
> >     <name>dfs.namenode.http-address.mycluster.nn1</name>
> >     <value>hadoop102:9870</value>
> >   </property>
> >   <property>
> >     <name>dfs.namenode.http-address.mycluster.nn2</name>
> >     <value>hadoop103:9870</value>
> >   </property>
> >   <property>
> >     <name>dfs.namenode.http-address.mycluster.nn3</name>
> >     <value>hadoop104:9870</value>
> >   </property>
> > 
> >   <!-- 指定NameNode元数据在JournalNode上的存放位置 -->
> >   <property>
> >     <name>dfs.namenode.shared.edits.dir</name>
> > <value>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster</value>
> >   </property>
> > 
> >   <!-- 访问代理类：client用于确定哪个NameNode为Active -->
> >   <property>
> >     <name>dfs.client.failover.proxy.provider.mycluster</name>
> >     <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
> >   </property>
> > 
> >   <!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
> >   <property>
> >     <name>dfs.ha.fencing.methods</name>
> >     <value>sshfence</value>
> >   </property>
> > 
> >   <!-- 使用隔离机制时需要ssh秘钥登录-->
> >   <property>
> >     <name>dfs.ha.fencing.ssh.private-key-files</name>
> >     <value>/home/atguigu/.ssh/id_rsa</value>
> >   </property>
> > ```
> >
> > + 上述完成后，liunx命令执行合集如下
> >
> > ```bash
> > #分发两个配置文件至各节点
> > xsync core-site.xml hdfs-site.xml
> > 
> > #启动journalnode服务,三个节点都需要执行该命令
> > hdfs --daemon start journalnode
> > 
> > #在hadoop102上，进行格式化操作，并启动NameNode
> > hdfs namenode -format
> > hdfs --daemon start namenode
> > 
> > #同步操作，对于hadoop103,hadoop104，同步NameNode元数据信息
> > hdfs namenode -bootstrapStandby #hadoop103,hadoop104都要执行
> > 
> > #三个节点均启动namenode和datanode
> > hdfs --daemon start namenode #剩余的hadoop103,hadoop104需要执行
> > hdfs --daemon start datanode #hadoop102,hadoop103,hadoop104都需要执行
> > 
> > #查看NameNode是否为Active
> > hdfs haadmin -getServiceState nn1 #nn2,nn3 
> > #三个namenode均为standby，因为没有配置自动故障转移
> > 
> > #设置hadoop102为Active
> > hdfs haadmin -transitionToActive nn1  #nn1下线后，需要手动用该命令指定其他namenode为Active
> > ```
>
> +++
>
> 自动故障转移的配置
>
> > + 自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程，如图所示。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。
> > + core-site.xml中添加配置：
> >
> > ```xml
> > <!-- 指定zkfc要连接的zkServer地址 -->
> > <property>
> > 	<name>ha.zookeeper.quorum</name>
> > 	<value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>
> > </property>
> > ```
> >
> > + hdfs-site.xml中添加配置：
> >
> > ```xml
> > <!-- 启用nn故障自动转移 -->
> > <property>
> > 	<name>dfs.ha.automatic-failover.enabled</name>
> > 	<value>true</value>
> > </property>
> > ```
> >
> > + 上述完成后，liunx命令执行合集如下
> >
> >   > 配置上述两项后，手动故障配置的命令则会失效，无法使用
> >
> > ```bash
> > #关闭HDFS
> > stop-dfs.sh
> > #namenode,datanode,journalnode,DFSZKFailoverController服务均会被关闭
> > #journalnode:保证每个nn中的数据一致
> > #DFSZKFailoverController(ZKFC):与zookeeper连接，
> > 
> > #关闭上述服务后，再启动zookeeper
> > myzookeeper.sh start
> > 
> > #初始化HA在Zookeeper中的状态(在zk中创建节点)
> > hdfs zkfc -formatZK
> > 
> > #启动HDFS
> > start-dfs.sh
> > #可以看到，自动出现一台Active,其他则为Standby
> > ```
> >
> > + 启动后的jar进程:
> >
> > ![image-20220702143501944](http://ybll.vip/md-imgs/202207021435039.png)
> >
> > + 关闭Active状态的NameNode后，ZK中`hadoop-ha`节点变化：
> >
> > ![image-20220702153001218](http://ybll.vip/md-imgs/202207021530327.png)
>
> +++

#### 2.3 YARN-HA配置

> + YARN-HA工作机制
>
> ![image-20220702161422573](http://ybll.vip/md-imgs/202207021614674.png)
>
> + 核心问题
>
> ![image-20220702161548274](http://ybll.vip/md-imgs/202207021615373.png)
>
> + 如何配置：`yarn-site.xml` , (先删除原来的配置项)
>
> + ```bash
>   #配置前，可先关闭hdfs
>   stop-dfs.sh
>   ```
>
> ```xml
> <property>
>         <name>yarn.nodemanager.aux-services</name>
>         <value>mapreduce_shuffle</value>
>     </property>
> 
>     <!-- 启用resourcemanager ha -->
>     <property>
>         <name>yarn.resourcemanager.ha.enabled</name>
>         <value>true</value>
>     </property>
>  
>     <!-- 声明两台resourcemanager的地址 -->
>     <property>
>         <name>yarn.resourcemanager.cluster-id</name>
>         <value>cluster-yarn1</value>
>     </property>
> 
>     <!--指定resourcemanager的逻辑列表-->
>     <property>
>         <name>yarn.resourcemanager.ha.rm-ids</name>
>         <value>rm1,rm2,rm3</value>
>     </property>
> <!-- ========== rm1的配置 ========== -->
>     <!-- 指定rm1的主机名 -->
>     <property>
>         <name>yarn.resourcemanager.hostname.rm1</name>
>         <value>hadoop102</value>
>     </property>
> 
>     <!-- 指定rm1的web端地址 -->
>     <property>
>         <name>yarn.resourcemanager.webapp.address.rm1</name>
>         <value>hadoop102:8088</value>
>     </property>
> 
>     <!-- 指定rm1的内部通信地址 -->
>     <property>
>         <name>yarn.resourcemanager.address.rm1</name>
>         <value>hadoop102:8032</value>
>     </property>
> 
>     <!-- 指定AM向rm1申请资源的地址 -->
>     <property>
>         <name>yarn.resourcemanager.scheduler.address.rm1</name>  
>         <value>hadoop102:8030</value>
>     </property>
> 
>     <!-- 指定供NM连接的地址 -->  
>     <property>
>     <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
>         <value>hadoop102:8031</value>
>     </property>
> 
> <!-- ========== rm2的配置 ========== -->
>     <!-- 指定rm2的主机名 -->
>     <property>
>         <name>yarn.resourcemanager.hostname.rm2</name>
>         <value>hadoop103</value>
>     </property>
>     <property>
>         <name>yarn.resourcemanager.webapp.address.rm2</name>
>         <value>hadoop103:8088</value>
>     </property>
>     <property>
>         <name>yarn.resourcemanager.address.rm2</name>
>         <value>hadoop103:8032</value>
>     </property>
>     <property>
>         <name>yarn.resourcemanager.scheduler.address.rm2</name>
>         <value>hadoop103:8030</value>
>     </property>
> 
>     <property>
> <name>yarn.resourcemanager.resource-tracker.address.rm2</name>
>         <value>hadoop103:8031</value>
>     </property>
> 
> <!-- ========== rm3的配置 ========== -->
>     <!-- 指定rm1的主机名 -->
>     <property>
>         <name>yarn.resourcemanager.hostname.rm3</name>
>         <value>hadoop104</value>
>     </property>
>     <!-- 指定rm1的web端地址 -->
>     <property>
>         <name>yarn.resourcemanager.webapp.address.rm3</name>
>         <value>hadoop104:8088</value>
>     </property>
>     <!-- 指定rm1的内部通信地址 -->
>     <property>
>         <name>yarn.resourcemanager.address.rm3</name>
>         <value>hadoop104:8032</value>
>     </property>
>     <!-- 指定AM向rm1申请资源的地址 -->
>     <property>
>         <name>yarn.resourcemanager.scheduler.address.rm3</name>  
>         <value>hadoop104:8030</value>
>     </property>
> 
>     <!-- 指定供NM连接的地址 -->  
>     <property>
>     <name>yarn.resourcemanager.resource-tracker.address.rm3</name>
>         <value>hadoop104:8031</value>
>     </property>
> 
>     <!-- 指定zookeeper集群的地址 --> 
>     <property>
>         <name>yarn.resourcemanager.zk-address</name>
>         <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>
>     </property>
> 
>     <!-- 启用自动恢复 --> 
>     <property>
>         <name>yarn.resourcemanager.recovery.enabled</name>
>         <value>true</value>
>     </property>
>  
>     <!-- 指定resourcemanager的状态信息存储在zookeeper集群 --> 
>     <property>
>         <name>yarn.resourcemanager.store.class</name>     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
> </property>
> 
>     <!-- 环境变量的继承 -->
>     <property>
>         <name>yarn.nodemanager.env-whitelist</name>
>         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
>     </property>
> ```
>
> + 启动相关linux命令
>
> ```bash
> #分发yarn-site.xml
> xsync etc/hadoop/yarn-site.xml
> 
> #启动YARN，任意节点启动都可以，因为yarn-site.xml中已配置
> start-yarn.sh
> 
> #查看服务状态
> yarn rmadmin -getServiceState rm1 #查看hadoop102的状态是Active还是Standby
> ```
>
> + 关闭Active状态的RM后，ZK中 `hadoop-ha` 节点变化：
>
> ```bash
> yarn --daemon stop resourcemanager
> ```
>
> ![image-20220702162831857](http://ybll.vip/md-imgs/202207021628983.png)

+++

## 3. 恢复原集群

1. NameNode 和 ResourceManager配置高可用后：

![image-20220702154539854](http://ybll.vip/md-imgs/202207021545953.png)

2. 恢复操作

```bash
#关闭hdfs
stop-dfs.sh
#关闭yarn
stop-yarn.sh
#关闭zookeeper
myzookeeper.sh stop
#可生成选择生成快照，以便后续直接恢复到HA配置完成

#更改环境变量文件，更改hadoop_home的值
sudo vim /etc/profile.d/my_env.sh

#分发环境变量文件至hadoop103,hadoop104
sudo scp -r /etc/profile.d/my_env.sh root@hadoop103:/etc/profile.d/my_env.sh
sudo scp -r /etc/profile.d/my_env.sh root@hadoop104:/etc/profile.d/my_env.sh

#设置环境变量生效,三个节点都需要设置
source /etc/profile.d/my_env.sh

#删除/tmp所有文件，三个节点都删除
sudo rm -rf /tmp/*
```



## 4. HA 整合 DataX

```json
"defaultFS": "hdfs://mycluster",
"hadoopConfig": {
    "dfs.nameservices": "mycluster",
    "dfs.client.failover.proxy.provider.mycluster": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
    "dfs.namenode.rpc-address.mycluster.nn2": "hadoop103:8020",
    "dfs.namenode.rpc-address.mycluster.nn3": "hadoop104:8020",
    "dfs.namenode.rpc-address.mycluster.nn1": "hadoop102:8020",
    "dfs.ha.namenodes.mycluster": "nn1,nn2,nn3"
},
```



