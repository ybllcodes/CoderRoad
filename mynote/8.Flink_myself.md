

+ slot : 任务槽，子任务可以共享【并行子任务在不同的slot】,使得一个 slot 可以保存作业的整个通道，
  + Task Slot : 静态概念，指TaskManager具有的并发执行能力，一个TaskManager默认1个Slot
  + parallelism : 动态概念，指TaskManager运行任务时，实际的并发数

# Flink

> + Apache Flink是为分布式、高性能、随时可用以及准确的流处理应用程序打造的开源的**有状态的流处理**框架
>
> + 在常见的集群环境中运行，以内存执行速度和任意规模来执行计算
> + 属于事件驱动型（Kafka也是事件驱动型代表）
>
> +++
>
> + Spark : 批处理；实时计算是也将数据划分为小批次再进行处理，由此看做流处理
> + Flink : 一切都是流组成，批是有界流，实时是无界流
>
> +++
>
> + 分层API
>   + Stateful Stream Processing  ==>  【最底层，功能实现最完善，但是开发难度大】
>   + DataStream【DataStreamSource对象】 /  DataSet【DataSource对象】 API   ==>【Core APIs】
>   + Table API 
>   + SQL 【使用简单】

+++



## Flink安装与部署

### IDEA案例

> + 使用 Flink 的相关jar包，创建集群环境，在本地运行Job任务
>
>   +++
>
> + pom.xml
>
>   ```xml
>    <properties>
>           <flink.version>1.13.0</flink.version>
>           <java.version>1.8</java.version>
>           <scala.binary.version>2.12</scala.binary.version>
>           <slf4j.version>1.7.30</slf4j.version>
>       </properties>
>   
>       <dependencies>
>           <dependency>
>               <groupId>org.apache.flink</groupId>
>               <artifactId>flink-java</artifactId>
>               <version>${flink.version}</version>
>           </dependency>
>   
>           <dependency>
>               <groupId>org.apache.flink</groupId>
>               <artifactId>flink-streaming-java_${scala.binary.version}</artifactId>
>               <version>${flink.version}</version>
>           </dependency>
>   
>           <dependency>
>               <groupId>org.apache.flink</groupId>
>               <artifactId>flink-clients_${scala.binary.version}</artifactId>
>               <version>${flink.version}</version>
>           </dependency>
>   
>           <dependency>
>               <groupId>org.apache.flink</groupId>
>               <artifactId>flink-runtime-web_${scala.binary.version}</artifactId>
>               <version>${flink.version}</version>
>           </dependency>
>   
>           <dependency>
>               <groupId>org.slf4j</groupId>
>               <artifactId>slf4j-api</artifactId>
>               <version>${slf4j.version}</version>
>           </dependency>
>           <dependency>
>               <groupId>org.slf4j</groupId>
>               <artifactId>slf4j-log4j12</artifactId>
>               <version>${slf4j.version}</version>
>           </dependency>
>           <dependency>
>               <groupId>org.apache.logging.log4j</groupId>
>               <artifactId>log4j-to-slf4j</artifactId>
>               <version>2.14.0</version>
>           </dependency>
>   
>           <!-- https://mvnrepository.com/artifact/org.projectlombok/lombok -->
>           <dependency>
>               <groupId>org.projectlombok</groupId>
>               <artifactId>lombok</artifactId>
>               <version>1.18.16</version>
>           </dependency>
>   
>           <dependency>
>               <groupId>org.apache.flink</groupId>
>               <artifactId>flink-connector-kafka_2.12</artifactId>
>               <version>${flink.version}</version>
>           </dependency>
>   
>           <dependency>
>               <groupId>com.alibaba</groupId>
>               <artifactId>fastjson</artifactId>
>               <version>1.2.75</version>
>           </dependency>
>           <dependency>
>               <groupId>org.apache.flink</groupId>
>               <artifactId>flink-connector-redis_2.11</artifactId>
>               <version>1.1.5</version>
>           </dependency>
>           <dependency>
>               <groupId>mysql</groupId>
>               <artifactId>mysql-connector-java</artifactId>
>               <version>5.1.49</version>
>           </dependency>
>           <dependency>
>               <groupId>org.apache.flink</groupId>
>               <artifactId>flink-connector-jdbc_2.12</artifactId>
>               <version>${flink.version}</version>
>           </dependency>
>   
>       </dependencies>
>   
>       <build>
>           <plugins>
>               <plugin>
>                   <groupId>org.apache.maven.plugins</groupId>
>                   <artifactId>maven-assembly-plugin</artifactId>
>                   <version>3.3.0</version>
>                   <configuration>
>                       <descriptorRefs>
>                           <descriptorRef>jar-with-dependencies</descriptorRef>
>                       </descriptorRefs>
>                   </configuration>
>                   <executions>
>                       <execution>
>                           <id>make-assembly</id>
>                           <phase>package</phase>
>                           <goals>
>                               <goal>single</goal>
>                           </goals>
>                       </execution>
>                   </executions>
>               </plugin>
>           </plugins>
>       </build>
>   ```
>
>   > 在属性中，我们定义了<scala.binary.version>，这指代的是所依赖的 Scala 版本。这有一点奇怪：Flink 底层是 Java，而且我们也只用 Java API，为什么还会依赖 Scala 呢？这是因为 Flink 的架构中使用了**Akka 来实现底层的分布式通信**，而 Akka 是用 Scala 开发的。我们本书中用到的 Scala 版本为 2.12。
>
> + log4j.properties
>
>   ```properties
>   log4j.rootLogger=error, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
>   log4j.appender.stdout.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n
>   
>   ```
>
> + 有界案例
>
>   ```java
>       public static void main(String[] args) throws Exception {
>           //1.获取流的执行环境
>           StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
>   
>           //为了看起来方便我们可以把并行度设置为1
>           env.setParallelism(1);
>   
>           //2.读取文件中的数据
>           DataStreamSource<String> streamSource = env.readTextFile("input/word.txt");
>   
>           //3.使用flatmap将每一行数据按照空格切出每一个单词
>           SingleOutputStreamOperator<String> wordDStream = streamSource.flatMap(new FlatMapFunction<String, String>() {
>               @Override
>               public void flatMap(String value, Collector<String> out) throws Exception {
>                   //将每一行数据按照空格切分
>                   String[] words = value.split(" ");
>                   for (String word : words) {
>                       out.collect(word);
>                   }
>               }
>           });
>   
>           //4.使用map将切出的每一个单词组成Tuple2元组
>           //使用lambda表达式会涉及到泛型擦除问题
>           SingleOutputStreamOperator<Tuple2<String, Integer>> wordToOneDStream = wordDStream.map( value -> Tuple2.of(value, 1)).returns(Types.TUPLE(Types.STRING,Types.INT));
>   
>           //5.将相同的单词聚合到一块
>           //方式一：通过下标指定key
>   //        KeyedStream<Tuple2<String, Integer>, Tuple> keyedStream = wordToOneDStream.keyBy(0);
>           //方式二：通过key的选择器手动指定key
>           KeyedStream<Tuple2<String, Integer>, String> keyedStream = wordToOneDStream.keyBy(new KeySelector<Tuple2<String, Integer>, String>() {
>               /**
>                *
>                * @param value 传入的数据
>                * @return
>                * @throws Exception
>                */
>               @Override
>               public String getKey(Tuple2<String, Integer> value) throws Exception {
>                   return value.f0;
>               }
>           });
>   
>           //6.做累加计算
>           SingleOutputStreamOperator<Tuple2<String, Integer>> result = keyedStream.sum(1);
>   
>           //7.打印到控制台
>           result.print();
>   
>           //执行
>           env.execute();
>       }
>   ```
>
> + 无界案例
>
>   ```java
>   public static void main(String[] args) throws Exception {
>           //1.获取流的执行环境
>   //        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
>           StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration());
>           //为了看起来方便我们可以把并行度设置为1
>           env.setParallelism(1);
>   
>           //2.从端口读取数据（无界流）
>           DataStreamSource<String> streamSource = env.socketTextStream("hadoop102", 9999);
>   
>           //3.使用flatmap将每一行数据按照空格切出每一个单词
>           SingleOutputStreamOperator<String> wordDStream = streamSource.flatMap(new FlatMapFunction<String, String>() {
>               @Override
>               public void flatMap(String value, Collector<String> out) throws Exception {
>                   String[] words = value.split(" ");
>                   for (String word : words) {
>                       out.collect(word);
>                   }
>               }
>           });
>   
>           //4.使用map将切出的每一个单词组成Tuple2元组
>           SingleOutputStreamOperator<Tuple2<String, Integer>> wordToOneDStream = wordDStream.map(new MapFunction<String, Tuple2<String, Integer>>() {
>               @Override
>               public Tuple2<String, Integer> map(String value) throws Exception {
>                   return Tuple2.of(value, 1);
>               }
>           });
>   
>           //5.将相同的单词聚合到一块
>           KeyedStream<Tuple2<String, Integer>, Tuple> keyedStream = wordToOneDStream.keyBy(0);
>   
>           //6.做累加计算
>           SingleOutputStreamOperator<Tuple2<String, Integer>> result = keyedStream.sum(1);
>   
>           //7.打印到控制台
>           result.print();
>   
>           //执行
>           env.execute();
>       }
>   ```

+++

### Flink集群部快速署

> + 单节点部署
>
>   > ```bash
>   > #hadoop102安装flink
>   > tar -zxvf flink-1.13.0-bin-scala_2.12.tgz -C /opt/module/ 
>   > 
>   > #启动集群
>   > bin/start-cluster.sh 
>   > 
>   > #关闭集群
>   > bin/stop-cluster.sh
>   > ```
>   >
>   > + Web UI
>   >
>   >   http://hadoop102:8081  【对flink集群和任务进行监控管理】
>   >
>   > + 任务提交默认也是 **8081** 端口
>   >
>   > + 查看进程
>   >
>   >   ![image-20220927221807799](http://ybll.vip/md-imgs/202209272218977.png)
>   >
>   >   ```bash
>   >   StandaloneSessionClusterEntrypoint 
>   >   TaskManagerRunner
>   >   #此时是 会话模式
>   >   #StandaloneSessionClusterEntrypoint是JobManager（Master）运行的进程
>   >   
>   >   #TaskManagerRunner是TaskManager（Worker）的进程。
>   >   ```
>   >
>   > +++
>
> + 多节点部署（会话模式）
>
>   > + hadoop102 : JobManager
>   >
>   > + hadoop103,hadoop104 : TaskManager
>   >
>   >   +++
>   >
>   > + **conf/flink-conf.yaml** 文件
>   >
>   >   ```yaml
>   >   # JobManager 节点地址.
>   >   jobmanager.rpc.address: hadoop102
>   >   ```
>   >
>   >   + 其他配置说明
>   >
>   >     > + jobmanager.memory.process.size：对 JobManager 进程可使用到的全部内存进行配置， 包括 JVM 元空间和其他开销，默认为 1600M，可以根据集群规模进行适当调整。
>   >     > + taskmanager.memory.process.size：对 TaskManager 进程可使用到的全部内存进行配置，包括 JVM 元空间和其他开销，默认为 1600M，可以根据集群规模进行适当调整。
>   >     > + taskmanager.numberOfTaskSlots：对每个 TaskManager 能够分配的 Slot 数量进行配置， 默认为 1，可根据 TaskManager 所在的机器能够提供给 Flink 的 CPU 数量决定。所谓Slot 就是TaskManager 中具体运行一个任务所分配的计算资源。
>   >     > + parallelism.default：Flink 任务执行的默认并行度，优先级低于代码中进行的并行度配置和任务提交时使用参数指定的并行度数量。
>   >
>   > + **workers** 文件
>   >
>   >   ```txt
>   >   hadoop103
>   >   hadoop104
>   >   ```
>   >
>   > + 分发flink包到 hadoop103,hadoop104
>   >
>   > + 启动集群
>   >
>   >   ```bash
>   >   bin/start-cluster.sh
>   >   #102进程:
>   >   StandaloneSessionClusterEntrypoint
>   >   #103进程：
>   >   TaskManagerRunner
>   >   
>   >   #关闭集群
>   >   bin/stop-cluster.sh
>   >   ```
>   >
>   > + Web UI 界面  `hadoop102:8081`
>   >
>   >   > + TaskManager数量为 2
>   >   > + 每个TaskManager的Slot数量默认为1，总Slot数为2
>   >
>   > +++
>
> + 说明
>
>   > + 以上两种都是会话模式
>   > + 以上两种都是静态分配资源
>
>   +++
>
> + 命令行模式
>
>   > ```bash
>   > bin/flink run -m hadoop102:8081 com.atguigu.wc.StreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar
>   > #提交任务【会话模式】
>   > ```
>   >
>   > + 执行后， Ctrl+C 退出该命令执行，集群不会关闭，Job也不会停
>   >
>   > + 需要去 WebUI界面 停止Job,
>   >
>   > + 命令行停止 Job
>   >
>   >   ```bash
>   >   ./bin/flink cancel application_XXXX_YY <jobId>
>   >   ```
>
>   +++
>
> +++



### Flink部署与原理

#### 运行原理

> + 我们编写的代码，对应着Flink集群上执行的一个作业
>+ 无论是本地启动，还是集群启动，都会先模拟启动一个Flink集群，
> + 然后将作业提交到集群上，创建好需要执行的任务，等待数据输入
> + 关键组件：
>   + 客户端（Client）: 获取我们的编写的代码，转换成Job,提交给JobManager
>   + 作业管理器（JobManager）: 对 Job 进行调度管理，分发任务给TaskManager
>   + 任务管理器（TaskManager）: 真正的任务执行，数据处理操作
> 
> ![image-20220925150327582](http://ybll.vip/md-imgs/202209251503691.png)



#### 部署模式

> + 会话模式
>
>   > + 只有一个集群，所有Job都在该集群执行
>   >
>   > + 集群资源共享，资源不够时，新提交的作业就会失败
>   > + 同一个TaskManager可能有多个作业的任务，其中一个任务发生故障，使得TaskManager宕机，该TaskManager所有作业都会收到影响
>   > + 适合 单个规模小，执行时间短的大量作业
>   >
>   > +++
>
> + 单作业模式
>
>   > + 严格的 一对一 模式，一个作业对应一个集群，作业结束，集群也会关闭
>  > + **Flink本身不可使用单作业模式，需要借助资源管理框架实现【Yarn、K8S...】**
>   > + 很常用，适合 规模大，执行时间长的作业
>  >
>   > +++
>
> + 应用模式
> 
>   > + 直接将用户编写代码提交给 JobManager上运行，每提交一个应用，就单独启动一个JobManager【即：启动一个集群】
>   > + 该应用执行结束后，集群关闭
>   >
>   > +++
> 
>+ 总结
> 
>  > + 会话模式，单作业模式 都是在客户端上执行用户代码，转换为Job，然后由客户端提交给JobManager，因此会占用网络带宽，需要下载依赖以及把二进制数据发送给JobManager
>   >
>   > + 应用模式，则是把代码交给JobManager，执行时，Flink集群(JobManager)有依赖,资源消耗少
>   >
>   >   +++
>   >
>  > + 会话模式是先有集群，然后等待着任务提交，期间资源会一直占着
>   >
>  > + 单作业模式是提交一个Job启动一个集群，用户提交代码【理解为一个应用】可能有多个算子，此时有多个Job,此时会开启多个集群
>   >
>   > + 应用模式，是一个用户代码【一个应用】，开启一个集群，集群中可能有多个Job（用户代码中有多个算子），此时同样只有一个集群
>   >
>   > +++
>
> +++



#### Standalone模式

> + 独立运行，不依赖任何外部资源管理平台
> + 当然独立也是有代价的：如果资源不足，或者出现故障，没有自动扩展或重分配资源的保证，必须手动处理
> + 所以独立模式一般只用在开发测试或作业非常少的场景下
>
> +++
>
> + 会话模式部署
>
>   > 上述 **Flink集群快速部署** 都是Standalone部署模式下的会话模式部署
>
> + **单作业模式部署【不支持】**
>
> + 应用模式部署【一般不用】
>
>   > + 应用模式下不会提前创建集群，所以不能调用 start-cluster.sh 脚本
>   > + 使用 bin/ 下的 standalone-job.sh 来创建一个 JobManager
>   >
>   > ```bash
>   > #具体步骤如下：
>   > #（1）	进入到 Flink 的安装路径下，将应用程序的 jar 包放到 lib/目录下。
>   > cp ./FlinkTutorial-1.0-SNAPSHOT.jar lib/
>   > #（2）	执行以下命令，启动 JobManager。
>   >  ./bin/standalone-job.sh start --job-classname com.atguigu.wc.StreamWordCount
>   > #（3）	同样是使用bin 目录下的脚本，启动 TaskManager。
>   > ./bin/taskmanager.sh start
>   > #（4）	如果希望停掉集群，同样可以使用脚本，命令如下。
>   > ./bin/standalone-job.sh stop
>   > ./bin/taskmanager.sh stop
>   > ```
>
> +++
>
> 配置高可用
>
> ---

#### Yarn模式

> + 客户端把 Flink 应用提交给 Yarn 的ResourceManager, 
> + Yarn 的 ResourceManager 会在 NodeManager 创建容器
> + 在Container容器上，Flink 会部署JobManager 和 TaskManager 的实例，从而启动集群
> + **Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配TaskManager 资源**
>
> +++

##### 环境准备

> ```bash
> sudo vim /etc/profile.d/my_env.sh
> HADOOP_HOME=/opt/module/hadoop-3.1.3
> export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
> export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
> export HADOOP_CLASSPATH=`hadoop classpath`
> ```
>
> + HADOOP_CLASSPATH 必须配置，使得flink能够找到hadoop Jar包
>
> + 需要启动 Hadoop集群
>
> + 修改 `flink-conf.yaml` 文件 【flink包的 conf 下】
>
>   ```yaml
>   jobmanager.memory.process.size: 1600m
>   taskmanager.memory.process.size: 1728m
>   taskmanager.numberOfTaskSlots: 8
>   parallelism.default: 1
>   ```
>
> +++

##### 会话模式[重要]

> + 需要首先申请一个 Yarn会话【Yarn session】,以此启动 Flink集群
>
> + Yarn Session 实际就是Yarn的一个 Application,有唯一的ApplicationID
>
>   ```bash
>   bin/yarn-session.sh -nm test
>   ```
>
>   > 可配置参数
>   >
>   > + -d：分离模式，如果你不想让 Flink YARN 客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，YARN session 也可以后台运行
>   > + -jm(--jobManagerMemory)：配置 JobManager 所需内存，默认单位 MB
>   > + -nm(--name)：配置在 YARN UI 界面上显示的任务名
>   > + -qu(--queue)：指定 YARN 队列名
>   > + -tm(--taskManager)：配置每个 TaskManager 所使用内存。
>   >
>   > ![image-20220927233741056](http://ybll.vip/md-imgs/202209272337177.png)
>   >
>   > +++
>   >
>   > Flink1.11.0 版本不再使用-n 参数和-s 参数分别指定 TaskManager 数量和 slot 数量， YARN 会按照需求动态分配 TaskManager 和 slot。所以从这个意义上讲，YARN 的会话模式也 不会把集群资源固定，同样是动态分配的
>
> + 上述启动后，会给出 WebUI地址，以及 YARN application ID
>
>   ```bash
>   Found Web Interface hadoop104:39735 of application 
>   'application_1622535605178_0003'
>   ```
>
> + 关闭后，集群也会关闭
>
> + 提交作业
>
>   > + 命令行提交，不需要 -m 指定 JobManager【由Yarn管理】
>   >
>   >   ```bash
>   >    bin/flink run
>   >   -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar
>   >   ```
>   >
>   > + Web UI提交：同样需要先上传 jar 包
>
> + 取消作业
>
>   > ```bash
>   > ./bin/flink cancel application_XXXX_YY<jobId>
>   > ```

+++

##### 单作业模式[重要]

> 直接提交作业，不需要先启动 Yarn Session
>
> +++
>
> + 命令行提交
>
>   ```bash
>   bin/flink run -d -t yarn-per-job -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar
>   ```
>
> + 查看取消作业
>
>   ```bash
>   ./bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY
>   
>   ./bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY <jobId>
>   ```
>
> +++

##### 应用模式

> + 提交作业
>
>   ```bash
>   bin/flink run-application -t yarn-application -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar
>   ```
>
> + 查看或取消作业
>
>   ```bash
>   ./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY
>   
>   ./bin/flink cancel -t yarn-application 
>   -Dyarn.application.id=application_XXXX_YY <jobId>
>   ```



##### 高可用

> 1

+++



### 运行命令合集

> + linux，单节点
>
>   `bin/start-cluster.sh`
>
>   + #102进程:
>     	StandaloneSessionClusterEntrypoint
>   + #103进程：
>     	TaskManagerRunner
>
>   + #关闭集群
>     `bin/stop-cluster.sh`
>   + 此时是会话模式
>
>   +++
>
> + -- 多节点部署 会话模式
>   -- 修改配置文件后，直接启动，workers | flink-conf.yaml
>   `bin/start-cluster.sh`
> + #102进程:
>   StandaloneSessionClusterEntrypoint
>   #103进程：
>   TaskManagerRunner
>
> +#关闭集群
> bin/stop-cluster.sh
>
> -- 以上两种都是静态分配资源
>
> 提交任务 ：
> bin/flink run -m hadoop102:8081 com.atguigu.TestClass ./test.jar
>
> 此时： ctrl + c 停止，集群和 Job都不会停
> 停止 Job : 
> 	./bin/flink cancel <jobId>
>
> 停止集群：
>
> 
>
> -- Flink Yarn模式部署
> --会话模式：先申请一个Yarn会话(Yarn的一个 Application)
> bin/yarn-session.sh -nm test
> 	--命令行关闭进程后，集群(Yarn Session[Application])也会关闭
> 	--取消作业
> 		./bin/flink cancel <jobId>
>
> --单作业模式
> 	--命令行提交(一次提交，一个 Application)
> 		bin/flink run -d -t yarn-per-job -c com.wc.StreamWordCount ./wc.jar
> 	--查看/取消作业
> 		./bin/flink list -t yarn-per-job -Dyarn.application.id=<jobId>
> 		--
> 		./bin/flink cancel -t yarn-per-job -Dyarn.application.id=<jobId>
>
> --应用模式
> 	--命令行提交
> 		bin/flink run-application -t yarn-application -c com.atguigu.wc.StreamWordCount ./wc.jar
> 	--查看/取消作业
> 		./bin/flink list -t yarn-application -Dyarn.application.id=<jobId>
> 		./bin/flink cancel -t yarn-application -Dyarn.application.id=<jobId>

## Flink运行架构

> + Flink作业提交和任务处理系统
>
> ![image-20220926100447833](http://ybll.vip/md-imgs/202209261004943.png)
>
> + 客户端负责作业的提交，【调用用户程序的main方法】将代码转换为 Dataflow Graph [数据流图]，并生成 JobGraph [作业图]，然后发送给JobManager
>
> + 提交给JobManager后，就可以关闭客户端与 JobManager的连接
>
> + 客户端可以随时连接到 JobManager,获取当前作业的状态和执行结果，也可以发送请求取消命令【fink cancel】
>
>   +++
>
> + TaskManager 和 TaskManager 可以以不同的方式启动：
>
>   + Standalone 模式启动，直接在Linux机器上启动
>   + 在容器中启动
>   + 由资源管理平台调度启动，如 YARN,K8S
>
> + TaskManager 启动之后，JobManager 会与它建立连接
>
> + 接着，TaskManaer将作业图（JobGraph）转换成可 执行的“执行图”（ExecutionGraph）分发给可用的 TaskManager
>
> + 然后就由 TaskManager 具体 执行任务
>
> +++

+++

### JobManager

> + JobManager 是一个 Flink 集群中任务管理和调度的核心，是控制应用执行的主进程
> + 每个应用都应该被唯一的 JobManager 所控制执行
> + 在高可用（HA）的场景下， 50 可能会出现多个 JobManager；这时只有一个是正在运行的领导节点（leader），其他都是备用 节点（standby）
>
> +++

#### JobMaster

#### ResourceManager

#### Dispatcher



### TaskManager





### 其他概念

#### Dataflow Graph

#### Parallelism 并行度

#### Operator Chain 算子链

#### Task 和 Task Slots



## Flink流处理核心编程

### Environment

> + Flink Job提交之前，需要先建立和Flink框架的联系【即**创建flink运行的环境**】，获取了环境信息，才能将 **task任务 调度到不同的taskManager 执行**
>
> + 批处理环境 和 流处理环境
>
>   ```java
>   // 批处理环境
>   ExecutionEnvironment benv = ExecutionEnvironment.getExecutionEnvironment();
>   
>   // 流式数据处理环境
>   StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
>   ```

+++



### Source

> + Flink框架可以从不同来源获取数据，将数据提交给框架进行处理，将获取数据的开源称之为 数据源
>
> +++
>
> **集合中获取**
>
> ```java
> public class Flink03_Source_Collection {
>     public static void main(String[] args) throws Exception {
>         //1.获取流的执行环境
>         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
>         env.setParallelism(1);
> 
>         //TODO 2.从集合获取数据
> //        List<Integer> list = Arrays.asList(1, 2, 3, 4);
> //        DataStreamSource<Integer> streamSource = env.fromCollection(list);
>         //TODO 从元素获取数据
>         DataStreamSource<Integer> streamSource = env.fromElements(1, 2, 3, 4);
> 
>         streamSource.print();
>         env.execute();
>     }
> }
> //输出
> //1
> //2
> //3
> //4
> ```
>
> +++
>
> **文件中获取**
>
> ```java
> //1.获取流的执行环境
> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
> env.setParallelism(1);
> //TODO 2.从文件获取数据
> DataStreamSource<String> streamSource = env.readTextFile("input/word.txt");
> streamSource.print();
> env.execute();
> ```
>
> +++
>
> **Socket中获取【监听端口，获取无界流】**
>
> ```java
> //2.从端口读取数据（无界流）
> DataStreamSource<String> streamSource = env.socketTextStream("hadoop102", 9999);
> ```
>
> +++
>
> **Kafka中获取**
>
> ```java
> public class Flink05_Source_Kafka {
>     public static void main(String[] args) throws Exception {
>         //1.获取流的执行环境
>         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
> 
>         env.setParallelism(1);
> 
>         //TODO 2.从Kafka读取数据 1.13之后新版写法
>     /*    KafkaSource<String> kafkaSource = KafkaSource.<String>builder()
>                 .setBootstrapServers("hadoop102:9092")
>                 .setGroupId("220509")
>                 .setTopics("sensor")
>                 .setStartingOffsets(OffsetsInitializer.latest())
>                 .setValueOnlyDeserializer(new SimpleStringSchema())
>                 .build();
>         DataStreamSource<String> streamSource = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "kafkaSource");*/
> 
>     //TODO kafkaSource通用写法
>         Properties properties = new Properties();
>         properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG
>                                , "hadoop102:9092");
>         properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "220509");
>         properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
> 
>         DataStreamSource<String> streamSource = env
>             .addSource(new FlinkKafkaConsumer<String>(
>                 "sensor", new SimpleStringSchema()
>                 , properties))
>             .setParallelism(2);
> 
>         streamSource.print();
>         env.execute();
>     }
> }
> ```
>
> +++
>
> **自定义Source**
>
> > + 实现 `SourceFunction` 相关接口【 `ParallelSourceFunction`  ...】
> > + 重写两个方法：`run()`  `cancel()`
> > + 实现 ParallelSourceFunction 接口,Source可以指定并行度
> >
> > ```java
> > public class Flink06_Source_Custom {
> >     public static void main(String[] args) throws Exception {
> >         //1.获取流的执行环境
> >         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
> >         env.setParallelism(1);
> > 
> >         //TODO 2.自定义Source读取数据
> >         DataStreamSource<WaterSensor> streamSource = env.addSource(new MySource()).setParallelism(2);
> >         streamSource.print();
> > 
> >         env.execute();
> >     }
> > 
> >     //随机生成WaterSensor数据
> > //    public static class MySource implements SourceFunction<WaterSensor>{
> >     //设置多并行度的方式
> >     public static class MySource implements ParallelSourceFunction<WaterSensor> {
> >         private Boolean isRunning = true;
> >         private Random random = new Random();
> > 
> >         @Override
> >         public void run(SourceContext<WaterSensor> ctx) throws Exception {
> >             while (isRunning){
> >                 ctx.collect(new WaterSensor("sensor"+random.nextInt(1000),System.currentTimeMillis(),random.nextInt(100)));
> >                 Thread.sleep(200);
> >             }
> >         }
> >         @Override
> >         public void cancel() {
> >             isRunning = false;
> >         }
> >     }
> > }
> > ```
>
> +++



### Transform

> + 转换算子 可以把 一个或多个 DataStream 转成 一个新的DataStream，程序可以把多个复杂的转换组合成复杂的数据流拓扑
>
> +++
>
> ##### map()
>
> +++
>
> ##### flatMap()
>
> +++
>
> ##### filter()
>
> +++
>
> ##### keyBy()
>
> +++
>
> ##### shuffle()
>
> +++
>
> ##### split() select()
>
> +++
>
> ##### connect()
>
> +++
>
> ##### union()
>
> +++
>
> ##### reduce()
>
> +++
>
> ##### process()
>
> +++

滚动集合算子

##### 重分区的几个算子





### Sink

##### KafkaSink

##### RedisSink

##### JDBCSink

##### 自定义Sink



## Flink流处理高阶编程





## Flink Table API

> + 动态表：随时间变化，可以不断更新，反映其动态输入表上的更改
>
> ![image-20220927112942389](http://ybll.vip/md-imgs/202209271129450.png)
>
> + 导入依赖 pom.xml
>
>   ```xml
>   <!--老版官方提供的依赖没有融合blink的-->
>   <dependency>
>       <groupId>org.apache.flink</groupId>
>       <artifactId>flink-table-planner_${scala.binary.version}</artifactId>
>       <version>${flink.version}</version>
>   </dependency>
>   
>   <!-- 直接使用下面的-->
>   <!--blink二次开发之后的依赖-->
>   <dependency>
>       <groupId>org.apache.flink</groupId>
>       <artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
>       <version>${flink.version}</version>
>   </dependency>
>   <dependency>
>       <groupId>org.apache.flink</groupId>
>       <artifactId>flink-csv</artifactId>
>       <version>${flink.version}</version>
>   </dependency>
>   <dependency>
>       <groupId>org.apache.flink</groupId>
>       <artifactId>flink-json</artifactId>
>       <version>${flink.version}</version>
>   </dependency>
>   ```
>
> + 使用总结
>
>   > 1. 创建流环境，StreamExecutionEnvironment
>   >
>   >    ```java
>   >    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
>   >    ```
>   >
>   > 2. 创建表的执行环境，StreamTableEnvironment
>   >
>   >    ```java
>   >    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
>   >    ```
>   >
>   > 3. 创建表：将流转换为表 【tableEnv 连接不同的数据源，FileSystem|Kakfa】
>   >
>   >    ```java
>   >    Table table = tableEnv.fromDataStream(waterSensorStream);
>   >    ```
>   >
>   >    > 从外部文件读取数据，通过 表执行环境 创建表
>   >    >
>   >    > + 文件中读取
>   >    >
>   >    >   ```java
>   >    >           Schema schema = new Schema();
>   >    >   //        schema.field("id", "String");
>   >    >           schema.field("id", DataTypes.STRING());
>   >    >           schema.field("ts", DataTypes.BIGINT());
>   >    >           schema.field("vc", DataTypes.INT());
>   >    >           tableEnv.connect(
>   >    >               new FileSystem().path("input/sensor-sql.txt")
>   >    >           	)
>   >    >               .withFormat(
>   >    >               	new Csv()
>   >    >               		.fieldDelimiter(',')
>   >    >               		.lineDelimiter("\n")
>   >    >           	)
>   >    >               .withSchema(schema)
>   >    >               .createTemporaryTable("sensor");
>   >    >           //4.获取Table对象
>   >    >           Table table = tableEnv.from("sensor");
>   >    >   ```
>   >    >
>   >    > + Kafka中读取
>   >    >
>   >    >   ```java
>   >    >           //TODO 3.连接kafka获取kafka数据
>   >    >           Schema schema = new Schema();
>   >    >   //        schema.field("id", "String");
>   >    >           schema.field("id", DataTypes.STRING());
>   >    >           schema.field("ts", DataTypes.BIGINT());
>   >    >           schema.field("vc", DataTypes.INT());
>   >    >           tableEnv.connect(
>   >    >               new Kafka()
>   >    >           		.version("universal")
>   >    >                   .topic("sensor")
>   >    >                   .property(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "hadoop102:9092")
>   >    >                   .property(ConsumerConfig.GROUP_ID_CONFIG, "0509")
>   >    >                   .startFromLatest()
>   >    >           	)
>   >    >               .withFormat(new Json())
>   >    >               .withSchema(schema)
>   >    >               .createTemporaryTable("sensor");
>   >    >           //4.将临时表转为Table
>   >    >           Table table = tableEnv.from("sensor");
>   >    >   ```
>   >
>   > 4. 查询数据
>   >
>   >    > ```java
>   >    > Table resultTable = table.groupBy($("id"))
>   >    >         .select($("id"), $("vc").sum().as("vcSum"));
>   >    > ```
>   >
>   > 5. 将表转化为流
>   >
>   >    > + 只追加流
>   >    >
>   >    >   ```java
>   >    >   DataStream<Row> dataStream = tableEnv.toAppendStream(resultTable, Row.class);
>   >    >   ```
>   >    >
>   >    > + 撤回流
>   >    >
>   >    >   ```java
>   >    >   DataStream<Tuple2<Boolean, Row>> dataStream = tableEnv.toRetractStream(resultTable, Row.class);
>   >    >   ```
>   >
>   > 6. 输出打印【通过表，或者通过表转换的流】
>   >
>   >    ```java
>   >    dataStream.print();
>   >    
>   >    //有表后，通过表环境，输出
>   >    //自动判断 只追加流 或 撤回流
>   >    tableEnv.executeSql("select * from sensor").print();
>   >    ```
>   >
>   >    > + 输出到文件
>   >    >
>   >    >   ```java
>   >    >   //TODO 3.连接外部文件系统读取文件中的数据到表中
>   >    >   		//Schema 表结构对象，和输入数据对应
>   >    >           Schema schema = new Schema();
>   >    >   //        schema.field("id", "String");
>   >    >           schema.field("id", DataTypes.STRING());
>   >    >           schema.field("ts", DataTypes.BIGINT());
>   >    >           schema.field("vc", DataTypes.INT());
>   >    >           tableEnv.connect(new FileSystem().path("output/sensor-sql.txt"))
>   >    >                   .withFormat(new Csv().fieldDelimiter('/'))
>   >    >                   .withSchema(schema)
>   >    >                   .createTemporaryTable("sensor");
>   >    >   
>   >    >           table.executeInsert("sensor");
>   >    >   		//通过table输出到外部系统，可以不用提交执行环境（没有算子） env.execute();
>   >    >   ```
>   >    >
>   >    > + 输出到Kafka
>   >    >
>   >    >   ```java
>   >    >           //TODO 3.连接kafka获取kafka数据
>   >    >           Schema schema = new Schema();
>   >    >   //        schema.field("id", "String");
>   >    >           schema.field("id", DataTypes.STRING());
>   >    >           schema.field("ts", DataTypes.BIGINT());
>   >    >           schema.field("vc", DataTypes.INT());
>   >    >           tableEnv.connect(
>   >    >                   new Kafka()
>   >    >                           .version("universal") //通用版本
>   >    >                           .topic("sensor")
>   >    >                           .property(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "hadoop102:9092")
>   >    >                           .sinkPartitionerRoundRobin()
>   >    >                   )
>   >    >                   .withFormat(new Csv())
>   >    >                   .withSchema(schema)
>   >    >                   .createTemporaryTable("sensor");
>   >    >           //将数据写入临时表中相当于写入 kafka
>   >    >           table.executeInsert("sensor");
>   >    >   		//通过table输出到外部系统，可以不用提交执行环境（没有算子） env.execute();
>   >    >   ```
>   >
>   > 7. 提交执行环境
>   >
>   >    ```java
>   >    env.execute();
>   >    ```
>
> + Flink SQL 案例
>
>   ```java
>       public static void main(String[] args) {
>           StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
>           env.setParallelism(1);
>           DataStreamSource<WaterSensor> waterSensorStream =
>                   env.fromElements(new WaterSensor("sensor_1", 1000L, 10),
>                           new WaterSensor("sensor_1", 2000L, 20),
>                           new WaterSensor("sensor_2", 3000L, 30),
>                           new WaterSensor("sensor_1", 4000L, 40),
>                           new WaterSensor("sensor_1", 5000L, 50),
>                           new WaterSensor("sensor_2", 6000L, 60));
>   
>           StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
>   
>           //将流转为表  未注册的表没有表名
>   //        Table table = tableEnv.fromDataStream(waterSensorStream);
>   
>           //可以通过Table去注册表名
>   //        tableEnv.createTemporaryView("sensor", table);
>   
>           //可以通过流去注册表名
>           tableEnv.createTemporaryView("sensor", waterSensorStream);
>   
>    
>           tableEnv.executeSql("select id,sum(vc) from sensor group by id").print();
>       }
>   ```
>
> +++

+++



